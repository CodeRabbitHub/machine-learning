{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c3a8bb-0e7d-4673-b10b-1313a16e7df0",
   "metadata": {},
   "source": [
    "# General Parameters\n",
    "\n",
    "### booster\n",
    "- **What it is:** The type of model used by XGBoost.\n",
    "- **Options:**\n",
    "  - **gbtree:** Uses tree-based models.\n",
    "  - **gblinear:** Uses linear functions.\n",
    "  - **dart:** Uses tree-based models with a dropout technique.\n",
    "- **Default:** gbtree.\n",
    "\n",
    "### device\n",
    "- **What it is:** The hardware on which XGBoost will run.\n",
    "- **Options:**\n",
    "  - **cpu:** Use the computer’s CPU.\n",
    "  - **cuda:** Use a GPU (CUDA device).\n",
    "  - **cuda:<ordinal>:** Specify which GPU to use if you have more than one.\n",
    "  - **gpu:** Use the default GPU available.\n",
    "  - **gpu:<ordinal>:** Specify which GPU to use from the list of available GPUs.\n",
    "- **Default:** cpu.\n",
    "\n",
    "### verbosity\n",
    "- **What it is:** The level of detail in the messages printed by XGBoost.\n",
    "- **Options:**\n",
    "  - **0:** Silent (no messages).\n",
    "  - **1:** Warnings only.\n",
    "  - **2:** Info messages (general information).\n",
    "  - **3:** Debug messages (detailed information for debugging).\n",
    "- **Default:** 1 (warnings only).\n",
    "\n",
    "### validate_parameters\n",
    "- **What it is:** Whether XGBoost checks if the input parameters are valid.\n",
    "- **What it does:** When set to True, it warns if there are any unknown or unused parameters.\n",
    "- **Default:** False (except for Python, R, and CLI interfaces where it’s True).\n",
    "\n",
    "### nthread\n",
    "- **What it is:** Number of CPU threads used by XGBoost.\n",
    "- **What it does:** Controls the number of parallel threads for running XGBoost. More threads can speed up processing but may cause contention if too many are used.\n",
    "- **Default:** Maximum number of available threads.\n",
    "\n",
    "### disable_default_eval_metric\n",
    "- **What it is:** Whether to disable the default evaluation metric.\n",
    "- **What it does:** When set to 1 or true, the default metric used to evaluate the model during training is disabled.\n",
    "- **Default:** False (default metric is enabled).\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for Tree Booster\n",
    "\n",
    "### eta (learning_rate)\n",
    "- **What it is:** A factor that makes the learning process slower and more cautious.\n",
    "- **What it does:** It shrinks the weight of each new feature after each boosting step, helping to prevent overfitting (when the model is too closely fitted to the training data and performs poorly on new data).\n",
    "- **Range:** Between 0 and 1. The default value is 0.3.\n",
    "\n",
    "### gamma (min_split_loss)\n",
    "- **What it is:** A threshold for deciding whether to split a node further.\n",
    "- **What it does:** If the reduction in the loss (error) is less than gamma, the algorithm will not split the node, making the model more conservative.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 0.\n",
    "\n",
    "### max_depth\n",
    "- **What it is:** The maximum depth of each tree.\n",
    "- **What it does:** Deeper trees can learn more complex patterns but may also overfit. Limiting the depth can help prevent this.\n",
    "- **Range:** Starts from 0 (no limit) to any positive number. The default value is 6.\n",
    "\n",
    "### min_child_weight\n",
    "- **What it is:** Minimum sum of weights needed in a child node.\n",
    "- **What it does:** If a split results in a child node with a total weight less than this value, the split is discarded. Higher values make the model more conservative.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 1.\n",
    "\n",
    "### max_delta_step\n",
    "- **What it is:** The maximum change allowed for each tree's leaf output.\n",
    "- **What it does:** Controls the change in leaf values to make the model updates more cautious, useful for logistic regression with imbalanced classes.\n",
    "- **Range:** Starts from 0 (no constraint) to any positive number. The default value is 0.\n",
    "\n",
    "### subsample\n",
    "- **What it is:** Fraction of the training data used to build each tree.\n",
    "- **What it does:** Helps prevent overfitting by using only a portion of the data for each tree.\n",
    "- **Range:** Between 0 and 1. The default value is 1.\n",
    "\n",
    "### sampling_method\n",
    "- **What it is:** The method used to sample training instances.\n",
    "- **What it does:** Determines how the training data is sampled for each tree. The 'uniform' method gives equal chance to each instance, while 'gradient_based' focuses on instances with higher gradients.\n",
    "- **Range:** Choices are 'uniform' or 'gradient_based'. The default is 'uniform'.\n",
    "\n",
    "### colsample_bytree, colsample_bylevel, colsample_bynode\n",
    "- **What it is:** Fraction of columns to sample for each tree, level, or node.\n",
    "- **What it does:** Reduces the number of features considered at each split, helping to prevent overfitting.\n",
    "- **Range:** Between 0 and 1. The default value is 1.\n",
    "\n",
    "### lambda (reg_lambda)\n",
    "- **What it is:** Regularization term for weights (L2 regularization).\n",
    "- **What it does:** Penalizes large weights, making the model more conservative.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 1.\n",
    "\n",
    "### alpha (reg_alpha)\n",
    "- **What it is:** Regularization term for weights (L1 regularization).\n",
    "- **What it does:** Adds a penalty for the absolute value of the weights, promoting sparsity (more weights set to zero).\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 0.\n",
    "\n",
    "### tree_method\n",
    "- **What it is:** Algorithm used for constructing the trees.\n",
    "- **What it does:** Determines the method used to create the decision trees, which can affect speed and accuracy.\n",
    "- **Range:** Choices are 'auto', 'exact', 'approx', 'hist'. The default is 'auto'.\n",
    "\n",
    "### scale_pos_weight\n",
    "- **What it is:** Balances positive and negative classes.\n",
    "- **What it does:** Helps when the training data is imbalanced by adjusting the weight of positive and negative classes.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 1.\n",
    "\n",
    "### updater\n",
    "- **What it is:** Sequence of tree updaters to run.\n",
    "- **What it does:** Defines the modular way trees are constructed and modified. It's usually set automatically but can be set manually for advanced use.\n",
    "- **Range:** Various strings like 'grow_colmaker', 'grow_histmaker', 'sync', etc.\n",
    "\n",
    "### refresh_leaf\n",
    "- **What it is:** Parameter of the refresh updater.\n",
    "- **What it does:** When set to 1, updates both leaf values and node statistics; when 0, only updates node statistics.\n",
    "- **Range:** 0 or 1. The default value is 1.\n",
    "\n",
    "### process_type\n",
    "- **What it is:** Type of boosting process to run.\n",
    "- **What it does:** Defines whether to create new trees (default) or update existing ones (update).\n",
    "- **Range:** Choices are 'default' or 'update'. The default is 'default'.\n",
    "\n",
    "### grow_policy\n",
    "- **What it is:** Controls how new nodes are added.\n",
    "- **What it does:** Determines whether to split nodes nearest the root (depthwise) or nodes with the highest loss change (lossguide).\n",
    "- **Range:** Choices are 'depthwise' or 'lossguide'. The default is 'depthwise'.\n",
    "\n",
    "### max_leaves\n",
    "- **What it is:** Maximum number of nodes to be added.\n",
    "- **What it does:** Limits the number of leaves (nodes at the end of branches) in the trees.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 0.\n",
    "\n",
    "### max_bin\n",
    "- **What it is:** Number of bins to bucket continuous features.\n",
    "- **What it does:** Affects how continuous features are split into discrete bins, impacting split quality and computation time.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 256.\n",
    "\n",
    "### num_parallel_tree\n",
    "- **What it is:** Number of trees built in parallel in each iteration.\n",
    "- **What it does:** Supports constructing multiple trees in parallel, typically used for boosted random forests.\n",
    "- **Range:** Starts from 1 and can go up indefinitely. The default value is 1.\n",
    "\n",
    "### monotone_constraints\n",
    "- **What it is:** Constraints for variable monotonicity.\n",
    "- **What it does:** Enforces specific monotonic (increasing or decreasing) relationships between features and the target.\n",
    "- **Range:** N/A (user-specified).\n",
    "\n",
    "### interaction_constraints\n",
    "- **What it is:** Constraints for feature interactions.\n",
    "- **What it does:** Specifies which features are allowed to interact with each other.\n",
    "- **Range:** N/A (user-specified).\n",
    "\n",
    "### multi_strategy\n",
    "- **What it is:** Strategy for training multi-target models.\n",
    "- **What it does:** Determines how to handle multiple outputs (targets) during training.\n",
    "- **Range:** Choices are 'one_output_per_tree' or 'multi_output_tree'. The default is 'one_output_per_tree'.\n",
    "\n",
    "### max_cached_hist_node\n",
    "- **What it is:** Maximum number of cached nodes for CPU histogram.\n",
    "- **What it does:** Controls the number of nodes cached to speed up training with the histogram method.\n",
    "- **Range:** Starts from 0 and can go up indefinitely. The default value is 65536.\n",
    "\n",
    "\n",
    "# Additional parameters for Dart Booster (booster=dart)\n",
    "\n",
    "### max_cat_to_onehot\n",
    "- **What it is:** A threshold to decide whether to use one-hot encoding for categorical features.\n",
    "- **What it does:** If the number of categories in a feature is less than this threshold, XGBoost uses one-hot encoding (each category is represented by a separate binary feature). If the number of categories is greater, the categories are split into different groups (nodes).\n",
    "- **Introduced in:** Version 1.6.0.\n",
    "\n",
    "### max_cat_threshold\n",
    "- **What it is:** The maximum number of categories considered for each split.\n",
    "- **What it does:** Limits the number of categories used when splitting the data to prevent overfitting (making the model too closely fit to the training data).\n",
    "- **Introduced in:** Version 1.7.0.\n",
    "\n",
    "### Note\n",
    "- These parameters are **experimental** and are only used when training with categorical data.\n",
    "- The **exact tree method** is not yet supported for these parameters.\n",
    "\n",
    "\n",
    "# Parameters for linear booster (booster=gblinear):\n",
    "\n",
    "### lambda (reg_lambda)\n",
    "- **What it is:** L2 regularization term on weights.\n",
    "- **What it does:** Adds a penalty for the sum of squared weights. Higher values make the model more conservative by preventing large weights.\n",
    "- **Default:** 0.\n",
    "\n",
    "### alpha (reg_alpha)\n",
    "- **What it is:** L1 regularization term on weights.\n",
    "- **What it does:** Adds a penalty for the absolute values of weights. Higher values make the model more conservative by encouraging sparsity (more weights set to zero).\n",
    "- **Default:** 0.\n",
    "\n",
    "### updater\n",
    "- **What it is:** The algorithm used to fit the linear model.\n",
    "- **Options:**\n",
    "  - **shotgun:** Uses a parallel coordinate descent algorithm. This method uses ‘hogwild’ parallelism, which means it’s fast but can produce slightly different results each time.\n",
    "  - **coord_descent:** Uses a regular coordinate descent algorithm. This method is also multithreaded but produces consistent results. If using a GPU, a GPU variant is used.\n",
    "- **Default:** shotgun.\n",
    "\n",
    "### feature_selector\n",
    "- **What it is:** The method for selecting and ordering features during training.\n",
    "- **Options:**\n",
    "  - **cyclic:** Cycles through features one at a time in a fixed order.\n",
    "  - **shuffle:** Similar to cyclic but shuffles features randomly before each update.\n",
    "  - **random:** Selects features randomly with replacement.\n",
    "  - **greedy:** Selects the feature with the largest gradient magnitude, which is the most impactful. It’s deterministic and can be restricted to the top_k most significant features, reducing complexity.\n",
    "  - **thrifty:** A more efficient approximation of greedy selection that reorders features based on their impact before each update. It can also be restricted to the top_k features.\n",
    "- **Default:** cyclic.\n",
    "\n",
    "### top_k\n",
    "- **What it is:** Number of top features to select in greedy and thrifty feature selectors.\n",
    "- **What it does:** Limits the number of features considered to the top_k most significant ones. A value of 0 means all features are used.\n",
    "- **Default:** 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa9606-9d49-47f6-a851-343fe3fbbbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
