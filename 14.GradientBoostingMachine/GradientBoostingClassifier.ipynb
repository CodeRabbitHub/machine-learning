{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897d93ab-e4ac-41f2-b909-e924028e8f35",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines (GBM)\n",
    "\n",
    "Gradient Boosting Machines (GBMs) are a powerful ensemble learning technique used primarily for regression and classification tasks. They build models in a sequential manner where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, a key aspect of optimization in machine learning.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Ensemble Learning\n",
    "\n",
    "Ensemble learning combines multiple models to produce a single robust model. GBM is an ensemble technique that builds a series of models where each new model corrects the errors of the preceding ones.\n",
    "\n",
    "#### 2. Boosting\n",
    "\n",
    "Boosting is an ensemble method that combines weak learners to create a strong learner. In the context of GBM, boosting refers to the iterative process where each subsequent model is trained to minimize the errors of the previous models.\n",
    "\n",
    "### Steps Involved in Gradient Boosting Machines\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation for Classification\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The GBM process begins by initializing the model with a constant value. For regression problems, this is typically the mean of the target values $ y $. For classification problems, it is usually the log-odds of the positive class.\n",
    "\n",
    "For a binary classification task, we start with the initial log-odds prediction:\n",
    "$$ F_0(x) = \\frac{1}{2} \\log\\left(\\frac{p}{1-p}\\right) $$\n",
    "where $ p $ is the probability of the positive class.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "GBM constructs an ensemble of trees in a sequential manner. At each iteration $ m $:\n",
    "\n",
    "**Step 2-1: Calculate Pseudo-Residuals**\n",
    "\n",
    "- Compute the pseudo-residuals $ r_{im} $, which are the gradients of the loss function with respect to the predictions. For logistic regression (binary classification), the residuals are:\n",
    "$$ r_{im} = y_i - p_{im} $$\n",
    "where $ p_{im} $ is the probability of the positive class predicted at iteration $ m-1 $.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a weak learner $ h_m(x) $ (often a decision tree) to these pseudo-residuals by minimizing the loss:\n",
    "$$ h_m(x) = \\arg\\min_h \\sum_{i=1}^N L(r_{im}, h(x_i)) $$\n",
    "\n",
    "**Step 2-3: Compute Terminal Node Values**\n",
    "\n",
    "- For each terminal node $ j $ in the tree $ h_m $, compute the optimal value $ \\gamma_{jm} $ that minimizes the loss:\n",
    "$$ \\gamma_{jm} = \\arg\\min_\\gamma \\sum_{x_i \\in R_{jm}} L(r_{im}, \\gamma) $$\n",
    "where $ R_{jm} $ is the region corresponding to terminal node $ j $ of the $ m $-th tree.\n",
    "\n",
    "For logistic loss, $ \\gamma_{jm} $ can be approximated as:\n",
    "$$ \\gamma_{jm} = \\frac{\\sum_{x_i \\in R_{jm}} r_{im}}{\\sum_{x_i \\in R_{jm}} (1 - r_{im})} $$\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted weak learner, scaled by a learning rate $ \\eta $:\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $ M $ iterations, the final boosted model $ F_M(x) $ is:\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Expanded Explanation of $ F_M(x) $\n",
    "\n",
    "Let's expand on each part of the equation $ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $:\n",
    "\n",
    "- **Initial Prediction $ F_0(x) $**: This is the starting point of the model, often set to the log-odds of the positive class for binary classification. It provides a baseline prediction.\n",
    "\n",
    "$$ F_0(x) = \\frac{1}{2} \\log\\left(\\frac{p}{1-p}\\right) $$\n",
    "\n",
    "- **Iterative Improvement $ \\sum_{m=1}^M \\eta h_m(x) $**: This is the sum of the predictions from all the weak learners, each scaled by the learning rate $ \\eta $. Each weak learner $ h_m(x) $ is trained to correct the errors (residuals) of the previous ensemble.\n",
    "\n",
    "$$ \\sum_{m=1}^M \\eta h_m(x) = \\eta h_1(x) + \\eta h_2(x) + \\ldots + \\eta h_M(x) $$\n",
    "\n",
    "Each term $ \\eta h_m(x) $ represents the contribution of the $ m $-th tree, scaled by the learning rate. The learning rate $ \\eta $ ensures that the model does not overfit by controlling the step size of each update.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "To provide a concrete example, consider the first few iterations of the gradient boosting process for a binary classification task:\n",
    "\n",
    "1. **Initial Prediction $ F_0(x) $**:\n",
    "   - Suppose we have the target values $ y = [1, 0, 1, 0] $.\n",
    "   - The initial log-odds prediction: $ F_0(x) = \\frac{1}{2} \\log\\left(\\frac{p}{1-p}\\right) $.\n",
    "\n",
    "2. **First Iteration (m = 1)**:\n",
    "   - Compute pseudo-residuals: $ r_{i1} = y_i - p_{i0} $, where $ p_{i0} = \\sigma(F_0(x_i)) $ and $ \\sigma $ is the sigmoid function.\n",
    "   - Fit a tree $ h_1(x) $ to these residuals.\n",
    "   - Compute optimal $ \\gamma_{j1} $ for each terminal node.\n",
    "   - Update the model: $ F_1(x) = F_0(x) + \\eta h_1(x) $.\n",
    "\n",
    "3. **Second Iteration (m = 2)**:\n",
    "   - Compute new residuals: $ r_{i2} = y_i - p_{i1} $, where $ p_{i1} = \\sigma(F_1(x_i)) $.\n",
    "   - Fit a tree $ h_2(x) $ to these residuals.\n",
    "   - Compute optimal $ \\gamma_{j2} $ for each terminal node.\n",
    "   - Update the model: $ F_2(x) = F_1(x) + \\eta h_2(x) $.\n",
    "\n",
    "This process continues for $ M $ iterations, with each iteration aiming to reduce the residuals and improve the model's accuracy.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in GBM include:\n",
    "\n",
    "- **n_estimators**: Number of boosting stages (i.e., the number of trees).\n",
    "- **learning_rate**: Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **max_depth**: Maximum depth of individual trees.\n",
    "- **min_samples_split**: Minimum number of samples required to split an internal node.\n",
    "- **min_samples_leaf**: Minimum number of samples required to be at a leaf node.\n",
    "- **subsample**: Fraction of samples used for fitting individual trees. Reducing this can improve generalization.\n",
    "- **loss function**: The loss function to be minimized, such as MSE for regression or log-loss for classification.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Accuracy**: GBM often achieves high accuracy on complex datasets.\n",
    "2. **Flexibility**: Can handle various types of data and different loss functions.\n",
    "3. **Feature Importance**: Provides insights into the importance of features.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Training Time**: Can be time-consuming due to the sequential nature of training.\n",
    "2. **Overfitting**: Prone to overfitting if not properly tuned.\n",
    "3. **Complexity**: More complex than simple models and harder to interpret.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how GBM can be implemented using popular libraries like Scikit-Learn or XGBoost in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the model\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gbm.predict(X_test)\n",
    "```\n",
    "\n",
    "For regression, you would use `GradientBoostingRegressor` similarly.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Gradient Boosting Machines are a powerful and flexible tool for both regression and classification tasks. They iteratively build an ensemble of weak learners, correcting errors at each step using gradient descent. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3fd6d-85b3-4f6e-b2f8-07538f5a281b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
