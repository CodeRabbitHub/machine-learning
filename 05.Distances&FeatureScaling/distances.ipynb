{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec50d96-afa8-442d-9335-8e69482ace51",
   "metadata": {},
   "source": [
    "\n",
    "# Distance metrics\n",
    "\n",
    "Understanding distance metrics is crucial in clustering algorithms as they quantify the dissimilarity or similarity between data points, forming the foundation for cluster formation. Various distance metrics, such as Euclidean distance, Manhattan distance, and Cosine similarity, offer distinct perspectives on data point separation. The most commonly used is Eucledian distance in case numerical data types and Hamming distance for categorical data types. \n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "<center><img src=\"./imgs/euc.png\"/></center>\n",
    "\n",
    "Euclidean distance is a widely recognized metric that measures the straight-line distance between two points in a multidimensional space. It is often familiar to many due to its intuitive geometric interpretation. This distance metric can take on any positive real number or be zero. Mathematically, Euclidean distance between two points, $x$ and $y$, in an $n$-dimensional space is calculated as the square root of the sum of squares of differences between corresponding coordinates:\n",
    "\n",
    "$$ d_n(x,y) =  \\sqrt {(x_1-y_1)^2 + (x_2-y_2)^2 + (x_3-y_3)^2 + \\ldots + (x_n-y_n)^2} $$\n",
    "\n",
    "For higher dimensions, the Euclidean distance computation involves adding the squared differences between each pair of coordinates and then taking the square root. Its versatility and ease of interpretation make it a fundamental tool in various fields, particularly in clustering algorithms where it serves as a crucial component for measuring dissimilarity between data points.\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "<center><img src=\"./imgs/man.png\"/></center>\n",
    "\n",
    "Manhattan distance is a metric used to measure the absolute numerical difference between two points in space, employing Cartesian coordinates. Unlike Euclidean distance, which calculates the straight-line distance using Pythagoras theorem, Manhattan distance considers distance as the sum of the line vectors (x,y), akin to a taxi navigating among blocks of buildings.\n",
    "\n",
    "$$d_n(x,y) = \\sum_{i=1}^{n}{|(x_i-y_i)|}$$\n",
    "\n",
    "Mathematically, the Manhattan distance between two points, $x$ and $y$, in an $n$-dimensional space is computed as the sum of the absolute differences between corresponding coordinates. This metric is particularly useful in scenarios where movement is constrained to a grid-like pattern, such as city navigation or grid-based data representation.\n",
    "\n",
    "### Minkowski Distance\n",
    "\n",
    "The Minkowski distance is a generalized distance metric calculated using the following formula:\n",
    "\n",
    "$$d_n(x,y) = {\\biggl(\\sum_{i=1}^{n}|(x_i-y_i)|^p\\biggr)}^{1/p}$$\n",
    "\n",
    "This distance metric allows for manipulation by substituting different values of 'p', enabling various ways to compute the distance between two data points. Consequently, Minkowski Distance is also referred to as Lp norm distance. Common values of 'p' yield well-known distance measures:\n",
    "\n",
    "- $p = 1$: Manhattan Distance\n",
    "- $p = 2$: Euclidean Distance\n",
    "- $p = \\infty$: Chebyshev Distance\n",
    "\n",
    "The flexibility of the Minkowski distance makes it a versatile tool in diverse applications, offering different perspectives on data similarity or dissimilarity depending on the chosen 'p' value.\n",
    "\n",
    "### Hamming Distance\n",
    "\n",
    "<center><img src=\"./imgs/hamm.png\"/></center>\n",
    "\n",
    "Hamming distance is a metric employed to calculate the distances between categorical variables, often referred to as nominal variables. It quantifies the number of differences between two binary strings, reflecting the dissimilarity between categorical values. Unlike numerical variables, categorical variables lack a natural ordering, making Hamming distance particularly suitable for assessing the similarity between categorical data points.\n",
    "\n",
    "When dealing exclusively with categorical features in a dataset, Hamming distance serves as a valuable tool for measuring the similarity between two data points. However, it's essential to note that Hamming distance can only be computed when comparing vectors of equal length; comparisons between vectors of unequal lengths are not feasible.\n",
    "\n",
    "### Gower Distance\n",
    "\n",
    "Gower (1971) distance is a hybrid measure designed to accommodate both continuous and categorical data. \n",
    "\n",
    "For continuous or ordinal data features, Gower distance employs either the Manhattan distance or a ranked ordinal Manhattan distance, respectively. On the other hand, for categorical data features, it utilizes the DICE coefficient, which quantifies the similarity between two sets.\n",
    "\n",
    "The DICE coefficient is calculated as:\n",
    "\n",
    "$$DICE = \\frac{2|X\\cap Y|}{|X|+|Y|} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "where $X$ and $Y$ represent two sets, and $TP$, $FP$, and $FN$ denote True Positives, False Positives, and False Negatives, respectively.\n",
    "\n",
    "The Gower distance between a pair of points $p$ and $q$, denoted as $G_n(p,q)$, is computed as:\n",
    "\n",
    "$$G_n(p,q) = \\frac{\\sum_{i=1}^{n}W_{pqk}S_{pqk}}{\\sum_{i=1}^{n}W_{pqk}}$$\n",
    "\n",
    "Here, $S_{pqk}$ represents either the Manhattan distance or the DICE coefficient for feature $k$, while $W_{pqk}$ is a binary indicator (1 or 0) denoting the validity of feature $k$. The Gower distance is calculated as the sum of feature scores divided by the sum of feature weights, providing a comprehensive measure of dissimilarity between data points with mixed data types.\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a widely utilized metric that addresses the challenges of high dimensionality often encountered with Euclidean distance. It quantifies the similarity between two vectors by measuring the cosine of the angle between them. Notably, cosine similarity remains invariant to the magnitude of the vectors, focusing solely on their orientation.\n",
    "\n",
    "Mathematically, the cosine similarity between two vectors, $x$ and $y$, is computed as:\n",
    "\n",
    "$$d_{(x,y)} = \\cos(\\theta) = \\frac{x \\cdot y}{||x|| \\ ||y||}$$\n",
    "\n",
    "Here, $x \\cdot y$ represents the dot product of the vectors, while $||x||$ and $||y||$ denote their respective magnitudes. \n",
    "\n",
    "Cosine similarity finds extensive application in scenarios involving high-dimensional data, where the magnitude of vectors is insignificant. In text analyses, for instance, it is commonly employed when data is represented by word counts, allowing for effective comparison and similarity assessment.\n",
    "\n",
    "### Haversine Distance\n",
    "\n",
    "<center><img src=\"./imgs/hav.png\"/></center>\n",
    "\n",
    "Haversine distance calculates the distance between two points on a sphere, typically represented by their longitudes and latitudes. Similar to Euclidean distance, it determines the shortest line connecting two points. However, unlike Euclidean distance, Haversine distance accounts for the curvature of the Earth's surface, assuming that the two points are positioned on a sphere.\n",
    "\n",
    "The formula for calculating Haversine distance between two points is:\n",
    "\n",
    "$$ d = 2\\ r\\ \\sin^{-1}\\biggl(\\sqrt{\\sin^2\\biggl(\\frac{\\phi_2-\\phi_1}{2}\\biggr) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\biggl(\\frac{\\lambda_2-\\lambda_1}{2}\\biggr)}\\biggr)$$\n",
    "\n",
    "Here, $r$ denotes the radius of the sphere, $\\phi_1$ and $\\phi_2$ represent the latitudes of the two points, and $\\lambda_1$ and $\\lambda_2$ represent their longitudes.\n",
    "\n",
    "One notable disadvantage of Haversine distance is its assumption that the points lie on a perfect sphere. In reality, the Earth's shape is more complex, which can lead to inaccuracies in distance calculations, especially over long distances or in regions with significant topographical variations.\n",
    "\n",
    "### Jaccard Distance\n",
    "\n",
    "<center><img src=\"./imgs/jac.png\"/></center>\n",
    "\n",
    "The Jaccard index, also known as the Intersection over Union, serves as a metric for assessing the similarity and diversity of sample sets. It quantifies the similarity between two sets by measuring the ratio of the size of their intersection to the size of their union. In essence, it represents the proportion of common entities between sets relative to their total number of entities.\n",
    "\n",
    "The Jaccard index, denoted as $J(x,y)$, is calculated as:\n",
    "\n",
    "$$J(x,y) = \\frac{|x \\cap y|}{|x \\cup y|}$$\n",
    "\n",
    "To derive the Jaccard distance, we subtract the Jaccard index from 1:\n",
    "\n",
    "$$D_{(x,y)} = 1 - J(x,y) = 1 - \\frac{|x \\cap y|}{|x \\cup y|}$$\n",
    "\n",
    "One notable drawback of the Jaccard index is its sensitivity to dataset size. Large datasets can disproportionately impact the index, potentially skewing results by inflating the union while maintaining a relatively constant intersection. Despite this limitation, the Jaccard index finds widespread application in scenarios involving binary or binarized data, such as image segmentation accuracy assessment in deep learning models or text similarity analysis. It enables the comparison of sets of patterns, facilitating tasks ranging from image analysis to natural language processing.\n",
    "\n",
    "\n",
    "# Importance of Feature Scaling:\n",
    "\n",
    "Feature scaling is crucial as it ensures that all features have the same scale. Many machine learning algorithms, including KNN, rely on distance-based metrics to assess the similarity between data points. When features are on different scales, the algorithm may assign more weight to features with larger scales, potentially resulting in biased or inaccurate outcomes.\n",
    "\n",
    "Let's consider an example of distance calculation using two features whose magnitudes/ranges vary greatly:\n",
    "\n",
    "$$\\text{Euclidean distance} = \\sqrt{(820000 - 325000)^2 - (3.75 - 0.50)^2}$$\n",
    "\n",
    "From the above equation, it's evident that features with high magnitudes contribute significantly more than those with lower magnitudes. Thus, normalizing the data to a range of 0-1 is recommended for better results.\n",
    "\n",
    "One common method to achieve this normalization is by using sklearn's MinMaxScaler:\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$\n",
    "\n",
    "**Note:** Standardization is not employed here because it doesn't assume data to follow any specific distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a845c34-1191-4aed-b608-359bfcc261ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
