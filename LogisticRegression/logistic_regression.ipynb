{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3970ea06-f8f0-4279-a442-818f0a569dc1",
   "metadata": {},
   "source": [
    "# Mastering Logistic Regression: Unveiling the Power of Binary Classification\n",
    "\n",
    "Logistic Regression is a crucial tool for handling binary outcomes, such as yes/no or true/false responses. In the world of classification tasks, this supervised learning method shines brightly, providing deep insights into estimating probabilities. Unlike linear regression, which deals with continuous values, logistic regression focuses on predicting the chances of a particular outcome based on input features.\n",
    "\n",
    "## Unveiling the Mechanism:\n",
    "\n",
    "The process begins much like linear regression, where input features are harmonized like instruments in a symphony, each carrying its weight determined by specific coefficients:\n",
    "\n",
    "$$ z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_n x_n $$\n",
    "\n",
    "Here, $ z $ represents the core of linear combination, with $ \\theta_0, \\theta_1, \\ldots, \\theta_n $ acting as the orchestrators, directing each input feature $ x_1, x_2, \\ldots, x_n $.\n",
    "\n",
    "However, the appeal of logistic regression lies in its transformative capability. In contrast to linear regression, logistic regression embraces the sigmoidal curve of the logistic function, shaping $ z $ into a domain of probabilities.:\n",
    "\n",
    "$$ y^p = h(x) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n)}} $$\n",
    "\n",
    "In this context, $ y^p $ emerges as the messenger of predicted probabilities, envisioning the probability of the positive class (for example, rain) among the array of input features $ x_1, x_2, \\ldots, x_n $. The sigmoidal curve ensures a confined range, encapsulating outputs between 0 and 1, making them interpretable as probabilities.\n",
    "\n",
    "#### Embracing the Probability Realm:\n",
    "\n",
    "Once the veil of probability $ y^p $ falls upon the scene, it invites interpretation, disclosing the probability of the positive outcome within the narrative. For example, a declaration of $ y^p = 0.8 $ unveils a tale of 80% certainty, depicting the potential occurrence of the positive outcome (like rain) amidst the murmurs of input features.\n",
    "\n",
    "\n",
    "## Log Loss: Understanding the Crucial Metric for Classification Models\n",
    "\n",
    "Log Loss stands as a cornerstone in the realm of classification model evaluation, particularly prominent in logistic regression. Its essence lies in quantifying the dissonance between actual outcomes and predicted probabilities, serving as a compass for model performance assessment.\n",
    "\n",
    "#### Unveiling the Formula:\n",
    "\n",
    "The heart of Log Loss resides in a concise yet powerful formula:\n",
    "\n",
    "$$ J(W) = -y \\cdot \\log(y^p) - (1-y) \\cdot \\log(1-y^p) $$\n",
    "\n",
    "Here, $y$ signifies the actual outcome, while $y^p$ denotes the predicted probability.\n",
    "\n",
    "#### Decoding Log Loss:\n",
    "\n",
    "The essence of Log Loss reveals itself in its capacity to impose a stringent penalty for incorrect predictions, guiding models towards precision and accuracy. Let's examine its influence through examples:\n",
    "\n",
    "**1. Precision in Prediction:**\n",
    "   - When the actual outcome $y$ aligns with the model's prediction $y^p \\approx 1$:\n",
    "     - As Log Loss $J(W)$ approaches 0, it signals optimal performance:\n",
    "     - Mathematically:\n",
    "       $$ y = 1 \\quad \\text{and} \\quad y^p \\approx 1 \\implies J(W) \\approx 0 $$\n",
    "\n",
    "     Let's assume $ y = 1 $ (actual outcome) and $ y^p = 0.95 $ (predicted probability).  \n",
    "     - Using the Log Loss formula: $ J(W) = -y \\cdot \\log(y^p) - (1-y) \\cdot \\log(1-y^p) $  \n",
    "     - Substituting the values: $ J(W) = -(1) \\cdot \\log(0.95) - (1-1) \\cdot \\log(1-0.95) $  \n",
    "     - Calculating: $ J(W) \\approx 0.051 $ (approximately)\n",
    "\n",
    "**2. Error Amplification:**\n",
    "   - Conversely, when predictions stray from reality:\n",
    "     - Log Loss amplifies proportionally, signifying the magnitude of misjudgment.\n",
    "     - Mathematically:\n",
    "       $$ y = 1 \\quad \\text{but} \\quad y^p \\neq 1 \\implies J(W) \\text{ escalates significantly} $$\n",
    "   \n",
    "     Let's assume $ y = 1 $ (actual outcome) and $ y^p = 0.2 $ (predicted probability).  \n",
    "     - Using the Log Loss formula: $ J(W) = -y \\cdot \\log(y^p) - (1-y) \\cdot \\log(1-y^p) $  \n",
    "     - Substituting the values: $ J(W) = -(1) \\cdot \\log(0.2) - (1-1) \\cdot \\log(1-0.2) $  \n",
    "     - Calculating: $ J(W) \\approx 1.609 $ (approximately)  \n",
    "\n",
    "**3. Sensitivity to Misclassifications:**\n",
    "   - Extending its sensitivity to all realms, including when $y = 0 $:\n",
    "     - Accurate predictions (where $y^p \\approx 0 $ entail minimal Log Loss.\n",
    "     - However, misclassifications (where $ y^p \\approx 1 $) incur substantial Log Loss.\n",
    "     - Mathematically:\n",
    "       $$ y = 0 \\quad \\text{and} \\quad y^p \\approx 0 \\implies J(W) \\approx 0 $$\n",
    "       $$ y = 0 \\quad \\text{but} \\quad y^p \\approx 1 \\implies J(W) \\rightarrow \\infty $$\n",
    "\n",
    "     Let's assume $ y = 0 $ (actual outcome) and $ y^p = 0.05 $ (predicted probability).  \n",
    "     - Using the Log Loss formula: $ J(W) = -y \\cdot \\log(y^p) - (1-y) \\cdot \\log(1-y^p) $\n",
    "     - Substituting the values: $ J(W) = -(0) \\cdot \\log(0.05) - (1-0) \\cdot \\log(1-0.05) $\n",
    "     - Calculating: $ J(W) \\approx 0.051 $ (approximately)\n",
    "    \n",
    "     Let's assume $ y = 0 $ (actual outcome) and $ y^p = 0.95 $ (predicted probability).  \n",
    "     - Using the Log Loss formula: $ J(W) = -y \\cdot \\log(y^p) - (1-y) \\cdot \\log(1-y^p) $\n",
    "     - Substituting the values: $ J(W) = -(0) \\cdot \\log(0.95) - (1-0) \\cdot \\log(1-0.95) $\n",
    "     - Calculating: $ J(W) \\rightarrow \\infty $ (approximately)\n",
    "\n",
    "#### Incentivizing Precision:\n",
    "\n",
    "In essence, Log Loss becomes a beacon for model refinement, compelling it to yield accurate probabilities for each class. By penalizing deviations from ground truth, Log Loss propels classification models towards enhanced performance, ushering in an era of precision-driven analytics.\n",
    "\n",
    "## Gradient descent for parameter optimisation \n",
    "Begin by evaluating the log loss function $ J(W) $ utilizing the predicted probabilities alongside the actual labels. Derive the gradients of the loss function concerning each parameter $ \\theta_j $ using the subsequent partial derivative formula:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (y^p - y) \\cdot x_j^{(i)}$$\n",
    "\n",
    "Subsequently, update each parameter $ \\theta_j $ by incorporating the gradients calculated in the prior step, alongside the learning rate $ \\alpha $, via the following formula:\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial J(W)}{\\partial \\theta_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06e9db-a8b6-49d0-ad9c-5f0379805916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
