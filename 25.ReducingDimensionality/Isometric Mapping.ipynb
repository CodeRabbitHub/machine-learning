{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6689e170-8d62-4812-9bac-f27caf438b6f",
   "metadata": {},
   "source": [
    "## Isometric Mapping\n",
    "\n",
    "Isomap, short for Isometric Mapping, is a dimensionality reduction technique used in machine learning and data analysis. It is particularly useful for nonlinear dimensionality reduction, aiming to preserve the intrinsic geometric structure of high-dimensional data in a lower-dimensional space. Let's delve into its workings, mathematical formulations, use cases, advantages, disadvantages, and underlying assumptions.\n",
    "\n",
    "###  Mathematical Formulation:\n",
    "\n",
    "#### 1. Nearest Neighbor Graph Construction:\n",
    "   - Given a dataset $X$ with $N$ data points, the first step involves constructing a nearest neighbor graph. For each data point $x_i$, find its $k$ nearest neighbors using a distance metric $d(x_i, x_j)$, typically Euclidean distance.\n",
    "\n",
    "#### 2. Geodesic Distance Estimation:\n",
    "   - Calculate the geodesic distance between every pair of data points using shortest path algorithms like Dijkstra's algorithm, considering the nearest neighbor graph as a weighted graph.\n",
    "\n",
    "#### 3. Isomap Embedding:\n",
    "   - Perform classical multidimensional scaling (MDS) on the matrix of geodesic distances to obtain a low-dimensional representation of the data. Classical MDS seeks a low-dimensional embedding that preserves the pairwise distances as much as possible.  \n",
    "\n",
    "   The classical MDS algorithm can be summarized as follows:\n",
    "   #### 3.1. Centering the Distance Matrix:\n",
    "   Subtract the row and column means from the matrix $D$ to obtain the centered matrix $B$:\n",
    "   $$B = -\\frac{1}{2}(D - 1_N D - D 1_N + 1_N D 1_N)$$\n",
    "   Where $1_N$ is the $N \\times N$ matrix of ones.\n",
    "   \n",
    "   #### 3.2. Eigenvalue Decomposition:\n",
    "   Compute the eigenvalue decomposition of $B$:\n",
    "   $$B = V \\Lambda V^T$$\n",
    "   Where $V$ is an $N \\times d$ matrix of eigenvectors corresponding to the largest $d$ eigenvalues, and $\\Lambda$ is a diagonal matrix of these eigenvalues.\n",
    "   #### 3.3. Embedding Coordinates:\n",
    "   The coordinates of the data points in the lower-dimensional space ($Y$) are given by the product of the matrix $V$ and the square root of the\n",
    "   diagonal matrix $\\Lambda$:\n",
    "   $$Y = V \\Lambda^{1/2}$$\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a dataset of 2D points forming a Swiss roll shape in a high-dimensional space. Isomap aims to unfold this Swiss roll into a lower-dimensional space while preserving its intrinsic geometry.\n",
    "\n",
    "### When to Use Isomap:\n",
    "\n",
    "   - **Nonlinear Manifold Learning**: When the underlying structure of data is assumed to lie on a nonlinear manifold.\n",
    "   - **Preservation of Global Structure**: When it's crucial to preserve the global structure of data, especially for visualization purposes.\n",
    "\n",
    "### How to Use Isomap:\n",
    "\n",
    "   - **Data Preprocessing**: Standardize or normalize data if needed.\n",
    "   - **Parameter Tuning**: Choose the number of nearest neighbors ($k$) and the desired output dimensionality.\n",
    "   - **Fit and Transform**: Fit the Isomap model on the dataset and transform it into the lower-dimensional space.\n",
    "\n",
    "### Advantages of Isomap:\n",
    "\n",
    "   - **Preservation of Nonlinear Structure**: Capable of capturing the nonlinear relationships among data points.\n",
    "   - **Global Structure Preservation**: Maintains the global structure of the dataset, unlike local methods such as PCA.\n",
    "   - **Robustness**: Relatively robust to noise and outliers due to its focus on preserving intrinsic geometry.\n",
    "\n",
    "### Disadvantages of Isomap:\n",
    "\n",
    "   - **Computational Complexity**: Building the nearest neighbor graph and computing geodesic distances can be computationally expensive, especially for large datasets.\n",
    "   - **Sensitivity to $k$**: The choice of the number of nearest neighbors ($k$) can significantly impact the results.\n",
    "   - **Dimensionality Curse**: Like many dimensionality reduction techniques, Isomap suffers from the curse of dimensionality when dealing with very high-dimensional data.\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "   - **Manifold Assumption**: Assumes that the high-dimensional data lies on a lower-dimensional manifold embedded in the original space.\n",
    "   - **Local Linearity**: Assumes that the local neighborhoods in the high-dimensional space are approximately linear.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Isomap is a powerful tool for dimensionality reduction, particularly suitable for datasets with nonlinear structures. By preserving the intrinsic geometry of data, it helps in visualizing and understanding high-dimensional datasets. However, it requires careful parameter tuning and can be computationally intensive. Understanding its assumptions and limitations is crucial for its effective application in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39e5b2-ef89-4e9f-b22f-a3443bb75f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
