{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2686d6a-8a47-42e0-927d-8f9109b0019c",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "In the world of machine learning, the **K-Nearest Neighbors (KNN)** algorithm is a really important tool. It's great for figuring out if something belongs to a certain category or predicting a value. KNN works by looking at how similar things are to each other in a dataset. If two things share a lot of features, they're probably close to each other. KNN uses this idea to make predictions.\n",
    "\n",
    "What makes KNN special is that it doesn't need to learn from data beforehand like many other algorithms. It's like a lazy learner; it doesn't put in the work until it has to make a prediction. This saves a lot of time and makes it faster.\n",
    "\n",
    "Another cool thing about KNN is that it doesn't assume anything about the data. It doesn't have a set way of looking at the information. This means it can handle all sorts of different data without needing to change how it works. That's why people call it a \"non-parametric learning algorithm.\"\n",
    "\n",
    "Overall, KNN is a simple yet powerful tool in machine learning. It's easy to understand and can handle lots of different tasks without needing much setup. That's why it's used in so many different areas of study and work.\n",
    "\n",
    "## Understand the K-nearest neighbors (KNN) Algorithm\n",
    "\n",
    "<center><img src=\"./imgs/knn.png\"/></center>\n",
    "\n",
    "The K-nearest neighbors (KNN) algorithm utilizes 'feature similarity' to make predictions for new data points. This means that the algorithm assigns a value to a new data point based on how closely it resembles the points in the training set. The algorithm operates through the following steps:\n",
    "\n",
    "1. **Data Loading**: Begin by loading both the training and test data sets.\n",
    "\n",
    "2. **Choosing K**: Determine the value of K, which represents the number of nearest data points. K can be any positive integer.\n",
    "\n",
    "3. **Distance Calculation**: For each point in the test data set, the algorithm calculates the distance between that point and every row in the training data set. The distance can be computed using various methods such as Euclidean, Manhattan, or Hamming distance. Typically, Euclidean distance is preferred.\n",
    "\n",
    "4. **Sorting**: Sort the calculated distances in ascending order.\n",
    "\n",
    "5. **Selecting Neighbors**: Choose the top K rows from the sorted array, representing the nearest neighbors to the test point.\n",
    "\n",
    "6. **Assigning Value**: Finally, assign a class or value to the test point based on the most frequent class or average value among these K nearest neighbors.\n",
    "\n",
    "\n",
    "\n",
    "## How to Choose the Right Value for K?\n",
    "\n",
    "Choosing the right value for K in the K-nearest neighbors (KNN) algorithm is crucial for making accurate predictions. Here's a simple guide on how to pick the best K value:\n",
    "\n",
    "1. **Try Different K Values**: Run the KNN algorithm with various K values. This helps you see how well the algorithm performs with different settings.\n",
    "\n",
    "2. **Reduce Errors on Test Data**: Select the K value that gives the fewest errors on your test data. This means choosing the K value that gives the most accurate predictions for your specific dataset.\n",
    "\n",
    "When deciding on the best K value, remember these points:\n",
    "\n",
    "- **Prediction Stability**: If you set K to 1, your predictions might be less reliable. This happens because if there's a wrong class close to your data point, it could lead to a wrong prediction.\n",
    "\n",
    "- **Precision vs. Stability**: Increasing K makes predictions more stable because it uses a majority or average vote. This often leads to more accurate predictions, but if you increase K too much, you might start seeing more errors, which suggests overfitting.\n",
    "\n",
    "- **Odd K Values for Voting**: If you're using majority voting, it's better to choose an odd K value. This helps avoid tie situations.\n",
    "\n",
    "To prevent your model from being too simple or too complex, follow these tips:\n",
    "\n",
    "- **Start with a Random K Value**: Begin with any K value and then adjust it based on how well your model performs.\n",
    "\n",
    "- **Think About Decision Boundaries**: A small K value can lead to unstable decision boundaries, while a larger K value can make them smoother, often resulting in better predictions.\n",
    "\n",
    "- **Analyze Error Rates**: Plot the error rate against different K values. Choose the K value that gives the lowest error rate, indicating the best performance.\n",
    "\n",
    "By following these steps, you can find the best K value for your KNN model, ensuring accurate predictions for your dataset.\n",
    "\n",
    "\n",
    "## Pros and Cons of KNN\r\n",
    "\r\n",
    "**Pros:**\r\n",
    "\r\n",
    "- **Simplicity**: K-nearest neighbors (KNN) is straightforward and easy to understand. It doesn't require complex mathematical formulas or assumptions about the data.\r\n",
    "\r\n",
    "- **No Assumptions**: Unlike some algorithms, KNN doesn't make any assumptions about the underlying distribution of the data. It works well even if the data is not linearly separable or doesn't follow a specific pattern.\r\n",
    "\r\n",
    "- **Versatility**: KNN is versatile and can be applied to both multi-class classification and regression problems with ease. It can handle various types of data, including numerical and categorical variables.\r\n",
    "\r\n",
    "**Cons:**\r\n",
    "\r\n",
    "- **Slow Performance with Large Data**: As the number of data points increases, KNN's computational complexity grows, making it slower compared to other algorithms. It needs to calculate the distance between the test point and all training points, which becomes time-consuming for large datasets.\r\n",
    "\r\n",
    "- **Memory Inefficiency**: KNN requires storing all training data in memory, which can be memory-intensive, especially for datasets with a large number of dimensions or features.\r\n",
    "\r\n",
    "- **Sensitive to Outliers**: KNN is sensitive to outliers, as they can significantly affect the distance calculation and thus the prediction. Outliers that are far from most data points can distort the decision boundaries and lead to less accurate predictions. Therefore, data preprocessing to handle outliers is crucial\n",
    "\n",
    "## Applications of KNN\r\n",
    "\r\n",
    "- **Finance**: Generating credit ratings of customers based on their financial behavior and history.\r\n",
    "\r\n",
    "- **Healthcare**: Classifying patients as having a disease or being healthy based on their medical records and symptoms.\r\n",
    "\r\n",
    "- **Political Science**: Predicting potential voters and classifying whether they are likely to vote or not, based on demographic and behavioral data.\r\n",
    "\r\n",
    "- **Handwriting Detection**: Classifying different styles of handwriting, such as cursive, print, or calligraphy.\r\n",
    "\r\n",
    "- **Image Recognition**: Classifying different categories of images, such as identifying objects, animals, or people in photographs.\r\n",
    " when using KNN.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1c4b6-ed54-4bbf-9500-774c87ec70bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
