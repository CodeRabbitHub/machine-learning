{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a95412d-3607-4fe5-8ca7-c9da51030650",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful statistical technique used for dimensionality reduction in machine learning and data science. It transforms high-dimensional data into a lower-dimensional form while preserving as much variability (information) as possible.\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "#### 1. **Data Standardization**\n",
    "The first step in PCA is to standardize the data. Standardization involves subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Given a dataset $X$ with $n$ observations and $p$ features:\n",
    "$$ X_{std} = \\frac{X - \\mu}{\\sigma} $$\n",
    "where $\\mu$ is the mean vector and $\\sigma$ is the standard deviation vector of the dataset.\n",
    "\n",
    "#### 2. **Covariance Matrix Computation**\n",
    "Next, compute the covariance matrix of the standardized data to understand how variables are correlated with each other.\n",
    "$$ \\Sigma = \\frac{1}{n-1} X_{std}^T X_{std} $$\n",
    "The covariance matrix $\\Sigma$ is a $p \\times p$ matrix, where each element $\\sigma_{ij}$ represents the covariance between feature $i$ and feature $j$.\n",
    "\n",
    "#### 3. **Eigenvalues and Eigenvectors**\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues ($\\lambda_i$) give the magnitude of the variance in the direction of the corresponding eigenvector ($v_i$).\n",
    "\n",
    "$$ \\Sigma v_i = \\lambda_i v_i $$\n",
    "\n",
    "#### 4. **Sorting Eigenvalues and Eigenvectors**\n",
    "Sort the eigenvalues in descending order and arrange the corresponding eigenvectors to form a new basis. The eigenvectors corresponding to the largest eigenvalues capture the most variance in the data.\n",
    "\n",
    "#### 5. **Projection onto Principal Components**\n",
    "Select the top $k$ eigenvectors (principal components) and project the standardized data onto these vectors.\n",
    "\n",
    "$$ X_{PCA} = X_{std} W_k $$\n",
    "where $W_k$ is a $p \\times k$ matrix containing the top $k$ eigenvectors.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose we have a dataset with two features: height and weight. Our goal is to reduce this to a single dimension.\n",
    "\n",
    "1. **Standardize the Data**\n",
    "\n",
    "    | Height (cm) | Weight (kg) |\n",
    "    |-------------|-------------|\n",
    "    | 170         | 70          |\n",
    "    | 180         | 80          |\n",
    "    | 160         | 60          |\n",
    "\n",
    "    After standardizing:\n",
    "\n",
    "    | Height (std) | Weight (std) |\n",
    "    |--------------|--------------|\n",
    "    | -0.267       | -0.267       |\n",
    "    | 1.069        | 1.069        |\n",
    "    | -0.801       | -0.801       |\n",
    "\n",
    "2. **Compute Covariance Matrix**\n",
    "\n",
    "    $$\n",
    "    \\Sigma = \\begin{bmatrix}\n",
    "    1.0 & 1.0 \\\\\n",
    "    1.0 & 1.0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**\n",
    "\n",
    "    Eigenvalues: $\\lambda_1 = 2, \\lambda_2 = 0$\n",
    "\n",
    "    Eigenvectors: $v_1 = \\frac{1}{\\sqrt{2}}[1, 1]^T, v_2 = \\frac{1}{\\sqrt{2}}[-1, 1]^T$\n",
    "\n",
    "4. **Projection**\n",
    "\n",
    "    Project the data onto the first principal component $v_1$:\n",
    "\n",
    "    $$\n",
    "    X_{PCA} = X_{std} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The result is a one-dimensional representation of the original data.\n",
    "\n",
    "### When to Use PCA\n",
    "\n",
    "- **High-dimensional data**: When dealing with datasets with many features, PCA helps in reducing the dimensionality.\n",
    "- **Data visualization**: PCA can reduce data to 2D or 3D for visualization purposes.\n",
    "- **Noise reduction**: PCA can help in removing noise from the data by retaining only the principal components with significant variance.\n",
    "\n",
    "### How to Use PCA\n",
    "\n",
    "1. **Standardize the data** to ensure each feature contributes equally.\n",
    "2. **Compute the covariance matrix** of the standardized data.\n",
    "3. **Calculate eigenvalues and eigenvectors** of the covariance matrix.\n",
    "4. **Sort eigenvalues and select the top k eigenvectors**.\n",
    "5. **Transform the data** by projecting it onto the selected eigenvectors.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Simplifies the complexity** of high-dimensional data.\n",
    "- **Improves computational efficiency** by reducing the number of dimensions.\n",
    "- **Helps in data visualization** by reducing data to 2D or 3D.\n",
    "- **Removes correlated features**, thus reducing multicollinearity.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Loss of interpretability**: Principal components are linear combinations of original features, which may not be easily interpretable.\n",
    "- **Not suitable for non-linear data**: PCA assumes linear relationships among variables.\n",
    "- **Sensitivity to scaling**: PCA requires standardized data to ensure each feature contributes equally.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "1. **Linearity**: PCA assumes linear relationships among variables.\n",
    "2. **Large variances have important structure**: PCA assumes that components with larger variances represent more significant information.\n",
    "3. **Orthogonality of principal components**: PCA assumes that principal components are orthogonal (uncorrelated).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "PCA is a versatile tool for dimensionality reduction, especially useful in high-dimensional datasets. By transforming data into a new basis defined by the principal components, PCA retains the most significant variance in fewer dimensions, aiding in visualization, noise reduction, and improving computational efficiency. However, it is important to be aware of its limitations and assumptions when applying it to real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169d8fc-5397-4268-8487-9501ebea675f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
