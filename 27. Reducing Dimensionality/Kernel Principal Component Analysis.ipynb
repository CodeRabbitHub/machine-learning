{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ba8fce-efd3-446b-9485-3b1dce00102b",
   "metadata": {},
   "source": [
    "## Kernel Principal Component Analysis (Kernel PCA)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) is an extension of Principal Component Analysis (PCA) that allows for nonlinear dimensionality reduction. By using kernel methods, Kernel PCA can capture complex structures in the data that linear PCA cannot.\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "#### 1. **Kernel Trick**\n",
    "\n",
    "The core idea of Kernel PCA is to map the input data into a higher-dimensional feature space where linear PCA can be applied. This is achieved through a kernel function $ k(x, y) $ that computes the dot product in the feature space without explicitly mapping the data points.\n",
    "\n",
    "Given a dataset $ X = \\{x_1, x_2, \\ldots, x_n\\} $ with $ n $ samples, the kernel function $ k $ defines the similarity between pairs of data points:\n",
    "\n",
    "$$ k(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) $$\n",
    "\n",
    "where $ \\phi $ is the mapping function to the higher-dimensional feature space.\n",
    "\n",
    "#### 2. **Kernel Matrix**\n",
    "\n",
    "Construct the kernel matrix $ K $ using the kernel function:\n",
    "\n",
    "$$ K_{ij} = k(x_i, x_j) $$\n",
    "\n",
    "#### 3. **Centering the Kernel Matrix**\n",
    "\n",
    "Center the kernel matrix $ K $ to ensure that the mapped data is centered in the feature space:\n",
    "\n",
    "$$ \\tilde{K} = K - 1_n K - K 1_n + 1_n K 1_n $$\n",
    "\n",
    "where $ 1_n $ is an $ n \\times n $ matrix with all elements equal to $ \\frac{1}{n} $.\n",
    "\n",
    "#### 4. **Eigenvalue Decomposition**\n",
    "\n",
    "Perform eigenvalue decomposition on the centered kernel matrix $ \\tilde{K} $:\n",
    "\n",
    "$$ \\tilde{K} v_i = \\lambda_i v_i $$\n",
    "\n",
    "where $ \\lambda_i $ are the eigenvalues and $ v_i $ are the corresponding eigenvectors.\n",
    "\n",
    "#### 5. **Principal Components in Feature Space**\n",
    "\n",
    "The principal components in the feature space are obtained by projecting the original data onto the eigenvectors:\n",
    "\n",
    "$$ y_i = \\sum_{j=1}^n \\alpha_j k(x_i, x_j) $$\n",
    "\n",
    "where $ \\alpha_j $ are the coefficients derived from the eigenvectors.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a dataset with a nonlinear structure, such as a two-dimensional dataset shaped like a spiral.\n",
    "\n",
    "1. **Kernel Function**\n",
    "\n",
    "   Choose a kernel function, such as the Radial Basis Function (RBF) kernel:\n",
    "\n",
    "   $$\n",
    "   k(x_i, x_j) = \\exp \\left( -\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2} \\right)\n",
    "   $$\n",
    "\n",
    "2. **Construct Kernel Matrix**\n",
    "\n",
    "   Calculate the kernel matrix $ K $ for all pairs of data points.\n",
    "\n",
    "3. **Center Kernel Matrix**\n",
    "\n",
    "   Center the kernel matrix $ K $ to obtain $ \\tilde{K} $.\n",
    "\n",
    "4. **Eigenvalue Decomposition**\n",
    "\n",
    "   Perform eigenvalue decomposition on $ \\tilde{K} $ to obtain eigenvalues $ \\lambda_i $ and eigenvectors $ v_i $.\n",
    "\n",
    "5. **Compute Principal Components**\n",
    "\n",
    "   Project the data onto the eigenvectors to obtain the principal components in the feature space.\n",
    "\n",
    "### When to Use Kernel PCA\n",
    "\n",
    "- **Nonlinear dimensionality reduction**: When the data has a complex, nonlinear structure that linear PCA cannot capture.\n",
    "- **Preprocessing**: To transform data into a lower-dimensional space before applying other machine learning algorithms.\n",
    "- **Feature extraction**: To extract meaningful features that capture the underlying structure of the data.\n",
    "\n",
    "### How to Use Kernel PCA\n",
    "\n",
    "1. **Choose a kernel function**: Select a kernel function (e.g., RBF, polynomial) that suits the data.\n",
    "2. **Compute the kernel matrix**: Calculate the kernel matrix using the chosen kernel function.\n",
    "3. **Center the kernel matrix**: Center the kernel matrix to ensure the data is centered in the feature space.\n",
    "4. **Perform eigenvalue decomposition**: Decompose the centered kernel matrix to obtain eigenvalues and eigenvectors.\n",
    "5. **Project data**: Compute the principal components by projecting the data onto the eigenvectors.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Nonlinear relationships**: Captures complex, nonlinear relationships in the data.\n",
    "- **Flexibility**: A variety of kernel functions can be used to suit different types of data.\n",
    "- **Improved performance**: Often leads to better performance in downstream tasks compared to linear PCA.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Computational complexity**: More computationally intensive than linear PCA, especially for large datasets.\n",
    "- **Kernel selection**: The choice of kernel function and parameters can significantly affect the results.\n",
    "- **Interpretability**: The resulting components may be less interpretable than those from linear PCA.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Kernel function**: Assumes an appropriate kernel function can be chosen to map the data into a feature space where linear PCA is effective.\n",
    "- **Nonlinear structure**: Assumes that the data has a nonlinear structure that can be captured by the chosen kernel.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) extends the capabilities of traditional PCA by enabling nonlinear dimensionality reduction. Through the use of kernel functions, Kernel PCA maps data into a higher-dimensional feature space where linear techniques can reveal underlying structures. Despite its computational demands and sensitivity to kernel selection, Kernel PCA is a powerful tool for uncovering complex patterns in data, making it valuable for preprocessing, feature extraction, and enhancing the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8382ae-2517-4913-821a-3e58614a3ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
