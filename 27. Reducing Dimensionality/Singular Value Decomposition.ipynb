{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2ff623-1683-4995-ad38-c8839236dbf2",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Singular Value Decomposition (SVD) is a linear algebra technique used to factorize a matrix into three component matrices. It is widely used in machine learning, data science, and statistics for tasks such as dimensionality reduction, noise reduction, and data compression.\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "#### 1. **Matrix Decomposition**\n",
    "\n",
    "Given a matrix $A \\in \\mathbb{R}^{m \\times n}$, SVD decomposes $A$ into three matrices:\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (its columns are orthonormal eigenvectors of $AA^T$).\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative real numbers on the diagonal (singular values).\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix (its columns are orthonormal eigenvectors of $A^T A$).\n",
    "\n",
    "The singular values in $\\Sigma$ are the square roots of the eigenvalues of $A^T A$ (or $AA^T$), and they are sorted in descending order.\n",
    "\n",
    "#### 2. **Truncated SVD**\n",
    "\n",
    "For dimensionality reduction, we can approximate $A$ by truncating the SVD. We keep only the top $k$ singular values and their corresponding vectors:\n",
    "\n",
    "$$ A_k = U_k \\Sigma_k V_k^T $$\n",
    "\n",
    "where $U_k \\in \\mathbb{R}^{m \\times k}$, $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$, and $V_k \\in \\mathbb{R}^{n \\times k}$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a matrix $A$:\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "3 & 2 & 2 \\\\\n",
    "2 & 3 & -2\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "1. **Compute SVD**\n",
    "\n",
    "   Calculate $U$, $\\Sigma$, and $V^T$.\n",
    "\n",
    "   $$\n",
    "   U = \\begin{bmatrix}\n",
    "   -0.7071 & -0.7071 \\\\\n",
    "   -0.7071 & 0.7071\n",
    "   \\end{bmatrix}, \\quad\n",
    "   \\Sigma = \\begin{bmatrix}\n",
    "   5 & 0 & 0 \\\\\n",
    "   0 & 3 & 0\n",
    "   \\end{bmatrix}, \\quad\n",
    "   V^T = \\begin{bmatrix}\n",
    "   -0.7071 & -0.7071 & 0 \\\\\n",
    "   -0.7071 & 0.7071 & 0 \\\\\n",
    "   0 & 0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Truncate SVD**\n",
    "\n",
    "   For $k = 1$:\n",
    "\n",
    "   $$\n",
    "   A_1 = U_1 \\Sigma_1 V_1^T = \\begin{bmatrix}\n",
    "   -0.7071 \\\\\n",
    "   -0.7071\n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix}\n",
    "   5\n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix}\n",
    "   -0.7071 & -0.7071 & 0\n",
    "   \\end{bmatrix}\n",
    "   = \\begin{bmatrix}\n",
    "   2.5 & 2.5 & 0 \\\\\n",
    "   2.5 & 2.5 & 0\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "### When to Use SVD\n",
    "\n",
    "- **Dimensionality reduction**: To reduce the number of features in a dataset while retaining most of the variance.\n",
    "- **Noise reduction**: To filter out noise by truncating small singular values.\n",
    "- **Data compression**: To represent data in a more compact form.\n",
    "- **Latent semantic analysis**: In natural language processing to identify relationships between words and documents.\n",
    "\n",
    "### How to Use SVD\n",
    "\n",
    "1. **Compute the SVD**: Factorize the matrix $A$ into $U$, $\\Sigma$, and $V^T$.\n",
    "2. **Select the number of components $k$**: Determine the number of singular values to retain based on the desired level of variance explained.\n",
    "3. **Truncate the matrices**: Keep only the top $k$ singular values and their corresponding vectors.\n",
    "4. **Reconstruct the matrix**: Use the truncated matrices to approximate the original matrix.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Optimal low-rank approximation**: SVD provides the best low-rank approximation of a matrix in terms of Frobenius norm.\n",
    "- **Versatile**: Applicable in various fields such as image compression, recommender systems, and more.\n",
    "- **Robust to noise**: Can effectively separate signal from noise in data.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Computationally expensive**: SVD computation can be slow for very large matrices.\n",
    "- **Storage requirements**: Requires storage for three potentially large matrices.\n",
    "- **Interpretability**: Singular vectors may not be easily interpretable in some applications.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Linearity**: Assumes linear relationships among variables.\n",
    "- **Data completeness**: Requires a complete data matrix without missing values for accurate decomposition.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Singular Value Decomposition (SVD) is a fundamental technique in linear algebra with widespread applications in machine learning and data science. By decomposing a matrix into its constituent parts, SVD provides powerful tools for dimensionality reduction, noise reduction, and data compression. Despite its computational demands, SVD's ability to reveal the underlying structure of data makes it an invaluable tool for data analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6cbec-091d-403d-95cb-e4642cc9b920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
