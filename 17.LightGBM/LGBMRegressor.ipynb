{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0150d737-07b6-48a0-9fd4-21c7eada0e0e",
   "metadata": {},
   "source": [
    "## LightGBM Regressor\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is an efficient implementation of the gradient boosting framework that leverages advanced techniques to achieve high performance and scalability. It is particularly designed for fast training speed, low memory usage, and better accuracy. LightGBM is suitable for both regression and classification tasks.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, optimizing a specific loss function.\n",
    "\n",
    "#### 2. Leaf-Wise Growth\n",
    "\n",
    "Unlike traditional boosting algorithms that grow trees level-wise, LightGBM grows trees leaf-wise. This means it splits the leaf with the maximum loss reduction, leading to deeper trees and potentially better accuracy.\n",
    "\n",
    "#### 3. Histogram-Based Decision Trees\n",
    "\n",
    "LightGBM uses histogram-based algorithms to speed up the training process. Features are bucketed into discrete bins, significantly reducing the computational cost and memory usage.\n",
    "\n",
    "### Steps Involved in LightGBM Regressor\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The LightGBM process begins by initializing the model with a constant value, typically the mean of the target values $y$.\n",
    "\n",
    "For a regression task:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $L$ is the loss function, such as Mean Squared Error (MSE), and $N$ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For regression, typically squared loss is used.\n",
    "- **Initial Prediction ($F_0$):** We find $\\gamma$ that minimizes the sum of the loss function. For MSE, this $\\gamma$ turns out to be the mean of $y$.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "LightGBM constructs an ensemble of trees in a sequential manner. At each iteration $m$:\n",
    "\n",
    "**Step 2-1: Calculate Gradient and Hessian**\n",
    "\n",
    "- Compute the gradient (first derivative) and Hessian (second derivative) of the loss function with respect to the predictions:\n",
    "\n",
    "$$ g_{im} = \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "$$ h_{im} = \\left[ \\frac{\\partial^2 L(y_i, F(x_i))}{\\partial F(x_i)^2} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For squared loss, the gradient $g_{im}$ and Hessian $h_{im}$ are given by:\n",
    "\n",
    "$$ g_{im} = y_i - F_{m-1}(x_i) $$\n",
    "$$ h_{im} = 1 $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Gradient ($g_{im}$):** The gradient measures the difference between the actual and predicted values.\n",
    "- **Hessian ($h_{im}$):** The Hessian is constant (1) for squared loss, simplifying the optimization process.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a regression tree $h_m(x)$ to the gradients $g_{im}$ using weighted least squares, where weights are given by the Hessians $h_{im}$.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weighted Least Squares:** Each split is chosen to minimize the weighted sum of squared errors, taking into account both gradients and Hessians.\n",
    "\n",
    "**Step 2-3: Compute Leaf Weights**\n",
    "\n",
    "- For each leaf $j$ in the tree $h_m$, compute the optimal leaf weight $\\gamma_{jm}$ that minimizes the loss:\n",
    "\n",
    "$$ \\gamma_{jm} = - \\frac{\\sum_{i \\in R_{jm}} g_{im}}{\\sum_{i \\in R_{jm}} h_{im}} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Leaf Weight ($\\gamma_{jm}$):** This value is used to update the modelâ€™s prediction for all samples in the leaf. It is derived from the ratio of the sum of gradients to the sum of Hessians within the leaf.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted tree, scaled by a learning rate $\\eta$:\n",
    "\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($\\eta$):** This controls the contribution of each new tree to the final model, helping to prevent overfitting.\n",
    "- **Model Update:** The new prediction $F_m(x)$ is the previous prediction $F_{m-1}(x)$ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $M$ iterations, the final boosted model $F(x)$ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in LightGBM Regressor include:\n",
    "\n",
    "- **num_leaves:** Maximum number of leaves in each tree.\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **n_estimators:** Number of boosting stages (i.e., the number of trees).\n",
    "- **max_depth:** Maximum depth of individual trees.\n",
    "- **min_child_weight:** Minimum sum of instance weight needed in a child.\n",
    "- **subsample:** Fraction of samples used for fitting individual trees.\n",
    "- **colsample_bytree:** Fraction of features used for fitting individual trees.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Performance:** LightGBM often achieves high accuracy on complex datasets.\n",
    "2. **Efficiency:** Optimized for speed and memory usage with histogram-based algorithms.\n",
    "3. **Scalability:** Can handle large datasets with millions of instances and features.\n",
    "4. **Flexibility:** Can handle various types of data and different loss functions.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "3. **Sensitive to Noisy Data:** Leaf-wise growth can lead to overfitting if not properly regularized.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how LightGBM Regressor can be implemented using the LightGBM library in Python:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "lgb_regressor = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = lgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "LightGBM Regressor is a powerful and efficient boosting technique for regression tasks. By leveraging advanced techniques such as leaf-wise growth and histogram-based algorithms, it achieves high performance and scalability. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b141e-f2d6-4ac4-9325-35a5289dcbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
