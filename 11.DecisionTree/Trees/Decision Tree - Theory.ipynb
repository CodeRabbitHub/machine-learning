{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2cde0a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca3d6",
   "metadata": {},
   "source": [
    "### What is a Decision Tree and Why Do We Need It?\n",
    "\n",
    "A decision tree is a type of machine learning tool used mainly for solving **classification and regression** problems. It's straightforward and useful when dealing with complex relationships between variables. Unlike linear models, decision trees **can handle non-linear data** effectively. They're also great for explaining **how a model works** to people.\n",
    "\n",
    "### Types of Decision Trees\n",
    "\n",
    "There are two main types of decision trees based on the type of target variable they handle:\n",
    "\n",
    "- **Categorical Variable Decision Tree**: This type deals with target variables that have distinct categories like 'Yes' or 'No'.\n",
    "  \n",
    "- **Continuous Variable Decision Tree**: This type is for target variables that are continuous, like predicting someone's income based on factors such as age and occupation.\n",
    "\n",
    "### Key Terms Related to Decision Trees\n",
    "\n",
    "- **Root Node**: Represents the entire dataset which is then divided into smaller groups.\n",
    "  \n",
    "- **Splitting**: The process of dividing a node into smaller sub-nodes based on certain criteria.\n",
    "  \n",
    "- **Decision Node**: A node that splits into further nodes based on decisions.\n",
    "  \n",
    "- **Leaf/Terminal Node**: Final nodes that do not split further.\n",
    "  \n",
    "- **Pruning**: Removing unnecessary sub-nodes to simplify the tree.\n",
    "  \n",
    "- **Branch/Sub-Tree**: Sections of the tree stemming from the main structure.\n",
    "  \n",
    "- **Parent and Child Node**: Nodes that split into smaller nodes (parent) and the resulting smaller nodes (children).\n",
    "\n",
    "### Handling Numerical and Categorical Data in Decision Trees\n",
    "\n",
    "Decision trees can handle both numerical and categorical data simultaneously. Each split in a decision tree is based on a specific feature:\n",
    "\n",
    "- For categorical features, the split is based on belonging to a particular class.\n",
    "  \n",
    "- For continuous features, the split is based on values above or below a threshold.\n",
    "\n",
    "The decision tree picks the best feature to split on at each step based on how it reduces uncertainty. Whether a feature is categorical or continuous doesn't affect this process.\n",
    "\n",
    "*In practice, it's often useful to convert categorical features into a numerical form using techniques like One-Hot Encoding.*\n",
    "\n",
    "\n",
    "### How to Choose the Set of Split Points\n",
    "\n",
    "The selection of split points for a variable depends on whether the variable is numeric or categorical.\n",
    "\n",
    "#### Numeric Predictor Variables\n",
    "- **Unique Values**: If a predictor is numeric and has unique values, there are *n – 1* split points for *n* data points. However, considering all these points may be impractical due to their number. Instead, common practice involves selecting split points based on certain percentiles of the value distribution (e.g., every tenth percentile like 10%, 20%, 30%, etc.).\n",
    "  \n",
    "  **Example**: For a column of numeric values [20, 25, 30, 35, 50, 55, 70, 90] with 8 data points, average values between each pair of data points are calculated, resulting in 7 split values [22.5, 27.5, 32.5, 42.5, 52.5, 62.5, 80.0]. Information gain (IG) is then computed for each split point to select the one that maximizes IG.\n",
    "\n",
    "#### Categorical Predictor Variables\n",
    "- **Binary Splitting**: With categorical predictors, it's common to use binary splitting, creating either one child node per class (*multiway splits*) or only two child nodes (*binary split*). Binary splits are preferred because multiway splits can quickly break data into small subsets, leading to overfitting.\n",
    "\n",
    "  - **Two-Class Predictor**: If a categorical predictor has two classes, there is only one possible split.\n",
    "  \n",
    "  - **Multiple-Class Predictor**: For predictors with more than two classes:\n",
    "    - **Small Number of Classes**: Consider all possible splits into two child nodes.\n",
    "    - **Large Number of Classes**: Order classes by their average output value and make a binary split into two groups of ordered classes (resulting in k – 1 possible splits for k classes).\n",
    "\n",
    "  **Example**: For three classes like apple, banana, and orange, potential binary splits include:\n",
    "  \n",
    "  |        | child1 |     child2     |\n",
    "  |:------:|:------:|:--------------:|\n",
    "  | split1 |  apple | banana, orange |\n",
    "  | split2 | banana |  apple, orange |\n",
    "  | split3 | orange |  apple, banana |\n",
    "\n",
    "  For *k* classes, there are *2^(k-1)-1* possible splits, which can be computationally intensive for large *k*. This can bias decision trees towards splitting variables with many classes, resulting in potentially significant improvements but also increasing the risk of overfitting.\n",
    "  \n",
    "  For four classes like apple, banana, orange and grapes potential binary splits include: \n",
    "  \n",
    "  | Split   | Child 1        | Child 2            |\n",
    "  |---------|----------------|--------------------|\n",
    "  | Split 1 | Grapes, Apple  | Banana, Orange     |\n",
    "  | Split 2 | Grapes, Banana | Apple, Orange      |\n",
    "  | Split 3 | Grapes, Orange | Apple, Banana      |\n",
    "  | Split 4 | Grapes         | Apple, Banana, Orange |\n",
    "  | Split 5 | Apple          | Grapes, Banana, Orange |\n",
    "  | Split 6 | Banana         | Grapes, Apple, Orange  |\n",
    "  | Split 7 | Orange         | Grapes, Apple, Banana  |\n",
    "\n",
    "\n",
    "\n",
    "### Criteria Used to Improve the Split\n",
    "\n",
    "The criteria for improving node splits depend on whether the target variable is continuous or categorical.\n",
    "\n",
    "#### Continuous Target Variable\n",
    "- **Reduction in Sum of Squared Errors (SSE)**:\n",
    "  When the outcome is numeric, the improvement is measured by the difference in the sum of squared errors (SSE) between the node and its child nodes after the split. The squared error for a node is calculated as:\n",
    "  \n",
    "  $$ \\sum_{i=1}^{n}{(y_i - c)}^2 $$\n",
    "  \n",
    "  Where:\n",
    "  - $n$ is the number of cases at the node.\n",
    "  - $c$ is the average outcome of all cases at that node.\n",
    "  - $y_i$ is the outcome value of the $i$-th case.\n",
    "\n",
    "#### Categorical Target Variable\n",
    "- **Gini Impurity**:\n",
    "\n",
    "    Gini Impurity is a measure of how likely a randomly chosen element in a dataset is to be incorrectly classified. It is used for generating classification trees, which are decision models that split the data into different groups based on some criteria. Gini Impurity ranges from 0 to 0.5, where 0 means all elements belong to the same class and 0.5 means the elements are evenly distributed across different classes.\n",
    "\n",
    "  For categorical outcomes, the split can be based on:\n",
    "  - **Gini Impurity**: \n",
    "    $$ Gini\\ impurity = \\sum_{i=1}^{k}{p_i(1-p_i)} $$\n",
    "    Where:\n",
    "    - $k$ is the number of classes.\n",
    "    - $p_i$ is the proportion of cases belonging to class $i$.\n",
    "    \n",
    "  - **Information Gain**:\n",
    "    $$ Information\\ Gain = Entropy(Parent) - Entropy(Children) $$\n",
    "    Where:\n",
    "    - **Entropy**:\n",
    "      $$ Entropy = -\\sum_{i=1}^{k}{p_i \\log_2(p_i)} $$\n",
    "      $p_i$ is the probability of class $i$.\n",
    "    \n",
    "- **Chi-Square**:\n",
    "  Another method for categorical targets is Chi-Square, which works on the statistical significance of differences between the parent node and child nodes. The Chi-Square value for a class is calculated as:\n",
    "  \n",
    "  $$ Chi-Square = \\sqrt{\\frac{(Actual-Expected)^2}{Expected}} $$\n",
    "  \n",
    "  Where:\n",
    "  - **Expected**: Expected value for a class in a child node based on parent node distribution.\n",
    "  - **Actual**: Actual value for a class in a child node.\n",
    "\n",
    "The chosen criteria aim to minimize impurity or error after the split, resulting in more homogeneous child nodes. Each criterion assesses the effectiveness of the split in reducing uncertainty or error, guiding the decision tree towards optimal node divisions.\n",
    "\n",
    "### Illustration of Node Splitting Considering SSE or Variance for a Decision Tree\n",
    "\n",
    "Consider a dataset of 50 startups, where we want to predict the profit based on various features including categorical and continuous variables.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "The dataset contains information about startups including the amount spent on Research and Development (R&D), Administration, Marketing Spend, State of operation, and the resulting Profit.\n",
    "\n",
    "#### Sample Data\n",
    "\n",
    "| R&D Spend | Administration | Marketing Spend | State      | Profit   |\n",
    "|-----------|----------------|-----------------|------------|----------|\n",
    "| 165349    | 136897         | 471784          | New York   | 192261   |\n",
    "| 162597    | 151377         | 443898          | California | 191792   |\n",
    "| 153442    | 101145         | 407934          | Florida    | 191050   |\n",
    "| ...       | ...            | ...             | ...        | ...      |\n",
    "\n",
    "\n",
    "![Splitting](Split.drawio.png)\n",
    "\n",
    "Since the **overall_variance(R&D>=100000) < overall_variance(State==FL)**, we prefer a split based on R&D.\n",
    "\n",
    "\n",
    "\n",
    "### Illustration of Node Splitting Considering Information Gain for a Decision Tree\n",
    "\n",
    "Consider an experiment with two predictors *variable1* and *variable2* and a target variable with two outcomes: *stop* and *continue*.\n",
    "\n",
    "| variable1 | variable2 | outcome   |\n",
    "|-----------|-----------|-----------|\n",
    "| 3         | 5         | stop      |\n",
    "| 7         | 6         | continue  |\n",
    "| 3         | 3         | stop      |\n",
    "| 4         | 8         | continue  |\n",
    "| 3         | 9         | continue  |\n",
    "| 6         | 5         | stop      |\n",
    "| 5         | 8         | continue  |\n",
    "| 6         | 4         | continue  |\n",
    "\n",
    "Let's calculate the initial entropy of the root node before any splitting:\n",
    "\n",
    "$$H = -\\left(\\frac{3}{8}\\right)\\log_2\\left(\\frac{3}{8}\\right) - \\left(\\frac{5}{8}\\right)\\log_2\\left(\\frac{5}{8}\\right) = 0.954$$\n",
    "\n",
    "Now we have two options: either to choose *variable1* for split or *variable2*. To decide which variable to split on, we will calculate entropies in both cases.\n",
    "\n",
    "**Splitting with *variable1* at split point 4 (average or 50th percentile):**\n",
    "- $p_{(>4)} = \\frac{4}{8}$\n",
    "- $H_{(>4)} = -\\left(\\frac{1}{4}\\right)\\log_2\\left(\\frac{1}{4}\\right) - \\left(\\frac{3}{4}\\right)\\log_2\\left(\\frac{3}{4}\\right) = 0.81$\n",
    "\n",
    "  | variable1 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 7         | continue  |\n",
    "  | 6         | stop      |\n",
    "  | 5         | continue  |\n",
    "  | 6         | continue  |\n",
    "\n",
    "- $p_{(\\leq 4)} = \\frac{4}{8}$\n",
    "- $H_{(\\leq 4)} = -\\left(\\frac{2}{4}\\right)\\log_2\\left(\\frac{2}{4}\\right) - \\left(\\frac{2}{4}\\right)\\log_2\\left(\\frac{2}{4}\\right) = 1.0$\n",
    "\n",
    "  | variable1 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 3         | stop      |\n",
    "  | 3         | stop      |\n",
    "  | 4         | continue  |\n",
    "  | 3         | continue  |\n",
    "\n",
    "Entropy after splitting by *variable1*:\n",
    "$$H(\\text{variable1}) = -p_{(>4)}H_{(>4)} - p_{(\\leq 4)}H_{(\\leq 4)} = 0.9$$\n",
    "\n",
    "**Splitting with *variable2* at split point 6 (average or 50th percentile):**\n",
    "- $p_{(>6)} = \\frac{3}{8}$\n",
    "- $H_{(>6)} = -\\left(\\frac{3}{3}\\right)\\log_2\\left(\\frac{3}{3}\\right) - \\left(\\frac{0}{3}\\right)\\log_2\\left(\\frac{0}{3}\\right) = 0$\n",
    "\n",
    "  | variable2 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 8         | continue  |\n",
    "  | 9         | continue  |\n",
    "  | 8         | continue  |\n",
    "\n",
    "- $p_{(\\leq 6)} = \\frac{5}{8}$\n",
    "- $H_{(\\leq 6)} = -\\left(\\frac{2}{5}\\right)\\log_2\\left(\\frac{2}{5}\\right) - \\left(\\frac{3}{5}\\right)\\log_2\\left(\\frac{3}{5}\\right) = 0.971$\n",
    "\n",
    "  | variable2 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 5         | stop      |\n",
    "  | 6         | continue  |\n",
    "  | 3         | stop      |\n",
    "  | 5         | stop      |\n",
    "  | 4         | continue  |\n",
    "\n",
    "Entropy after splitting by *variable2*:\n",
    "$$H(\\text{variable2}) = - p_{(>6)}H_{(>6)} - p_{(\\leq 6)}H_{(\\leq 6)} = 0.28$$\n",
    "\n",
    "Calculate information gain after each split:\n",
    "$$IG(1) = H - H(\\text{variable1}) = 0.954 - 0.9 = 0.054$$\n",
    "$$IG(2) = H - H(\\text{variable2}) = 0.954 - 0.28 = 0.674$$\n",
    "\n",
    "We observe that if we split the node by considering *variable2*, we achieve the highest information gain. Therefore, we select *variable2* for the split and continue this process for impure nodes until we obtain pure leaf nodes for the decision tree. The following diagram shows the decision tree in this case.\n",
    "\n",
    "![Decision Tree](DecisionTree.png)\n",
    "\n",
    "\n",
    "### Illustration of Node Splitting Considering Gini Impurity for a Decision Tree\n",
    "\n",
    "Consider the same experiment data from above.\n",
    "\n",
    "| variable1 | variable2 | outcome   |\n",
    "|-----------|-----------|-----------|\n",
    "| 3         | 5         | stop      |\n",
    "| 7         | 6         | continue  |\n",
    "| 3         | 3         | stop      |\n",
    "| 4         | 8         | continue  |\n",
    "| 3         | 9         | continue  |\n",
    "| 6         | 5         | stop      |\n",
    "| 5         | 8         | continue  |\n",
    "| 6         | 4         | continue  |\n",
    "\n",
    "We have two variables for splitting. We need to calculate the weighted Gini impurity for both *variable1* and *variable2*. Whichever variable gives the lowest impurity will be selected for the split.\n",
    "\n",
    "**Splitting with *variable1* at split point 4 (average value):**\n",
    "- Gini Impurity (>4) = 1 - $\\left(\\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2\\right) = 0.375$\n",
    "\n",
    "  | variable1 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 7         | continue |\n",
    "  | 6         | stop     |\n",
    "  | 5         | continue |\n",
    "  | 6         | continue |\n",
    "\n",
    "- Gini Impurity (<=4) = 1 - $\\left(\\left(\\frac{2}{4}\\right)^2 + \\left(\\frac{2}{4}\\right)^2\\right) = 0.375$\n",
    "\n",
    "  | variable1 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 3         | stop     |\n",
    "  | 3         | stop     |\n",
    "  | 4         | continue |\n",
    "  | 3         | continue |\n",
    "\n",
    "- Weighted Gini Impurity (variable1) = $\\frac{4}{8} \\times 0.375 + \\frac{4}{8} \\times 0.5 = 0.4375$\n",
    "\n",
    "**Splitting with *variable2* at split point 6 (average value):**\n",
    "- Gini Impurity (>6) = 1 - $\\left(\\left(\\frac{3}{3}\\right)^2 + \\left(\\frac{0}{3}\\right)^2\\right) = 0$\n",
    "\n",
    "  | variable2 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 8         | continue |\n",
    "  | 9         | continue |\n",
    "  | 8         | continue |\n",
    "\n",
    "- Gini Impurity (<=6) = 1 - $\\left(\\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2\\right) = 0.48$\n",
    "\n",
    "  | variable2 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 5         | stop     |\n",
    "  | 6         | continue |\n",
    "  | 3         | stop     |\n",
    "  | 5         | stop     |\n",
    "  | 4         | continue |\n",
    "\n",
    "- Weighted Gini Impurity (variable2) = $\\frac{3}{8} \\times 0 + \\frac{5}{8} \\times 0.48 = 0.3$\n",
    "\n",
    "From the above calculations, we see that the weighted Gini impurity is less for *variable2*. Therefore, we choose *variable2* for splitting the data. This process is continued recursively to build the entire decision tree.\n",
    "\n",
    "**Note**: For the same data, using Gini impurity and entropy measure, different variables can be selected as the root node. This is because each algorithm has its bias. Gini impurity tends to select wider spread of data for variable selection, whereas the entropy method is biased toward compact data with lower spread.\n",
    "\n",
    "\n",
    "### Illustration of Node Splitting Considering Chi-Square for a Decision Tree\n",
    "\n",
    "Let's consider an example with categorical predictors and a categorical target for ease of understanding:\n",
    "\n",
    "| Performance | Class | Outcome       |\n",
    "|-------------|-------|---------------|\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Average     | X     | Doesn't Play  |\n",
    "| Average     | IX    | Doesn't Play  |\n",
    "| Below Avg   | X     | Doesn't Play  |\n",
    "| Average     | IX    | Doesn't Play  |\n",
    "| Average     | X     | Doesn't Play  |\n",
    "| Below Avg   | IX    | Doesn't Play  |\n",
    "\n",
    "Distribution of target before splitting (Parent node):\n",
    "\n",
    "- Plays Cricket = $6/12 = 0.5$\n",
    "- Doesn't play Cricket = $6/12 = 0.5$\n",
    "\n",
    "Let's try to split the node by considering the Performance variable and calculate the Chi-Square.\n",
    "\n",
    "**Splitting by Performance:**\n",
    "- Performance: Average\n",
    "  - Expected Cricket Players = $7 * (0.5) = 3.5$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0.267$\n",
    "  \n",
    "  - Expected Non-Players = $7 * (0.5) = 3.5$\n",
    "  - Actual Non-Players = $4$\n",
    "  - Chi-Square Non-Players = $0.267$\n",
    "\n",
    "- Performance: Below Avg\n",
    "  - Expected Cricket Players = $5 * (0.5) = 2.5$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0.316$\n",
    "  \n",
    "  - Expected Non-Players = $5 * (0.5) = 2.5$\n",
    "  - Actual Non-Players = $2$\n",
    "  - Chi-Square Non-Players = $0.316$\n",
    "\n",
    "Total Chi-Square (Performance) = $0.316 + 0.316 + 0.267 + 0.267 = 1.166$\n",
    "\n",
    "**Splitting by Class:**\n",
    "- Class: IX\n",
    "  - Expected Cricket Players = $6 * (0.5) = 3$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0$\n",
    "  \n",
    "  - Expected Non-Players = $6 * (0.5) = 3$\n",
    "  - Actual Non-Players = $3$\n",
    "  - Chi-Square Non-Players = $0$\n",
    "\n",
    "- Class: X\n",
    "  - Expected Cricket Players = $6 * (0.5) = 3$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0$\n",
    "  \n",
    "  - Expected Non-Players = $6 * (0.5) = 3$\n",
    "  - Actual Non-Players = $3$\n",
    "  - Chi-Square Non-Players = $0$\n",
    "\n",
    "Total Chi-Square (Class) = $0 + 0 + 0 + 0 = 0$\n",
    "\n",
    "We see that the total Chi-Square is greater when splitting by Performance than by Class, so we choose Performance to split the root node. This process is repeated recursively until we obtain pure leaf nodes.\n",
    "\n",
    "\n",
    "### Advantages of Decision Tree\n",
    "\n",
    "- It can be used for both classification and regression problems.\n",
    "- It can handle both continuous and categorical variables.\n",
    "- No feature scaling (standardization and normalization) required in the case of decision trees as they use a rule-based approach instead of distance calculation.\n",
    "- Non-linear parameters don't affect the performance of a decision tree as compared to curve-based algorithms.\n",
    "- Decision trees can automatically handle missing values.\n",
    "- Decision trees are usually robust to outliers and can handle them automatically.\n",
    "- Training period is shorter compared to Random Forest because it generates only one tree unlike a forest of trees in Random Forest.\n",
    "\n",
    "### Disadvantages of Decision Tree\n",
    "\n",
    "- Overfitting is a common problem with decision trees, leading to wrong predictions.\n",
    "- Overfitting can result in high variance in the output, leading to errors in final estimations and reduced accuracy.\n",
    "- Adding a new data point may require re-generation of the entire tree, leading to recalculations of all nodes.\n",
    "- Decision trees are not suitable for large datasets, as a single tree may become complex and prone to overfitting. Random Forest is preferred for large datasets.\n",
    "\n",
    "### Popular Algorithms\n",
    "\n",
    "Some of the popular algorithms used for constructing decision trees are:\n",
    "\n",
    "1. ID3 (Iterative Dichotomiser): Uses Information Gain as the attribute selection measure.\n",
    "2. C4.5 (Successor of ID3): Uses Gain Ratio as the attribute selection measure.\n",
    "3. CART (Classification and Regression Trees): Uses Gini Index as the attribute selection measure.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "Various methods are used to handle missing values in decision trees:\n",
    "\n",
    "- Ignoring missing values (older algorithms like ID3).\n",
    "- Treating missing values as another category (nominal feature).\n",
    "- Real handling approaches involve not using data points with missing values in split evaluations.\n",
    "\n",
    "Approaches to distributing missing value instances to child nodes include:\n",
    "\n",
    "- All going to the node with the most instances.\n",
    "- Distribution to all children with diminished weights.\n",
    "- Random distribution to a single child node.\n",
    "- Using surrogates to distribute instances based on resembling input features.\n",
    "\n",
    "### Robustness to Outliers\n",
    "\n",
    "- Regression trees are affected by outliers, as they depend on average square values of both groups.\n",
    "- Classification trees are generally less affected by outliers.\n",
    "\n",
    "### Pruning a Tree\n",
    "\n",
    "Pruning involves cutting back a decision tree to avoid overfitting:\n",
    "\n",
    "**Post-Pruning:**\n",
    "- **Minimum error**: Prune back to the point with the minimum cross-validated error.\n",
    "- **Smallest tree**: Prune slightly beyond the minimum error, creating a smaller, more interpretable tree.\n",
    "\n",
    "**Pre-Pruning (Early Stopping):**\n",
    "- Early stopping prevents overfitting by stopping the tree-building process early if the error does not decrease significantly with further splits.\n",
    "- Used together or separately, early stopping and pruning can improve tree accuracy and interpretability.\n",
    "\n",
    "For best accuracy, minimum error pruning without early stopping is recommended. Smallest tree pruning without early stopping provides a balance between accuracy and interpretability, while early stopping can save time and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e5fcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
