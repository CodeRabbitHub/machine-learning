{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e36bc91-5479-42ea-8439-bbc4e7a322e4",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques\n",
    "\n",
    "1. **Filter Methods:**\n",
    "   - **Correlation-based Feature Selection:** Features with high correlation to the target variable or with other features are selected.\n",
    "   - **Chi-square Test:** Selects categorical features that are most dependent on the target variable.\n",
    "   - **Information Gain:** Selects features that best split the dataset into different classes.\n",
    "   - **ANOVA:** Selects features that have the most significant effect on the target variable.\n",
    "   - **Variance Thresholding:** Removes features with low variance, assuming they contain less useful information.\n",
    "\n",
    "2. **Wrapper Methods:**\n",
    "   - **Forward Selection:** Starts with an empty set of features and adds features one by one that most improve model performance.\n",
    "   - **Backward Elimination:** Starts with all features and removes them one by one until the model's performance decreases.\n",
    "   - **Recursive Feature Elimination (RFE):** Recursively removes the least important features based on model coefficients or feature importance.\n",
    "   - **Recursive Feature Addition (RFA):** Adds features recursively based on their importance.\n",
    "\n",
    "3. **Embedded Methods:**\n",
    "   - **Lasso Regression (L1 regularization):** Encourages sparsity by setting some coefficients to zero, effectively performing feature selection.\n",
    "   - **Ridge Regression (L2 regularization):** Shrinks less important feature coefficients towards zero, but doesn't eliminate them entirely.\n",
    "   - **Elastic Net:** Combines L1 and L2 penalties, providing a balance between feature selection and coefficient shrinkage.\n",
    "   - **Decision Trees/Random Forest Feature Importance:** Selects features based on how much they reduce impurity in decision trees.\n",
    "   - **Gradient Boosting Feature Importance:** Measures the improvement of model performance when splitting based on each feature.\n",
    "\n",
    "4. **Dimensionality Reduction Techniques:**\n",
    "   - **Principal Component Analysis (PCA):** Transforms features into a lower-dimensional space by linearly combining them.\n",
    "   - **Linear Discriminant Analysis (LDA):** Maximizes class separability while reducing dimensionality.\n",
    "   - **t-Distributed Stochastic Neighbor Embedding (t-SNE):** Reduces dimensionality, often used for visualization.\n",
    "   - **Autoencoders:** Learns a compressed representation of features, reducing dimensionality while preserving important information.\n",
    "\n",
    "5. **Hybrid Methods:**\n",
    "   - **Genetic Algorithms, Particle Swarm Optimization, Simulated Annealing:** Search-based approaches that iteratively select subsets of features based on some optimization criterion.\n",
    "   - **Forward-backward feature selection:** Combines forward and backward selection strategies to find the best subset of features.\n",
    "\n",
    "6. **Statistical Methods:**\n",
    "   - **Student's t-test, Mann-Whitney U test, Kolmogorov-Smirnov test, Wilcoxon signed-rank test:** These tests are applied to determine if there are significant differences between the distributions of features with respect to different classes.\n",
    "\n",
    "7. **Information-Theoretic Methods:**\n",
    "   - **Mutual Information, Conditional Mutual Information, Kullback-Leibler Divergence:** Measure the amount of information shared between features and target variables, selecting features that provide the most information.\n",
    "\n",
    "8. **Distance-Based Methods:**\n",
    "   - **Relief, ReliefF, MDS (Multi-Dimensional Scaling):** Measures the relevance of features by considering the distance between instances in the feature space.\n",
    "\n",
    "9. **Sparse Methods:**\n",
    "   - **L1 Regularization, Sparse PCA, Sparse Regression:** Encourage sparsity by penalizing the number of non-zero coefficients, thus performing feature selection implicitly.\n",
    "\n",
    "10. **Clustering-Based Methods:**\n",
    "    - **K-means clustering:** Features are selected based on their centroids in different clusters.\n",
    "    - **Hierarchical clustering-based feature selection:** Features are selected based on hierarchical clustering dendrogram.\n",
    "\n",
    "11. **Correlation-Based Methods:**\n",
    "    - **Pearson, Spearman, Kendall Rank Correlation:** Features with the highest correlation with the target variable are selected.\n",
    "\n",
    "12. **Subset Search Methods:**\n",
    "    - **Exhaustive Search, Beam Search, Genetic Algorithm:** Search through the space of all possible feature subsets to find the best subset based on some criterion.\n",
    "\n",
    "13. **Greedy Methods:**\n",
    "    - **Forward Selection, Backward Elimination, Forward-backward Selection:** Iteratively add or remove features based on their individual performance or combined performance.\n",
    "\n",
    "14. **Model-Based Methods:**\n",
    "    - **Model-based Feature Importance:** Features are selected based on their importance in a specific model (e.g., linear model coefficients, decision tree feature importance).\n",
    "\n",
    "These techniques offer various approaches to feature selection, each suitable for different types of data, model requirements, and computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb4097-9cb2-40c6-a7a6-5bf967f37dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
