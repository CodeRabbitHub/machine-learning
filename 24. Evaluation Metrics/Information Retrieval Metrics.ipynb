{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc44445-e419-484d-9cbe-b57a2f9499ba",
   "metadata": {},
   "source": [
    "### Information Retrieval Metrics: Detailed Explanation\n",
    "\n",
    "#### 1. Precision at k (P@k)\n",
    "- **Definition**: Precision at k is the proportion of relevant documents retrieved in the top k documents of a ranked list.\n",
    "  \n",
    "  $$\n",
    "  P@k = \\frac{\\text{Number of relevant documents in top } k}{k}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use P@k when you want to evaluate the precision of retrieval at a specific rank.\n",
    "- **Advantages**: Provides a measure of precision at a specified retrieval rank.\n",
    "- **Disadvantages**: Does not consider the relevance of documents beyond rank k.\n",
    "\n",
    "#### 2. Average Precision (AP)\n",
    "- **Definition**: Average Precision computes the average precision at each relevant document rank position in the list of retrieved documents.\n",
    "  \n",
    "  $$\n",
    "  AP = \\frac{1}{\\text{Number of relevant documents}} \\sum_{k=1}^{N} P@k \\times \\text{rel}_k\n",
    "  $$\n",
    "  \n",
    "  where $ \\text{rel}_k $ is an indicator function that is 1 if the item at rank k is relevant and 0 otherwise.\n",
    "\n",
    "- **When to Use**: Use AP when you want to evaluate the average precision across all relevant documents.\n",
    "- **Advantages**: Accounts for the order of retrieval and provides a single metric for ranking quality.\n",
    "- **Disadvantages**: Sensitive to the number of relevant documents.\n",
    "\n",
    "#### 3. Mean Average Precision (MAP)\n",
    "- **Definition**: Mean Average Precision computes the average of the Average Precision values over a set of queries or topics.\n",
    "  \n",
    "  $$\n",
    "  MAP = \\frac{1}{\\text{Number of queries}} \\sum_{i=1}^{Q} AP_i\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use MAP when you want to evaluate the overall performance of a ranking algorithm across multiple queries.\n",
    "- **Advantages**: Provides a single metric for comparing retrieval systems across multiple queries.\n",
    "- **Disadvantages**: Sensitive to the distribution of relevant documents across queries.\n",
    "\n",
    "#### 4. Normalized Discounted Cumulative Gain (NDCG)\n",
    "- **Definition**: NDCG measures the quality of a ranked list by considering both relevance and rank position. It discounts the gain based on the position of the retrieved documents and normalizes it.\n",
    "  \n",
    "  $$\n",
    "  \\text{DCG@k} = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  \\text{IDCG@k} = \\sum_{i=1}^{\\text{min}(k, \\text{Number of relevant documents})} \\frac{2 - 1}{\\log_2(i + 1)}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  NDCG@k = \\frac{\\text{DCG@k}}{\\text{IDCG@k}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use NDCG when you want to evaluate the quality of a ranked list considering relevance and rank position.\n",
    "- **Advantages**: Accounts for the graded relevance of documents and the position in the ranked list.\n",
    "- **Disadvantages**: Sensitive to the distribution of relevance scores.\n",
    "\n",
    "#### 5. Reciprocal Rank\n",
    "- **Definition**: Reciprocal Rank is the reciprocal of the rank of the first relevant document retrieved in the ranked list.\n",
    "  \n",
    "  $$\n",
    "  \\text{Reciprocal Rank} = \\frac{1}{\\text{Rank of first relevant document}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use Reciprocal Rank when you want to measure the effectiveness of the retrieval system based on the rank of the first relevant document.\n",
    "- **Advantages**: Provides a simple measure of retrieval effectiveness.\n",
    "- **Disadvantages**: Only considers the rank of the first relevant document.\n",
    "\n",
    "#### 6. Mean Reciprocal Rank (MRR)\n",
    "- **Definition**: Mean Reciprocal Rank computes the average of the Reciprocal Rank values over a set of queries or topics.\n",
    "  \n",
    "  $$\n",
    "  \\text{MRR} = \\frac{1}{\\text{Number of queries}} \\sum_{i=1}^{Q} \\text{Reciprocal Rank}_i\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use MRR when you want to evaluate the average effectiveness of the retrieval system across multiple queries.\n",
    "- **Advantages**: Provides a single metric for comparing retrieval systems across multiple queries.\n",
    "- **Disadvantages**: Sensitive to the distribution of relevant documents across queries.\n",
    "\n",
    "#### 7. F-measure (Information Retrieval)\n",
    "- **Definition**: F-measure combines precision and recall into a single metric, providing a balance between them.\n",
    "  \n",
    "  $$\n",
    "  F\\text{-measure} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use F-measure when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9b2c7-840f-42ca-923f-25d8cfc67c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
