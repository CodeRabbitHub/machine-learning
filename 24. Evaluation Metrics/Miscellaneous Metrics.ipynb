{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57204292-2e98-41a5-ab0f-45e1974d196a",
   "metadata": {},
   "source": [
    "### Miscellaneous Metrics: Detailed Explanation\n",
    "\n",
    "#### 1. Jaccard Similarity Score\n",
    "- **Definition**: Jaccard Similarity Score measures the similarity between two sets by calculating the ratio of the intersection to the union of the sets.\n",
    "  \n",
    "  $$\n",
    "  \\text{Jaccard Similarity} = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Jaccard Similarity Score is used to compare the similarity between two sets, commonly used in text analysis, recommendation systems, and clustering.\n",
    "- **Advantages**: Simple and intuitive metric for measuring set similarity.\n",
    "- **Disadvantages**: Does not consider the order or frequency of elements within the sets.\n",
    "\n",
    "#### 2. Kendall’s Tau\n",
    "- **Definition**: Kendall’s Tau measures the association between two ranked variables by counting concordant and discordant pairs.\n",
    "  \n",
    "  $$\n",
    "  \\tau = \\frac{{\\text{number of concordant pairs} - \\text{number of discordant pairs}}}{{\\text{total number of pairs}}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Kendall’s Tau is used to quantify the similarity of rankings or to assess correlation in non-parametric data analysis.\n",
    "- **Advantages**: Robust to outliers and ties in the data.\n",
    "- **Disadvantages**: Less intuitive interpretation compared to other correlation measures.\n",
    "\n",
    "#### 3. Spearman’s Rank Correlation\n",
    "- **Definition**: Spearman’s Rank Correlation measures the strength and direction of association between two ranked variables using the ranks instead of the actual values.\n",
    "  \n",
    "  $$\n",
    "  \\rho = 1 - \\frac{{6 \\sum d_i^2}}{{n(n^2 - 1)}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Spearman’s Rank Correlation is used to assess monotonic relationships between variables or to compare rankings.\n",
    "- **Advantages**: Robust to outliers and non-linear relationships.\n",
    "- **Disadvantages**: Requires ordinal or rank data.\n",
    "\n",
    "#### 4. Perplexity (for language models)\n",
    "- **Definition**: Perplexity measures the uncertainty or entropy of a probability distribution, often used to evaluate the performance of language models.\n",
    "  \n",
    "  $$\n",
    "  \\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(x_i)}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Perplexity is used to assess the quality of language models, with lower perplexity indicating better performance.\n",
    "- **Advantages**: Provides a quantitative measure of language model performance.\n",
    "- **Disadvantages**: Interpretation may be difficult for non-technical audiences.\n",
    "\n",
    "#### 5. Gini Coefficient\n",
    "- **Definition**: Gini Coefficient quantifies the inequality in a distribution by calculating the ratio of the area between the Lorenz curve and the diagonal line to the total area under the diagonal line.\n",
    "  \n",
    "  $$\n",
    "  G = \\frac{A}{A + B}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Gini Coefficient is used to measure economic inequality, wealth distribution, and classification performance evaluation.\n",
    "- **Advantages**: Intuitive measure of inequality, commonly used in economics and classification tasks.\n",
    "- **Disadvantages**: Sensitive to changes in distribution shape.\n",
    "\n",
    "#### 6. Top-k Accuracy\n",
    "- **Definition**: Top-k Accuracy measures the proportion of correct predictions within the top k predictions made by a model.\n",
    "\n",
    "  Let $ C_i $ be the set of correct labels for instance $ i $ and $ T_i $ be the top $ k $ predictions for instance $ i $. Then Top-k Accuracy is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{Top-k Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|C_i \\cap T_i|}{k}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Top-k Accuracy is used when multiple predictions are made for each instance, and only the top k predictions are considered.\n",
    "- **Advantages**: Provides a more flexible evaluation metric compared to standard accuracy.\n",
    "- **Disadvantages**: May require tuning the value of k based on the application.\n",
    "\n",
    "#### 7. Normalized Mutual Information (NMI)\n",
    "- **Definition**: Normalized Mutual Information measures the mutual dependence between two variables, adjusted for the number of clusters and the entropy of each variable.\n",
    "  \n",
    "  $$\n",
    "  \\text{NMI}(A, B) = \\frac{I(A, B)}{\\sqrt{H(A) \\cdot H(B)}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: NMI is used to evaluate clustering algorithms by comparing the clustering results with ground truth labels.\n",
    "- **Advantages**: Adjusts for chance agreement and cluster size imbalances.\n",
    "- **Disadvantages**: Requires ground truth labels for evaluation.\n",
    "\n",
    "#### 8. Mutual Information\n",
    "- **Definition**: Mutual Information measures the amount of information shared between two variables, quantifying the reduction in uncertainty of one variable given the other variable.\n",
    "  \n",
    "  $$\n",
    "  I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\left( \\frac{p(x, y)}{p(x) p(y)} \\right)\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Mutual Information is used in feature selection, dimensionality reduction, and clustering.\n",
    "- **Advantages**: Provides a measure of dependency between variables.\n",
    "- **Disadvantages**: Sensitive to the scale and distribution of variables.\n",
    "\n",
    "#### 9. Entropy\n",
    "- **Definition**: Entropy measures the uncertainty or disorder in a probability distribution, quantifying the average level of information or surprise in the data.\n",
    "  \n",
    "  $$\n",
    "  H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_2(p(x_i))\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Entropy is used in information theory, decision trees, and machine learning for feature selection and model evaluation.\n",
    "- **Advantages**: Provides a measure of uncertainty or randomness in data.\n",
    "- **Disadvantages**: Interpretation may be challenging for non-technical users.\n",
    "\n",
    "#### 10. Cross-Entropy\n",
    "- **Definition**: Cross-Entropy measures the average number of bits needed to represent or transmit an event from one distribution when using the optimal encoding based on another distribution.\n",
    "  \n",
    "  $$\n",
    "  H(p, q) = - \\sum_{x} p(x) \\log q(x)\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Cross-Entropy is used in machine learning for evaluating classification models and optimizing neural networks.\n",
    "- **Advantages**: Provides a measure of dissimilarity between probability distributions.\n",
    "- **Disadvantages**: May be sensitive to outliers and class imbalance.\n",
    "\n",
    "#### 11. Zero-One Loss\n",
    "- **Definition**: Zero-One Loss measures the fraction of incorrect predictions made by a classifier, where 0 indicates correct classification and 1 indicates incorrect classification.\n",
    "  \n",
    "  $$\n",
    "  \\text{Zero-One Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}(y_i \\neq \\hat{y}_i)\n",
    "  $$\n",
    "  \n",
    "**When to Use**: Zero-One Loss is suitable when the misclassification costs for different classes are equal, and when it's important to have a clear interpretation of classification errors.\n",
    "- **Advantages**: Provides a clear interpretation of classification errors and is easy to understand.\n",
    "- **Disadvantages**: Does not consider the confidence or proximity of predictions, treating all misclassifications equally.\n",
    "\n",
    "#### 12. Hinge Loss\n",
    "- **Definition**: Hinge Loss is a margin-based loss function used in support vector machines (SVMs) for binary classification, penalizing misclassifications based on the margin between classes.\n",
    "  \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\max(0, 1 - y \\cdot \\hat{y})\n",
    "  $$\n",
    "\n",
    "- **When to Use**: Hinge Loss is used in SVMs and other margin-based classifiers for optimizing model parameters.\n",
    "- **Advantages**: Encourages large margins between classes, leading to robust classifiers.\n",
    "- **Disadvantages**: May not be suitable for probabilistic models and multi-class classification.\n",
    "\n",
    "Apologies for the oversight. Here's the Earth Mover's Distance (EMD) formula and its explanation:\n",
    "\n",
    "#### 13. Earth Mover’s Distance (EMD)\n",
    "- **Definition**: Earth Mover’s Distance measures the dissimilarity between two probability distributions by calculating the minimum amount of work needed to transform one distribution into the other.\n",
    "  \n",
    "  $$\n",
    "  \\text{EMD} = \\min_{\\gamma} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\gamma_{ij} \\cdot c_{ij}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ \\gamma $ is a transportation plan that specifies how much mass from each point in the source distribution is moved to each point in the target distribution.\n",
    "  - $ c_{ij} $ represents the cost of moving mass from point $ i $ in the source distribution to point $ j $ in the target distribution.\n",
    "  - $ m $ and $ n $ are the number of points in the source and target distributions, respectively.\n",
    "\n",
    "- **When to Use**: EMD is used in computer vision, image processing, and text analysis for comparing histograms and probability distributions.\n",
    "- **Advantages**: Provides a measure of dissimilarity that accounts for the structure and support of distributions.\n",
    "- **Disadvantages**: Computationally intensive for large distributions.\n",
    "\n",
    "#### 14. Mean Reciprocal Rank (MRR)\n",
    "- **Definition**: Mean Reciprocal Rank calculates the average reciprocal rank of relevant items in a ranked list, often used in information retrieval and recommendation systems.\n",
    "  \n",
    "  $$\n",
    "  \\text{MRR} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\text{rank}_i}\n",
    "  $$\n",
    "\n",
    "- **When to Use**: MRR is used to evaluate the effectiveness of systems that return ranked lists of items, with higher values indicating better performance.\n",
    "- **Advantages**: Provides a single metric for comparing retrieval systems based on the rank of relevant items.\n",
    "- **Disadvantages**: May be sensitive to the distribution of relevant items and the ranking algorithm.\n",
    "\n",
    "These miscellaneous metrics offer diverse tools for evaluating various aspects of data, models, and systems across different domains and applications. The choice of metric depends on the specific context, objectives, and characteristics of the data or system being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467270a-4cae-4ede-af87-987ff0a9b010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
