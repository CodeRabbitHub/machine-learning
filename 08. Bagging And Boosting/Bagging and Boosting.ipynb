{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4bf740-005f-4beb-9aa0-f5292343dd10",
   "metadata": {},
   "source": [
    "# Bagging and Boosting: Detailed Explanation\n",
    "\n",
    "Bagging and boosting are two popular ensemble learning techniques in machine learning that combine the predictions from multiple models to improve overall performance. While both methods aim to enhance the accuracy and robustness of the predictions, they do so in fundamentally different ways.\n",
    "\n",
    "## Bagging (Bootstrap Aggregating)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Data Sampling**:\n",
    "   - From the original dataset, multiple subsets are created through bootstrapping. Bootstrapping involves random sampling with replacement, meaning some instances may appear multiple times in a subset while others may not appear at all.\n",
    "   \n",
    "2. **Model Training**:\n",
    "   - A separate model (often the same type of model) is trained on each of these bootstrapped subsets.\n",
    "\n",
    "3. **Aggregation**:\n",
    "   - For regression tasks, the predictions from all models are averaged.\n",
    "   - For classification tasks, a majority vote is taken from the predictions of all models.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "Let's denote the original dataset as $D$ with $n$ samples. Each subset $D_i$ is obtained by sampling $n$ instances from $D$ with replacement.\n",
    "\n",
    "- **Training**:\n",
    "  Each model $M_i$ is trained on a subset $D_i$.\n",
    "\n",
    "- **Prediction**:\n",
    "  For regression:\n",
    "  $$\n",
    "  \\hat{y} = \\frac{1}{B} \\sum_{i=1}^B M_i(x)\n",
    "  $$\n",
    "  For classification:\n",
    "  $$\n",
    "  \\hat{y} = \\text{mode}(M_1(x), M_2(x), \\ldots, M_B(x))\n",
    "  $$\n",
    "  where $B$ is the number of models.\n",
    "\n",
    "### Illustrations\n",
    "\n",
    "<center><img src=\"fig/Bagging.png\"/></center>\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages**:\n",
    "  - Reduces variance and helps to prevent overfitting.\n",
    "  - Can handle high-dimensional data well.\n",
    "  - Easy to parallelize as each model is independent.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Does not reduce bias if individual models are biased.\n",
    "  - Requires more computational resources due to multiple models.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Sequential Training**:\n",
    "   - Models are trained sequentially, each trying to correct the errors of the previous one.\n",
    "\n",
    "2. **Weight Adjustment**:\n",
    "   - Instances that are incorrectly predicted by a model are given higher weights so that the next model focuses more on these difficult cases.\n",
    "\n",
    "3. **Final Prediction**:\n",
    "   - The final prediction is a weighted sum (or majority vote) of all individual model predictions.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "Let's denote the initial weight of each instance as $\\frac{1}{n}$.\n",
    "\n",
    "- **Training**:\n",
    "  Each model $M_i$ is trained on the weighted dataset.\n",
    "\n",
    "- **Error Calculation**:\n",
    "  The error of model $M_i$ is calculated as:\n",
    "  $$\n",
    "  \\epsilon_i = \\sum_{j=1}^n w_j I(y_j \\neq M_i(x_j))\n",
    "  $$\n",
    "  where $w_j$ is the weight of instance $j$, $y_j$ is the true label, and $I$ is the indicator function.\n",
    "\n",
    "- **Model Weight**:\n",
    "  The weight of model $M_i$ is:\n",
    "  $$\n",
    "  \\alpha_i = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_i}{\\epsilon_i}\\right)\n",
    "  $$\n",
    "\n",
    "- **Weight Update**:\n",
    "  The weights of instances are updated as:\n",
    "  $$\n",
    "  w_j \\leftarrow w_j \\exp(\\alpha_i I(y_j \\neq M_i(x_j)))\n",
    "  $$\n",
    "\n",
    "- **Normalization**:\n",
    "  The weights are then normalized.\n",
    "\n",
    "- **Final Prediction**:\n",
    "  For classification:\n",
    "  $$\n",
    "  \\hat{y} = \\text{sign}\\left(\\sum_{i=1}^B \\alpha_i M_i(x)\\right)\n",
    "  $$\n",
    "\n",
    "### Illustrations\n",
    "\n",
    "<center><img src=\"fig/Boosting.png\"/></center>\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages**:\n",
    "  - Reduces both bias and variance.\n",
    "  - Can achieve high accuracy with relatively simple models.\n",
    "  - Focuses on difficult instances, improving model performance on challenging data.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Can be sensitive to noisy data and outliers.\n",
    "  - More difficult to parallelize due to sequential nature.\n",
    "  - Requires careful tuning of hyperparameters to prevent overfitting.\n",
    "\n",
    "## When to Use Bagging and Boosting\n",
    "\n",
    "- **Bagging**:\n",
    "  - When the model has high variance (e.g., decision trees).\n",
    "  - When computational resources are available for parallel processing.\n",
    "  - When simplicity and ease of implementation are preferred.\n",
    "\n",
    "- **Boosting**:\n",
    "  - When the model has high bias (e.g., simple models like decision stumps).\n",
    "  - When achieving the highest possible accuracy is crucial.\n",
    "  - When dealing with imbalanced datasets, as boosting can focus on minority classes.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Bagging**: Reduces variance, works well with high-variance models, involves parallel model training and averaging or voting.\n",
    "- **Boosting**: Reduces both bias and variance, works well with weak learners, involves sequential model training with weight adjustment and weighted predictions.\n",
    "\n",
    "Both techniques are powerful tools in the ensemble learning arsenal, each suited to different types of modeling challenges and datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f84552-7b82-47ab-b46d-c772cc4786e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
