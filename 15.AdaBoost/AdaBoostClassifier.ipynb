{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe9cbdb-5e3c-463e-b4fd-ef2680c290b6",
   "metadata": {},
   "source": [
    "## AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is one of the most popular and influential ensemble learning methods. Unlike Gradient Boosting Machines (GBM) which focus on reducing residual errors by fitting new models to the gradient of the loss function, AdaBoost focuses on adjusting the weights of the training instances so that subsequent weak learners focus more on difficult cases. It is primarily used for binary classification tasks, though extensions exist for multi-class classification and regression.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Ensemble Learning\n",
    "\n",
    "Ensemble learning combines multiple models to produce a single robust model. AdaBoost is an ensemble technique that builds a series of models where each new model focuses more on the misclassified instances by adjusting their weights.\n",
    "\n",
    "#### 2. Boosting\n",
    "\n",
    "Boosting is an ensemble method that combines weak learners to create a strong learner. In the context of AdaBoost, boosting refers to the iterative process where each subsequent model is trained with a focus on the instances that were misclassified by the previous models.\n",
    "\n",
    "### Steps Involved in AdaBoost\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The AdaBoost process begins by initializing the weights of the training instances equally. If there are $ N $ training instances, each weight $ w_i $ is initialized to:\n",
    "$$ w_i = \\frac{1}{N} $$\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "AdaBoost constructs an ensemble of weak learners in a sequential manner. At each iteration $ m $:\n",
    "\n",
    "**Step 2-1: Train Weak Learner**\n",
    "\n",
    "- Train a weak learner $ h_m(x) $ on the weighted training data.\n",
    "\n",
    "**Step 2-2: Compute Error**\n",
    "\n",
    "- Compute the weighted error $ \\epsilon_m $ of the weak learner:\n",
    "$$ \\epsilon_m = \\frac{\\sum_{i=1}^N w_i \\mathbb{1}(h_m(x_i) \\neq y_i)}{\\sum_{i=1}^N w_i} $$\n",
    "where $ \\mathbb{1} $ is the indicator function that equals 1 if $ h_m(x_i) \\neq y_i $ and 0 otherwise.\n",
    "\n",
    "**Step 2-3: Compute Learner Weight**\n",
    "\n",
    "- Compute the weight $ \\alpha_m $ of the weak learner, which measures the importance of the learner in the final model:\n",
    "$$ \\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right) $$\n",
    "\n",
    "**Step 2-4: Update Weights**\n",
    "\n",
    "- Update the weights of the training instances. Misclassified instances are given more weight, and correctly classified instances are given less weight:\n",
    "$$ w_i \\leftarrow w_i \\exp(\\alpha_m \\mathbb{1}(h_m(x_i) \\neq y_i)) $$\n",
    "\n",
    "- Normalize the weights to ensure they sum to 1:\n",
    "$$ w_i \\leftarrow \\frac{w_i}{\\sum_{i=1}^N w_i} $$\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $ M $ iterations, the final boosted model $ F(x) $ is a weighted sum of the weak learners:\n",
    "$$ F(x) = \\sum_{m=1}^M \\alpha_m h_m(x) $$\n",
    "\n",
    "The final prediction for binary classification is obtained by taking the sign of $ F(x) $:\n",
    "$$ \\hat{y} = \\text{sign}(F(x)) $$\n",
    "\n",
    "### Expanded Explanation of $ F(x) $\n",
    "\n",
    "Let's expand on each part of the equation $ F(x) = \\sum_{m=1}^M \\alpha_m h_m(x) $:\n",
    "\n",
    "- **Weak Learner $ h_m(x) $**: Each weak learner is a simple model, such as a decision stump (a tree with a single split), that is trained on the weighted training data.\n",
    "\n",
    "- **Learner Weight $ \\alpha_m $**: This weight indicates the importance of the weak learner in the final model. It is computed based on the weighted error of the learner, giving more weight to more accurate learners.\n",
    "\n",
    "- **Model Aggregation $ \\sum_{m=1}^M \\alpha_m h_m(x) $**: The final model is a linear combination of all the weak learners, weighted by their respective importance.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "To provide a concrete example, consider the first few iterations of the AdaBoost process for a binary classification task:\n",
    "\n",
    "1. **Initial Weights $ w_i $**:\n",
    "   - Suppose we have 4 training instances. Each weight is initialized to $ \\frac{1}{4} = 0.25 $.\n",
    "\n",
    "2. **First Iteration (m = 1)**:\n",
    "   - Train a weak learner $ h_1(x) $.\n",
    "   - Compute the weighted error $ \\epsilon_1 $.\n",
    "   - Compute the learner weight $ \\alpha_1 = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_1}{\\epsilon_1}\\right) $.\n",
    "   - Update the weights $ w_i $.\n",
    "\n",
    "3. **Second Iteration (m = 2)**:\n",
    "   - Train a new weak learner $ h_2(x) $ on the updated weights.\n",
    "   - Compute the weighted error $ \\epsilon_2 $.\n",
    "   - Compute the learner weight $ \\alpha_2 = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_2}{\\epsilon_2}\\right) $.\n",
    "   - Update the weights $ w_i $.\n",
    "\n",
    "This process continues for $ M $ iterations, with each iteration aiming to focus more on the misclassified instances and improve the model's accuracy.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in AdaBoost include:\n",
    "\n",
    "- **n_estimators**: Number of boosting stages (i.e., the number of weak learners).\n",
    "- **learning_rate**: A factor that multiplies the weight $ \\alpha_m $ of each weak learner. It controls the contribution of each weak learner and helps in preventing overfitting.\n",
    "- **base_estimator**: The type of weak learner to use (e.g., decision stump).\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement.\n",
    "2. **Versatility**: Can be used with various weak learners.\n",
    "3. **Effectiveness**: Often performs well even with simple weak learners.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Sensitivity to Noisy Data**: Can be sensitive to noisy data and outliers.\n",
    "2. **Overfitting**: Can overfit if the weak learners are too complex or if there are too many iterations.\n",
    "3. **Computationally Intensive**: Can be slow to train, especially with a large number of weak learners.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how AdaBoost can be implemented using Scikit-Learn in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)  # Decision stump\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = ada.predict(X_test)\n",
    "```\n",
    "\n",
    "For regression, you would use `AdaBoostRegressor` similarly.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "AdaBoost is a powerful and versatile boosting technique for binary classification tasks. By iteratively adjusting the weights of the training instances, it focuses more on difficult cases, resulting in a strong ensemble model. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379162c-f7d6-4f5c-991e-3c69820bf73d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
