{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceed19f3-8ade-43f0-9b8c-d24fe0aa5dcb",
   "metadata": {},
   "source": [
    "### Classification Metrics: Detailed Explanation\n",
    "\n",
    "#### 1. Accuracy\n",
    "- **Definition**: Accuracy is the ratio of correctly predicted instances to the total instances.\n",
    "  \n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use accuracy when class distribution is balanced and all classes are equally important.\n",
    "- **Advantages**: Simple and easy to interpret.\n",
    "- **Disadvantages**: Misleading for imbalanced datasets as it can give a high value for models that simply predict the majority class.\n",
    "\n",
    "#### 2. Precision\n",
    "- **Definition**: Precision is the ratio of correctly predicted positive observations to the total predicted positives.\n",
    "  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use precision when the cost of false positives is high.\n",
    "- **Advantages**: Highlights how many of the predicted positive cases are actually positive.\n",
    "- **Disadvantages**: Does not consider false negatives.\n",
    "\n",
    "#### 3. Recall (Sensitivity)\n",
    "- **Definition**: Recall is the ratio of correctly predicted positive observations to all the observations in the actual class.\n",
    "  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use recall when the cost of false negatives is high.\n",
    "- **Advantages**: Measures how well the model captures the actual positives.\n",
    "- **Disadvantages**: Does not consider false positives.\n",
    "\n",
    "#### 4. F1 Score\n",
    "- **Definition**: F1 Score is the harmonic mean of precision and recall.\n",
    "  \n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use F1 Score when you need a balance between precision and recall.\n",
    "- **Advantages**: Provides a single metric balancing both false positives and false negatives.\n",
    "- **Disadvantages**: Can be misleading if the distribution of class is skewed.\n",
    "\n",
    "#### 5. F2 Score\n",
    "- **Definition**: F2 Score is a weighted version of the F1 Score that gives more importance to recall.\n",
    "  \n",
    "  $$\n",
    "  F2 = (1 + 2^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use F2 Score when recall is more important than precision.\n",
    "- **Advantages**: Balances precision and recall with greater emphasis on recall.\n",
    "- **Disadvantages**: Less intuitive and less commonly used than F1 Score.\n",
    "\n",
    "#### 6. Specificity (True Negative Rate)\n",
    "- **Definition**: Specificity is the ratio of correctly predicted negative observations to all the observations in the actual negative class.\n",
    "  \n",
    "  $$\n",
    "  \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use specificity when the cost of false positives is high.\n",
    "- **Advantages**: Measures the proportion of actual negatives correctly identified.\n",
    "- **Disadvantages**: Does not consider false negatives.\n",
    "\n",
    "#### 7. Balanced Accuracy\n",
    "- **Definition**: Balanced Accuracy is the average of recall obtained on each class.\n",
    "  \n",
    "  $$\n",
    "  \\text{Balanced Accuracy} = \\frac{1}{2} \\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} \\right)\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use when dealing with imbalanced datasets.\n",
    "- **Advantages**: Accounts for imbalanced datasets better than simple accuracy.\n",
    "- **Disadvantages**: Can be less intuitive than accuracy.\n",
    "\n",
    "#### 8. ROC AUC Score\n",
    "- **Definition**: ROC AUC Score measures the area under the receiver operating characteristic curve, which plots the true positive rate against the false positive rate at various threshold settings.\n",
    "  \n",
    "  $$\n",
    "  \\text{ROC AUC} = \\int_{0}^{1} TPR(FPR) \\, dFPR\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use to evaluate the performance of a binary classifier.\n",
    "- **Advantages**: Provides a single metric for model performance across all classification thresholds.\n",
    "- **Disadvantages**: Can be misleading in highly imbalanced datasets.\n",
    "\n",
    "#### 9. Precision-Recall AUC\n",
    "- **Definition**: Precision-Recall AUC measures the area under the precision-recall curve.\n",
    "  \n",
    "  $$\n",
    "  \\text{PR AUC} = \\int_{0}^{1} \\text{Precision}(\\text{Recall}) \\, d\\text{Recall}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use when you have imbalanced datasets and care more about the minority class.\n",
    "- **Advantages**: Focuses on the performance of the positive class.\n",
    "- **Disadvantages**: Can be less intuitive than ROC AUC.\n",
    "\n",
    "#### 10. Logarithmic Loss (Log Loss)\n",
    "- **Definition**: Log Loss measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  \n",
    "  $$\n",
    "  \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use when you need to evaluate the probabilities predicted by a model.\n",
    "- **Advantages**: Accounts for the uncertainty of the predictions.\n",
    "- **Disadvantages**: Sensitive to extreme probabilities.\n",
    "\n",
    "#### 11. Matthews Correlation Coefficient (MCC)\n",
    "- **Definition**: MCC is a measure of the quality of binary classifications, accounting for all four quadrants of the confusion matrix.\n",
    "  \n",
    "  $$\n",
    "  MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP) (TP + FN) (TN + FP) (TN + FN)}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use when you need a balanced measure for binary classification.\n",
    "- **Advantages**: Provides a more balanced measure that accounts for all four confusion matrix quadrants.\n",
    "- **Disadvantages**: Can be more complex to interpret.\n",
    "\n",
    "#### 12. Cohen’s Kappa\n",
    "- **Definition**: Cohen’s Kappa measures the agreement between two raters or classifiers.\n",
    "  \n",
    "  $$\n",
    "  \\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "  $$\n",
    "  \n",
    "  where $ p_o $ is the observed agreement and $ p_e $ is the expected agreement.\n",
    "  \n",
    "- **When to Use**: Use to evaluate the agreement between two raters/classifiers.\n",
    "- **Advantages**: Accounts for the possibility of the agreement occurring by chance.\n",
    "- **Disadvantages**: Can be difficult to interpret.\n",
    "\n",
    "#### 13. Jaccard Index\n",
    "- **Definition**: Jaccard Index measures the similarity between the actual and predicted sets.\n",
    "  \n",
    "  $$\n",
    "  \\text{Jaccard Index} = \\frac{TP}{TP + FP + FN}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use for binary and multiclass classification to evaluate similarity.\n",
    "- **Advantages**: Provides a straightforward measure of similarity.\n",
    "- **Disadvantages**: Less informative than some other metrics.\n",
    "\n",
    "#### 14. Brier Score\n",
    "- **Definition**: Brier Score measures the mean squared difference between predicted probabilities and the actual outcome.\n",
    "  \n",
    "  $$\n",
    "  \\text{Brier Score} = \\frac{1}{n} \\sum_{i=1}^{n} (p_i - y_i)^2\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use to evaluate the accuracy of probabilistic predictions.\n",
    "- **Advantages**: Measures the accuracy of probability predictions.\n",
    "- **Disadvantages**: Less intuitive than classification metrics like accuracy.\n",
    "\n",
    "#### 15. Hamming Loss\n",
    "- **Definition**: Hamming Loss is the fraction of incorrect predictions (misclassifications).\n",
    "  \n",
    "  $$\n",
    "  \\text{Hamming Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}(\\hat{y}_i \\neq y_i)\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use for multi-label classification problems.\n",
    "- **Advantages**: Simple and easy to understand.\n",
    "- **Disadvantages**: May not capture the severity of misclassifications.\n",
    "\n",
    "#### 16. Fowlkes-Mallows Index (FMI)\n",
    "- **Definition**: FMI measures the geometric mean of precision and recall.\n",
    "  \n",
    "  $$\n",
    "  FMI = \\sqrt{\\frac{TP}{TP + FP} \\cdot \\frac{TP}{TP + FN}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use for clustering and binary classification.\n",
    "- **Advantages**: Combines precision and recall in a single measure.\n",
    "- **Disadvantages**: Less commonly used than F1 Score.\n",
    "\n",
    "#### 17. G-Mean (Geometric Mean)\n",
    "- **Definition**: G-Mean is the geometric mean of sensitivity and specificity.\n",
    "  \n",
    "  $$\n",
    "  G\\text{-Mean} = \\sqrt{\\text{Sensitivity} \\times \\text{Specificity}}\n",
    "  $$\n",
    "  \n",
    "- **When to Use**: Use for imbalanced datasets to balance sensitivity and specificity.\n",
    "- **Advantages**: Balances performance on both classes.\n",
    "- **Disadvantages**: Can be less intuitive to interpret.\n",
    "\n",
    "Sure, let's continue:\n",
    "\n",
    "#### 18. Balanced Error Rate (BER) (Continued)\n",
    "- **Definition**: BER calculates the average of the error rates for each class. It is useful for evaluating the performance of classifiers on imbalanced datasets.\n",
    "  \n",
    "  $$\n",
    "  \\text{BER} = \\frac{1}{2} \\left( \\frac{FP}{FP + TN} + \\frac{FN}{FN + TP} \\right)\n",
    "  $$\n",
    "\n",
    "- **When to Use**: BER is particularly useful when evaluating classifiers on imbalanced datasets where one class dominates the other(s).\n",
    "- **Advantages**: Provides a balanced measure of error across different classes.\n",
    "- **Disadvantages**: May not capture specific performance characteristics of individual classes.\n",
    "\n",
    "These metrics provide a comprehensive set of tools for evaluating the performance of classification models under different scenarios and requirements. Depending on the specific goals of the classification task and the characteristics of the dataset, different metrics may be more appropriate for assessing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2c393-aaef-4517-926a-f2d3b7134750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
