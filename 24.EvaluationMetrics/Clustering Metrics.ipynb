{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3ea2bf-d18f-4cd8-8f65-70c11e7bc1ff",
   "metadata": {},
   "source": [
    "### Clustering Metrics: Detailed Explanation\n",
    "\n",
    "#### 1. Silhouette Score\n",
    "- **Definition**: Silhouette Score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "  \n",
    "  $$\n",
    "  \\text{Silhouette Score} = \\frac{b - a}{\\max(a, b)}\n",
    "  $$\n",
    "  \n",
    "  where $ a $ is the mean distance between a sample and all other points in the same class, and $ b $ is the mean distance between a sample and all other points in the nearest cluster that the sample is not a part of.\n",
    "\n",
    "- **When to Use**: Use Silhouette Score when you want to evaluate the quality of clustering.\n",
    "- **Advantages**: Provides a succinct interpretation of cluster cohesion and separation.\n",
    "- **Disadvantages**: Requires the number of clusters as input, which may not always be known.\n",
    "\n",
    "#### 2. Davies-Bouldin Index\n",
    "- **Definition**: Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster, where similarity is based on the ratio of within-cluster distances to between-cluster distances. A lower score indicates better clustering.\n",
    "  \n",
    "  $$\n",
    "  \\text{Davies-Bouldin Index} = \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\max_{j \\neq i} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)\n",
    "  $$\n",
    "  \n",
    "  where $ n_c $ is the number of clusters, $ \\sigma_i $ is the average distance from the centroid of cluster $ i $ to all points in cluster $ i $, and $ d(c_i, c_j) $ is the distance between the centroids of clusters $ i $ and $ j $.\n",
    "\n",
    "- **When to Use**: Use Davies-Bouldin Index when you want a measure of cluster compactness and separation.\n",
    "- **Advantages**: Provides a relatively simple interpretation of cluster quality.\n",
    "- **Disadvantages**: Requires the number of clusters as input.\n",
    "\n",
    "#### 3. Adjusted Rand Index (ARI)\n",
    "- **Definition**: Adjusted Rand Index computes the similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. It corrects for chance grouping.\n",
    "  \n",
    "  $$\n",
    "  \\text{ARI} = \\frac{\\text{RI} - \\text{ExpectedRI}}{\\max(\\text{RI}) - \\text{ExpectedRI}}\n",
    "  $$\n",
    "  \n",
    "  where RI is the Rand Index and ExpectedRI is the expected value of the Rand Index.\n",
    "  \n",
    "- **When to Use**: Use ARI when you have ground truth labels and want to evaluate the agreement between two clusterings.\n",
    "- **Advantages**: Corrects for chance grouping and provides a measure of clustering similarity.\n",
    "- **Disadvantages**: Sensitive to the number of clusters and sample size.\n",
    "\n",
    "#### 4. Adjusted Mutual Information (AMI)\n",
    "- **Definition**: Adjusted Mutual Information measures the agreement between two clusterings, adjusting for chance. It is based on the concept of mutual information, which measures the amount of information obtained about one variable through another variable.\n",
    "  \n",
    "  $$\n",
    "  \\text{AMI} = \\frac{I(X; Y) - E[I(X; Y)]}{\\max(H(X), H(Y)) - E[I(X; Y)]}\n",
    "  $$\n",
    "  \n",
    "  where $ I(X; Y) $ is the mutual information between the true and predicted clusterings, $ E[I(X; Y)] $ is the expected mutual information, and $ H(X) $ and $ H(Y) $ are the entropies of the true and predicted clusterings, respectively.\n",
    "\n",
    "- **When to Use**: Use AMI when you want to measure the agreement between two clusterings, correcting for chance.\n",
    "- **Advantages**: Adjusts for chance and provides a measure of clustering similarity.\n",
    "- **Disadvantages**: Sensitive to the number of clusters and sample size.\n",
    "\n",
    "#### 5. Calinski-Harabasz Index (Variance Ratio Criterion)\n",
    "- **Definition**: Calinski-Harabasz Index measures the ratio of between-cluster dispersion to within-cluster dispersion. A higher value indicates better clustering.\n",
    "  \n",
    "  $$\n",
    "  \\text{CH Index} = \\frac{\\text{Tr}(B_k)}{\\text{Tr}(W_k)} \\times \\frac{n - k}{k - 1}\n",
    "  $$\n",
    "  \n",
    "  where $ B_k $ is the between-cluster dispersion matrix, $ W_k $ is the within-cluster dispersion matrix, $ n $ is the number of samples, and $ k $ is the number of clusters.\n",
    "\n",
    "- **When to Use**: Use Calinski-Harabasz Index when you want a measure of cluster compactness and separation.\n",
    "- **Advantages**: Provides a relatively simple interpretation of cluster quality.\n",
    "- **Disadvantages**: Sensitive to the number of clusters.\n",
    "\n",
    "#### 6. Dunn Index\n",
    "- **Definition**: Dunn Index measures the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. A higher value indicates better clustering.\n",
    "  \n",
    "  $$\n",
    "  \\text{Dunn Index} = \\frac{\\text{min}_{i \\neq j} \\text{dist}(C_i, C_j)}{\\max_{i} \\text{dist}(C_i)}\n",
    "  $$\n",
    "  \n",
    "  where $ \\text{dist}(C_i, C_j) $ is the distance between clusters $ C_i $ and $ C_j $, and $ \\text{dist}(C_i) $ is the diameter of cluster $ C_i $.\n",
    "\n",
    "- **When to Use**: Use Dunn Index when you want a measure of cluster compactness and separation.\n",
    "- **Advantages**: Provides a simple interpretation of cluster quality.\n",
    "- **Disadvantages**: Sensitive to the number of clusters.\n",
    "\n",
    "#### 7. V-Measure\n",
    "- **Definition**: V-Measure is the harmonic mean of homogeneity and completeness, providing a single measure of clustering quality.\n",
    "  \n",
    "  $$\n",
    "  \\text{V-Measure} = \\frac{2 \\cdot \\text{Homogeneity} \\cdot \\text{Completeness}}{\\text{Homogeneity} + \\text{Completeness}}\n",
    "  $$\n",
    "  \n",
    "  where homogeneity measures the extent to which clusters contain only data points belonging to a single class, and completeness measures the extent to which all data points that are members of a given class are assigned to the same cluster.\n",
    "\n",
    "- **When to Use**: Use V-Measure when you want a single measure that captures both homogeneity and completeness.\n",
    "- **Advantages**: Provides a balanced measure of clustering quality.\n",
    "- **Disadvantages**: Sensitivity to the number of clusters and sample size.\n",
    "\n",
    "These clustering metrics provide a range of tools for evaluating the quality of clustering algorithms under different scenarios and requirements. Depending on the specific goals of the clustering task and the characteristics of the dataset, different metrics may be more appropriate for assessing clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb92dc1-ee4b-4b82-92af-8243cbbda2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
