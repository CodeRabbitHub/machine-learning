{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26a0eda-3b4a-403a-885c-6ae70822c276",
   "metadata": {},
   "source": [
    "## AdaBoost Regressor\n",
    "\n",
    "AdaBoost Regressor is an extension of the AdaBoost algorithm for regression tasks. It works similarly to AdaBoost for classification but focuses on minimizing the prediction error for continuous target variables. Like its classification counterpart, AdaBoost Regressor builds an ensemble of weak learners in a sequential manner, where each new learner attempts to correct the errors of its predecessor.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Ensemble Learning\n",
    "\n",
    "Ensemble learning combines multiple models to produce a single robust model. AdaBoost Regressor is an ensemble technique that builds a series of models where each new model focuses more on the instances with larger prediction errors by adjusting their weights.\n",
    "\n",
    "#### 2. Boosting\n",
    "\n",
    "Boosting is an ensemble method that combines weak learners to create a strong learner. In the context of AdaBoost Regressor, boosting refers to the iterative process where each subsequent model is trained with a focus on the instances that were poorly predicted by the previous models.\n",
    "\n",
    "### Steps Involved in AdaBoost Regressor\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The AdaBoost Regressor process begins by initializing the weights of the training instances equally. If there are $ N $ training instances, each weight $ w_i $ is initialized to:\n",
    "$$ w_i = \\frac{1}{N} $$\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "AdaBoost Regressor constructs an ensemble of weak learners in a sequential manner. At each iteration $ m $:\n",
    "\n",
    "**Step 2-1: Train Weak Learner**\n",
    "\n",
    "- Train a weak learner $ h_m(x) $ on the weighted training data.\n",
    "\n",
    "**Step 2-2: Compute Error**\n",
    "\n",
    "- Compute the weighted error $ \\epsilon_m $ of the weak learner:\n",
    "$$ \\epsilon_m = \\frac{\\sum_{i=1}^N w_i |y_i - h_m(x_i)|}{\\sum_{i=1}^N w_i} $$\n",
    "where $ y_i $ is the true value and $ h_m(x_i) $ is the prediction by the weak learner.\n",
    "\n",
    "**Step 2-3: Compute Learner Weight**\n",
    "\n",
    "- Compute the weight $ \\alpha_m $ of the weak learner, which measures the importance of the learner in the final model:\n",
    "$$ \\alpha_m = \\eta \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right) $$\n",
    "where $ \\eta $ is the learning rate.\n",
    "\n",
    "**Step 2-4: Update Weights**\n",
    "\n",
    "- Update the weights of the training instances. Instances with larger prediction errors are given more weight:\n",
    "$$ w_i \\leftarrow w_i \\exp(\\alpha_m |y_i - h_m(x_i)|) $$\n",
    "\n",
    "- Normalize the weights to ensure they sum to 1:\n",
    "$$ w_i \\leftarrow \\frac{w_i}{\\sum_{i=1}^N w_i} $$\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $ M $ iterations, the final boosted model $ F(x) $ is a weighted sum of the weak learners:\n",
    "$$ F(x) = \\sum_{m=1}^M \\alpha_m h_m(x) $$\n",
    "\n",
    "### Expanded Explanation of $ F(x) $\n",
    "\n",
    "Let's expand on each part of the equation $ F(x) = \\sum_{m=1}^M \\alpha_m h_m(x) $:\n",
    "\n",
    "- **Weak Learner $ h_m(x) $**: Each weak learner is a simple model, such as a decision stump (a tree with a single split), that is trained on the weighted training data.\n",
    "\n",
    "- **Learner Weight $ \\alpha_m $**: This weight indicates the importance of the weak learner in the final model. It is computed based on the weighted error of the learner, giving more weight to more accurate learners.\n",
    "\n",
    "- **Model Aggregation $ \\sum_{m=1}^M \\alpha_m h_m(x) $**: The final model is a linear combination of all the weak learners, weighted by their respective importance.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "To provide a concrete example, consider the first few iterations of the AdaBoost Regressor process for a regression task:\n",
    "\n",
    "1. **Initial Weights $ w_i $**:\n",
    "   - Suppose we have 4 training instances. Each weight is initialized to $ \\frac{1}{4} = 0.25 $.\n",
    "\n",
    "2. **First Iteration (m = 1)**:\n",
    "   - Train a weak learner $ h_1(x) $.\n",
    "   - Compute the weighted error $ \\epsilon_1 $.\n",
    "   - Compute the learner weight $ \\alpha_1 = \\eta \\ln\\left(\\frac{1 - \\epsilon_1}{\\epsilon_1}\\right) $.\n",
    "   - Update the weights $ w_i $.\n",
    "\n",
    "3. **Second Iteration (m = 2)**:\n",
    "   - Train a new weak learner $ h_2(x) $ on the updated weights.\n",
    "   - Compute the weighted error $ \\epsilon_2 $.\n",
    "   - Compute the learner weight $ \\alpha_2 = \\eta \\ln\\left(\\frac{1 - \\epsilon_2}{\\epsilon_2}\\right) $.\n",
    "   - Update the weights $ w_i $.\n",
    "\n",
    "This process continues for $ M $ iterations, with each iteration aiming to focus more on the instances with larger errors and improve the model's accuracy.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in AdaBoost Regressor include:\n",
    "\n",
    "- **n_estimators**: Number of boosting stages (i.e., the number of weak learners).\n",
    "- **learning_rate**: A factor that multiplies the weight $ \\alpha_m $ of each weak learner. It controls the contribution of each weak learner and helps in preventing overfitting.\n",
    "- **base_estimator**: The type of weak learner to use (e.g., decision stump).\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement.\n",
    "2. **Versatility**: Can be used with various weak learners.\n",
    "3. **Effectiveness**: Often performs well even with simple weak learners.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Sensitivity to Noisy Data**: Can be sensitive to noisy data and outliers.\n",
    "2. **Overfitting**: Can overfit if the weak learners are too complex or if there are too many iterations.\n",
    "3. **Computationally Intensive**: Can be slow to train, especially with a large number of weak learners.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how AdaBoost Regressor can be implemented using Scikit-Learn in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the model\n",
    "base_estimator = DecisionTreeRegressor(max_depth=3)  # A simple decision tree\n",
    "ada = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = ada.predict(X_test)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "AdaBoost Regressor is a powerful and versatile boosting technique for regression tasks. By iteratively adjusting the weights of the training instances, it focuses more on difficult cases, resulting in a strong ensemble model. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbb468-9433-417f-ad37-e1c56ce6fb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
