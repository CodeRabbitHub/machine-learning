{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1f0805-61a7-47ba-b60e-22150c83be75",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes Classifier with Email Classification\n",
    "\n",
    "In the domain of email filtering, the Naive Bayes classifier assumes a pivotal role, efficiently distinguishing between legitimate emails and spam with exceptional accuracy. Let us delve into its intricacies and examine its significance in the realm of email classification.\n",
    "\n",
    "## Exploring the Basics\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Before delving into Naive Bayes, let's acquaint ourselves with Bayes' theorem, a cornerstone of probability theory:\n",
    "\n",
    "$$P(\\frac{A}{B}) = \\frac{P({B}/{A})P(A)}{P(B)}$$\n",
    "\n",
    "Where, \n",
    "\n",
    "- **$P(\\frac{A}{B})$**: This represents the probability of event $A$ occurring given that event $B$ has occurred. In other words, it's the likelihood of $A$ happening, given the context of $B$.\n",
    "\n",
    "- **$P(B|A)$**: This is the conditional probability of event $B$ occurring given that event $A$ has occurred. It represents the likelihood of observing $B$ under the condition that $A$ has already happened.\n",
    "\n",
    "- **$P(A)$**: This denotes the probability of event $A$ occurring independently, without any additional context or condition.\n",
    "\n",
    "- **$P(B)$**: Similarly, this denotes the probability of event $B$ occurring independently, without any additional context or condition.\n",
    "\n",
    "\n",
    "This theorem lays the groundwork for probabilistic inference, guiding our understanding of how evidence influences our beliefs.\n",
    "\n",
    "## Unveiling the 'Naive' Assumption  \n",
    "\n",
    "The term \"naive\" attached to Naive Bayes arises from its bold assumption regarding feature independence. It suggests that the existence or absence of one feature doesn't impact the presence or absence of another. While this assumption often doesn't hold true in reality, it surprisingly enhances the classifier's effectiveness.\n",
    "\n",
    "**Example Scenario**:  \n",
    "\n",
    "   - **Email 1 (Spam)**: \"Urgent offer! Get exclusive deals now!\"  \n",
    "   - **Email 2 (Legitimate)**: \"This is an urgent reminder about your appointment.\"  \n",
    "   - **Email 3 (Spam)**: \"Urgent! Amazing offer awaits! Act now!\"  \n",
    "\n",
    "In this scenario, both Email 1 and Email 3 contain the words \"urgent\" and \"offer\" together, which are indicative of spam.\n",
    "\n",
    "# Multinomial Naive Bayes classifier.\n",
    "\n",
    "#### Exploring Multinomial Naive Bayes for Email Classification\n",
    "\n",
    "Let's delve into a concise example to demonstrate the application of multinomial Naive Bayes in classifying emails as either spam or legitimate.\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "**Legitimate Emails:**\n",
    "1. Email 1: \"Hello, I am interested in your business proposal.\"\n",
    "2. Email 2: \"Please find attached the meeting agenda for tomorrow.\"\n",
    "3. Email 3: \"Reminder: Your appointment is scheduled for next week.\"\n",
    "\n",
    "**Spam Emails:**\n",
    "1. Email 1: \"Get rich quick! Buy our amazing products now!\"\n",
    "2. Email 2: \"Congratulations! You have won a free vacation.\"\n",
    "3. Email 3: \"Enlarge your bank account with our guaranteed investment plan.\"\n",
    "\n",
    "### Step 1: Count the Occurrences of Words\n",
    "\n",
    "#### Legitimate Emails:\n",
    "- Total words: 20  \n",
    "  word_count = {\n",
    "    \"Hello\": 1,\n",
    "    \"I\": 1,\n",
    "    \"am\": 1,\n",
    "    \"interested\": 1,\n",
    "    \"in\": 1,\n",
    "    \"your\": 1,\n",
    "    \"business\": 1,\n",
    "    \"proposal\": 1,\n",
    "    \"Please\": 1,\n",
    "    \"find\": 1,\n",
    "    \"attached\": 1,\n",
    "    \"the\": 1,\n",
    "    \"meeting\": 1,\n",
    "    \"agenda\": 1,\n",
    "    \"for\": 1,\n",
    "    \"tomorrow\": 1,\n",
    "    \"Reminder\": 1,\n",
    "    \"Your\": 1,\n",
    "    \"appointment\": 1,\n",
    "    \"is\": 1\n",
    "  }\n",
    "\n",
    "\n",
    "#### Spam Emails:\n",
    "- Total words: 21  \n",
    "  word_count = {\n",
    "    \"Get\": 1,\n",
    "    \"rich\": 1,\n",
    "    \"quick!\": 1,\n",
    "    \"Buy\": 1,\n",
    "    \"our\": 1,\n",
    "    \"amazing\": 1,\n",
    "    \"products\": 1,\n",
    "    \"now!\": 1,\n",
    "    \"Congratulations!\": 1,\n",
    "    \"You\": 1,\n",
    "    \"have\": 1,\n",
    "    \"won\": 1,\n",
    "    \"a\": 1,\n",
    "    \"free\": 1,\n",
    "    \"vacation\": 1,\n",
    "    \"Enlarge\": 1,\n",
    "    \"bank\": 1,\n",
    "    \"account\": 1,\n",
    "    \"with\": 1,\n",
    "    \"guaranteed\": 1,\n",
    "    \"investment\": 1,\n",
    "    \"plan\": 1\n",
    "  }\n",
    "\n",
    "\n",
    "### Step 2: Calculate Probabilities  \n",
    "\n",
    "**Note:** We use Laplace smoothing to avoid zero probabilities. Let's assume alpha (smoothing parameter) is 1. This means that for each word in our vocabulary, we add 1 to both the numerator and denominator when calculating probabilities. This ensures that even if a word did not appear in the training data for a particular class, it still has a non-zero probability of occurring in that class.\n",
    "\n",
    "#### Legitimate Emails:\n",
    "- Total words: 20\n",
    "- Prior probability (P(Legitimate)): 3/6 = 0.5\n",
    "- Word probabilities (P(word|Legitimate)):\n",
    "  - P(\"Hello\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  - P(\"tomorrow\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  - P(\"business\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  (and so on for other words)\n",
    "\n",
    "#### Spam Emails:\n",
    "- Total words: 21\n",
    "- Prior probability (P(Spam)): 3/6 = 0.5\n",
    "- Word probabilities (P(word|Spam)):\n",
    "  - P(\"Get\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  - P(\"rich\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  - P(\"quick!\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  (and so on for other words)\n",
    "\n",
    "### Step 3: Make Predictions\n",
    "\n",
    "Suppose we receive a new email: \"Urgent: Double your income with our exclusive offer!\"\n",
    "\n",
    "We calculate the probabilities for this email being legitimate and spam using Naive Bayes with Laplace smoothing and make a prediction based on the higher probability.\n",
    "\n",
    "- P(Legitimate) = 0.5 * P(\"Urgent\" | Legitimate) * P(\"Double\" | Legitimate) * ... * P(\"offer\" | Legitimate)\n",
    "- P(Spam) = 0.5 * P(\"Urgent\" | Spam) * P(\"Double\" | Spam) * ... * P(\"offer\" | Spam)\n",
    "\n",
    "Then, we compare P(Legitimate) and P(Spam) to classify the email as either legitimate or spam, based on which probability is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b554e30-15cb-446b-aedd-b22a58e92a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
