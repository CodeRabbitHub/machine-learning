{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1f0805-61a7-47ba-b60e-22150c83be75",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes Classifier with Email Classification\n",
    "\n",
    "In the domain of email filtering, the Naive Bayes classifier assumes a pivotal role, efficiently distinguishing between legitimate emails and spam with exceptional accuracy. Let us delve into its intricacies and examine its significance in the realm of email classification.\n",
    "\n",
    "## Exploring the Basics\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Before delving into Naive Bayes, let's acquaint ourselves with Bayes' theorem, a cornerstone of probability theory:\n",
    "\n",
    " $$P(\\frac{A}{B}) = \\frac{P({B}/{A})P(A)}{P(B)}$$\n",
    "\n",
    "Where, \n",
    "\n",
    "- **$P(\\frac{A}{B})$**: This represents the probability of event $A$ occurring given that event $B$ has occurred. In other words, it's the likelihood of $A$ happening, given the context of $B$.\n",
    "\n",
    "- **$P(B|A)$**: This is the conditional probability of event $B$ occurring given that event $A$ has occurred. It represents the likelihood of observing $B$ under the condition that $A$ has already happened.\n",
    "\n",
    "- **$P(A)$**: This denotes the probability of event $A$ occurring independently, without any additional context or condition.\n",
    "\n",
    "- **$P(B)$**: Similarly, this denotes the probability of event $B$ occurring independently, without any additional context or condition.\n",
    "\n",
    "\n",
    "This theorem lays the groundwork for probabilistic inference, guiding our understanding of how evidence influences our beliefs.\n",
    "\n",
    "## Unveiling the 'Naive' Assumption  \n",
    "\n",
    "The term \"naive\" attached to Naive Bayes arises from its bold assumption regarding feature independence. It suggests that the existence or absence of one feature doesn't impact the presence or absence of another. While this assumption often doesn't hold true in reality, it surprisingly enhances the classifier's effectiveness.\n",
    "\n",
    "**Example Scenario**:  \n",
    "\n",
    "   - **Email 1 (Spam)**: \"Urgent offer! Get exclusive deals now!\"  \n",
    "   - **Email 2 (Legitimate)**: \"This is an urgent reminder about your appointment.\"  \n",
    "   - **Email 3 (Spam)**: \"Urgent! Amazing offer awaits! Act now!\"  \n",
    "\n",
    "In this scenario, both Email 1 and Email 3 contain the words \"urgent\" and \"offer\" together, which are indicative of spam.\n",
    "\n",
    "# Multinomial Naive Bayes classifier.\n",
    "\n",
    "The Multinomial Naive Bayes classifier is a type of classification algorithm used for tasks where the input features are discrete counts. One common application is text classification, where it categorizes text documents based on the frequency of specific words in those documents. This classifier assumes that the input features are generated from a multinomial distribution, where each feature represents a count of a particular event or category.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The probability of the input features given the class can be calculated using the following formula:\n",
    "\n",
    "$ P(\\text{features} \\vert \\text{class}) = \\prod_{i} P(\\text{feature}_i \\vert \\text{class})^{count_i} $\n",
    "\n",
    "Where:\n",
    "- $ \\text{feature}_i $ is the $ i $-th input feature\n",
    "- $ \\text{count}_i $ is the count of $ \\text{feature}_i $ in the input data\n",
    "- $ P(\\text{feature}_i \\vert \\text{class}) $ is the probability of $ \\text{feature}_i $ occurring in the class\n",
    "\n",
    "This formula computes the likelihood of observing the input features given a specific class. It multiplies the probabilities of observing each feature in the class raised to the power of its count. The assumption of feature independence allows for the simplification of the joint probability into a product of individual probabilities.\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier\n",
    "\n",
    "The Bernoulli Naive Bayes classifier is similar to the Multinomial Naive Bayes classifier, but it assumes that the input features are binary (i.e., 0 or 1). One typical application is classifying images based on the presence or absence of certain features in the images. This classifier assumes that each feature is independent of the others given the class label, making it suitable for tasks like spam filtering or sentiment analysis.\n",
    "\n",
    "### Formula\n",
    "\n",
    "For the Bernoulli Naive Bayes classifier, the probability of the input features given the class is calculated using the following formula:\n",
    "\n",
    "$ P(\\text{features} \\vert \\text{class}) = \\prod_{i} P(\\text{feature}_i = 1 \\vert \\text{class})^{feature_i} \\times P(\\text{feature}_i = 0 \\vert \\text{class})^{(1 - feature_i)} $\n",
    "\n",
    "Where:\n",
    "- $ \\text{feature}_i $ is the $ i $-th input feature\n",
    "- $ \\text{feature}_i = 1 $ if $ \\text{feature}_i $ is present in the input data, 0 otherwise\n",
    "- $ P(\\text{feature}_i = 1 \\vert \\text{class}) $ is the probability of $ \\text{feature}_i $ being present in the class\n",
    "- $ P(\\text{feature}_i = 0 \\vert \\text{class}) $ is the probability of $ \\text{feature}_i $ being absent in the class\n",
    "\n",
    "This formula computes the likelihood of observing the input features given a specific class. It multiplies the probabilities of each feature being present or absent in the class, depending on whether the feature is present (1) or absent (0) in the input data.\n",
    "\n",
    "\n",
    "## Gaussian Naive Bayes Classifier\n",
    "\n",
    "The Gaussian Naive Bayes classifier is used for classification tasks where the input features are continuous and normally distributed. One typical application is classifying medical patients based on their age, height, and weight.\n",
    "\n",
    "### Formula\n",
    "\n",
    "For the Gaussian Naive Bayes classifier, the probability of the input features given the class is calculated using the following formula:\n",
    "\n",
    "$ P(\\text{features} \\vert \\text{class}) = \\prod_{i} \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\right) \\times \\exp \\left( -\\frac{(feature_i - \\mu_i)^2}{2\\sigma_i^2} \\right) $\n",
    "\n",
    "Where:\n",
    "- $ \\text{feature}_i $ is the $ i $-th input feature\n",
    "- $ \\mu_i $ is the mean of $ \\text{feature}_i $ for the class\n",
    "- $ \\sigma_i $ is the standard deviation of $ \\text{feature}_i $ for the class\n",
    "\n",
    "This formula computes the likelihood of observing the input features given a specific class. It calculates the probability density function (PDF) of each feature being observed in the class, assuming a normal distribution with mean $ \\mu_i $ and standard deviation $ \\sigma_i $.\n",
    "\n",
    "\n",
    "## Exploring Multinomial Naive Bayes for Email Classification\n",
    "\n",
    "Let's delve into a concise example to demonstrate the application of multinomial Naive Bayes in classifying emails as either spam or legitimate.\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "**Legitimate Emails:**\n",
    "1. Email 1: \"Hello, I am interested in your business proposal.\"\n",
    "2. Email 2: \"Please find attached the meeting agenda for tomorrow.\"\n",
    "3. Email 3: \"Reminder: Your appointment is scheduled for next week.\"\n",
    "\n",
    "**Spam Emails:**\n",
    "1. Email 1: \"Get rich quick! Buy our amazing products now!\"\n",
    "2. Email 2: \"Congratulations! You have won a free vacation.\"\n",
    "3. Email 3: \"Enlarge your bank account with our guaranteed investment plan.\"\n",
    "\n",
    "### Step 1: Count the Occurrences of Words\n",
    "\n",
    "#### Legitimate Emails:\n",
    "- Total words: 20  \n",
    "  word_count = {\n",
    "    \"Hello\": 1,\n",
    "    \"I\": 1,\n",
    "    \"am\": 1,\n",
    "    \"interested\": 1,\n",
    "    \"in\": 1,\n",
    "    \"your\": 1,\n",
    "    \"business\": 1,\n",
    "    \"proposal\": 1,\n",
    "    \"Please\": 1,\n",
    "    \"find\": 1,\n",
    "    \"attached\": 1,\n",
    "    \"the\": 1,\n",
    "    \"meeting\": 1,\n",
    "    \"agenda\": 1,\n",
    "    \"for\": 1,\n",
    "    \"tomorrow\": 1,\n",
    "    \"Reminder\": 1,\n",
    "    \"Your\": 1,\n",
    "    \"appointment\": 1,\n",
    "    \"is\": 1\n",
    "  }\n",
    "\n",
    "\n",
    "#### Spam Emails:\n",
    "- Total words: 21  \n",
    "  word_count = {\n",
    "    \"Get\": 1,\n",
    "    \"rich\": 1,\n",
    "    \"quick!\": 1,\n",
    "    \"Buy\": 1,\n",
    "    \"our\": 1,\n",
    "    \"amazing\": 1,\n",
    "    \"products\": 1,\n",
    "    \"now!\": 1,\n",
    "    \"Congratulations!\": 1,\n",
    "    \"You\": 1,\n",
    "    \"have\": 1,\n",
    "    \"won\": 1,\n",
    "    \"a\": 1,\n",
    "    \"free\": 1,\n",
    "    \"vacation\": 1,\n",
    "    \"Enlarge\": 1,\n",
    "    \"bank\": 1,\n",
    "    \"account\": 1,\n",
    "    \"with\": 1,\n",
    "    \"guaranteed\": 1,\n",
    "    \"investment\": 1,\n",
    "    \"plan\": 1\n",
    "  }\n",
    "\n",
    "\n",
    "### Step 2: Calculate Probabilities  \n",
    "\n",
    "**Note:** We use Laplace smoothing to avoid zero probabilities. Let's assume alpha (smoothing parameter) is 1. This means that for each word in our vocabulary, we add 1 to both the numerator and denominator when calculating probabilities. This ensures that even if a word did not appear in the training data for a particular class, it still has a non-zero probability of occurring in that class.\n",
    "\n",
    "#### Legitimate Emails:\n",
    "- Total words: 20\n",
    "- Prior probability (P(Legitimate)): 3/6 = 0.5\n",
    "- Word probabilities (P(word|Legitimate)):\n",
    "  - P(\"Hello\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  - P(\"tomorrow\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  - P(\"business\" | Legitimate) = (1 + 1) / (20 + 20) = 2/40\n",
    "  (and so on for other words)\n",
    "\n",
    "#### Spam Emails:\n",
    "- Total words: 21\n",
    "- Prior probability (P(Spam)): 3/6 = 0.5\n",
    "- Word probabilities (P(word|Spam)):\n",
    "  - P(\"Get\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  - P(\"rich\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  - P(\"quick!\" | Spam) = (1 + 1) / (21 + 21) = 2/42\n",
    "  (and so on for other words)\n",
    "\n",
    "### Step 3: Make Predictions\n",
    "\n",
    "Suppose we receive a new email: \"Urgent: Double your income with our exclusive offer!\"\n",
    "\n",
    "We calculate the probabilities for this email being legitimate and spam using Naive Bayes with Laplace smoothing and make a prediction based on the higher probability.\n",
    "\n",
    "- P(Legitimate) = 0.5 * P(\"Urgent\" | Legitimate) * P(\"Double\" | Legitimate) * ... * P(\"offer\" | Legitimate)\n",
    "- P(Spam) = 0.5 * P(\"Urgent\" | Spam) * P(\"Double\" | Spam) * ... * P(\"offer\" | Spam)\n",
    "\n",
    "Then, we compare P(Legitimate) and P(Spam) to classify the email as either legitimate or spam, based on which probability is higher.\n",
    "\n",
    "## Benefits of Using the Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes classifier is a simple yet powerful machine learning algorithm that offers several advantages for various classification tasks. Here’s a detailed breakdown of its key benefits:\n",
    "\n",
    "- **Simplicity and Ease of Implementation:** The Naive Bayes algorithm is remarkably straightforward to understand and implement. Its underlying mathematical principles are based on Bayes’ theorem, which is a fundamental concept in probability theory. This simplicity makes it an excellent choice for beginners and experienced practitioners alike.\n",
    "\n",
    "- **Efficiency and Speed:** The Naive Bayes classifier is known for its exceptional computational efficiency. Both the training and prediction processes are relatively fast, making it well-suited for real-time applications where quick classification decisions are crucial. This efficiency stems from the algorithm’s ability to directly compute probabilities without iterative optimization.\n",
    "\n",
    "- **Robustness to Noise and Outliers:** The Naive Bayes classifier demonstrates remarkable resilience against noisy data and outliers. Its inherent assumption of feature independence makes it less susceptible to the influence of irrelevant or misleading data points. This robustness is particularly valuable in real-world scenarios where data quality may not be pristine.\n",
    "\n",
    "- **Versatility and Applicability:** The Naive Bayes classifier is remarkably versatile and can be applied to a wide range of classification tasks involving different data types. It can effectively handle text data, image data, and numerical data, making it a general-purpose tool for various domains.\n",
    "\n",
    "- **Scalability to Large Datasets:** The Naive Bayes classifier scales well to large datasets without compromising its efficiency or performance. Its ability to handle high-dimensional data makes it suitable for large-scale classification problems.\n",
    "\n",
    "## Pitfalls of the Naive Bayes Classifier\n",
    "\n",
    "Despite its numerous advantages, the Naive Bayes classifier has certain limitations and potential drawbacks that should be considered when employing it:\n",
    "\n",
    "- **Assumption of Feature Independence:** The Naive Bayes classifier relies on the assumption of conditional independence, which states that the input features are independent of each other given the class label. In reality, this assumption is often violated, as features may exhibit dependencies or correlations. This assumption can lead to suboptimal performance in cases where feature dependencies are significant.\n",
    "\n",
    "- **Sensitivity to Zero-Frequency Events:** The Naive Bayes classifier can be sensitive to the presence of zero-frequency events, where a particular feature-value combination is not observed during training. This can lead to assigning zero probability to such events, hindering the classifier’s ability to make accurate predictions.\n",
    "\n",
    "- **Handling Non-Normal Data Distributions:** The Naive Bayes classifier, particularly the Gaussian Naive Bayes variant, assumes that the features within each class follow a normal distribution. This assumption may not hold true for all datasets, especially those involving non-numerical data. Deviations from normality can affect the classifier’s performance.\n",
    "\n",
    "- **Limited Performance in Complex Problems:** The Naive Bayes classifier may struggle with highly complex classification tasks, particularly those involving intricate relationships between features or non-linear decision boundaries. In such cases, more sophisticated algorithms may be more suitable.\n",
    "\n",
    "- **Potential for Overfitting:** Like any machine learning algorithm, the Naive Bayes classifier can be susceptible to overfitting, where it memorizes the training data too well and fails to generalize to unseen data. Careful evaluation and parameter tuning can help mitigate this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b554e30-15cb-446b-aedd-b22a58e92a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
