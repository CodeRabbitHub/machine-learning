{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79e64e7-72d6-4104-9e6b-a72c9ed9927f",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines (GBM)\n",
    "\n",
    "Gradient Boosting Machines (GBMs) are a powerful ensemble learning technique used primarily for regression and classification tasks. They build models in a sequential manner where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, a key aspect of optimization in machine learning.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Ensemble Learning\n",
    "\n",
    "Ensemble learning combines multiple models to produce a single robust model. GBM is an ensemble technique that builds a series of models where each new model corrects the errors of the preceding ones.\n",
    "\n",
    "#### 2. Boosting\n",
    "\n",
    "Boosting is an ensemble method that combines weak learners to create a strong learner. In the context of GBM, boosting refers to the iterative process where each subsequent model is trained to minimize the errors of the previous models.\n",
    "\n",
    "### Steps Involved in Gradient Boosting Machines\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation for Regression\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The GBM process begins by initializing the model with a constant value. For regression problems, this is typically the mean of the target values $y$. For classification problems, it could be the log-odds of the classes.\n",
    "\n",
    "For a regression task:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $ L $ is the loss function, such as Mean Squared Error (MSE), and $ N $ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For regression, typically squared loss is used.\n",
    "- **Initial Prediction ($F_0$):** We find $\\gamma$ that minimizes the sum of the loss function. For MSE, this $\\gamma$ turns out to be the mean of $y$.\n",
    "- **Derivation:** Taking the derivative of the sum of squared losses with respect to $\\gamma$ and setting it to zero gives us:\n",
    "  $$ \\frac{\\partial}{\\partial \\gamma} \\sum_{i=1}^N (y_i - \\gamma)^2 = 0 $$\n",
    "  $$ -2 \\sum_{i=1}^N (y_i - \\gamma) = 0 $$\n",
    "  $$ 2N\\gamma = 2 \\sum_{i=1}^N y_i $$\n",
    "  $$ N\\gamma = \\sum_{i=1}^N y_i $$\n",
    "  $$ \\gamma = \\frac{1}{N} \\sum_{i=1}^N y_i $$\n",
    "\n",
    "This result shows that the optimal constant prediction $\\gamma$ for minimizing the sum of squared errors is the average or the mean of the observed values i.e $F_0(x) =\\gamma = \\bar{y}$.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "GBM constructs an ensemble of trees in a sequential manner. At each iteration $m$:\n",
    "\n",
    "**Step 2-1: Calculate Residuals**\n",
    "\n",
    "- Compute the pseudo-residuals $r_{im}$, which are the gradients of the loss function with respect to the predictions:\n",
    "$$ r_{im} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For squared loss, the residuals simplify to:\n",
    "$$ r_{im} = y_i - F_{m-1}(x_i) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Residuals ($r_{im}$):** These are the negative gradients of the loss function and represent the difference between the actual and predicted values.\n",
    "- **Interpretation:** These residuals are used as the new target values for the next tree. They guide the model on how to adjust its predictions to reduce the overall error.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a weak learner $h_m(x)$ (often a decision tree) to these pseudo-residuals by minimizing the loss:\n",
    "$$ h_m(x) = \\arg\\min_h \\sum_{i=1}^N L(r_{im}, h(x_i)) $$\n",
    "\n",
    "**Step 2-3: Compute Terminal Node Values**\n",
    "\n",
    "- For each terminal node $j$ in the tree $h_m$, compute the optimal value $\\gamma_{jm}$ that minimizes the loss:\n",
    "$$ \\gamma_{jm} = \\arg\\min_\\gamma \\sum_{x_i \\in R_{jm}} L(r_{im}, \\gamma) $$\n",
    "\n",
    "For squared loss, $\\gamma_{jm}$ is the mean of the residuals in the terminal node $R_{jm}$:\n",
    "$$ \\gamma_{jm} = \\frac{1}{n_j} \\sum_{x_i \\in R_{jm}} r_{im} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Terminal Node Value ($\\gamma_{jm}$):** This is the value added to the predictions of all samples in the terminal node. For regression, itâ€™s the mean of residuals in that node.\n",
    "- **Derivation:** Taking the derivative of the loss function within each terminal node and setting it to zero, we get the mean of residuals as the optimal value.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted weak learner, scaled by a learning rate $\\eta$:\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($\\eta$):** This controls the contribution of each new tree to the final model. It helps in preventing overfitting.\n",
    "- **Model Update:** The new prediction $F_m(x)$ is the previous prediction $F_{m-1}(x)$ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "#### 3. Model Update\n",
    "\n",
    "The model is updated by adding the new tree to the ensemble. This process is repeated for a specified number of iterations $M$.\n",
    "\n",
    "#### 4. Final Prediction\n",
    "\n",
    "The final prediction is the sum of the predictions from all the weak learners, each scaled by the learning rate $\\eta$. Each weak learner $h_m(x)$ is trained to correct the errors (residuals) of the previous ensemble.\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) = F_0(x) + \\eta h_1(x) + \\eta h_2(x) + \\ldots + \\eta h_M(x) $$\n",
    "\n",
    "Each term $\\eta h_m(x)$ represents the contribution of the $m$-th tree, scaled by the learning rate. The learning rate $\\eta$ ensures that the model does not overfit by controlling the step size of each update.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "To provide a concrete example, consider the first few iterations of the gradient boosting process:\n",
    "\n",
    "1. **Initial Prediction $F_0(x)$**:\n",
    "   - Suppose we have the target values $y = [3, 4, 5, 6]$.\n",
    "   - The initial prediction is the mean: $F_0(x) = 4.5$.\n",
    "\n",
    "2. **First Iteration (m = 1)**:\n",
    "   - Compute residuals: $r_{i1} = y_i - F_0(x_i) = [3 - 4.5, 4 - 4.5, 5 - 4.5, 6 - 4.5] = [-1.5, -0.5, 0.5, 1.5]$.\n",
    "   - Fit a tree $h_1(x)$ to these residuals.\n",
    "   - Compute optimal $\\gamma_{j1}$ for each terminal node.\n",
    "   - Update the model: $F_1(x) = F_0(x) + \\eta h_1(x)$.\n",
    "\n",
    "3. **Second Iteration (m = 2)**:\n",
    "   - Compute new residuals: $r_{i2} = y_i - F_1(x_i)$.\n",
    "   - Fit a tree $h_2(x)$ to these residuals.\n",
    "   - Compute optimal $\\gamma_{j2}$ for each terminal node.\n",
    "   - Update the model: $F_2(x) = F_1(x) + \\eta h_2(x)$.\n",
    "\n",
    "This process continues for $M$ iterations, with each iteration aiming to reduce the residuals and improve the model's accuracy.\n",
    "\n",
    "### Illustration\n",
    "\n",
    "<center><img src=\"fig/gbm.png\"/></center>\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in GBM include:\n",
    "\n",
    "- **n_estimators**: Number of boosting stages (i.e., the number of trees).\n",
    "- **learning_rate**: Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **max_depth**: Maximum depth of individual trees.\n",
    "- **min_samples_split**: Minimum number of samples required to split an internal node.\n",
    "- **min_samples_leaf**: Minimum number of samples required to be at a leaf node.\n",
    "- **subsample**: Fraction of samples used for fitting individual trees. Reducing this can improve generalization.\n",
    "- **loss function**: The loss function to be minimized, such as MSE for regression or log-loss for classification.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Accuracy**: GBM often achieves high accuracy on complex datasets.\n",
    "2. **Flexibility**: Can handle various types of data and different loss functions.\n",
    "3. **Feature Importance**: Provides insights into the importance of features.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Training Time**: Can be time-consuming due to the sequential nature of training.\n",
    "2. **Overfitting**: Prone to overfitting if not properly tuned.\n",
    "3. **Complexity**: More complex than simple models and harder to interpret.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how GBM can be implemented using popular libraries like Scikit-Learn or XGBoost in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the model\n",
    "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gbm.predict(X_test)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Gradient Boosting Machines are a powerful and flexible tool for both regression and classification tasks. They iteratively build an ensemble of weak learners, correcting errors at each step using gradient descent. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932853c-7ae5-4efc-a83a-d83087ab2d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
