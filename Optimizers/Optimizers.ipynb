{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9319becf-09f5-46a2-9c0b-8752b513f2a5",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm used to minimize a function by iteratively moving towards the direction of steepest descent. In the context of machine learning, it's commonly employed to optimize the parameters of a model to minimize a given loss function.\r\n",
    "\r\n",
    "Mathematical Representation:\r\n",
    "The algorithm updates the parameters $w$ in the following manner:\n",
    "\n",
    "$$ g_{t} = \\nabla_{w} f(w_{t}) $$\n",
    "\n",
    "Where:   \n",
    "- $ g_t $ is gradients of loss $ f $ w.r.t parameters $ w $  \n",
    "- $ \\nabla_{t} f(w_{t}) = \\left( \\frac{\\partial f}{\\partial w_1}(w_{t}), \\frac{\\partial f}{\\partial w_2}(w_{t}), \\ldots, \\frac{\\partial f}{\\partial w_n}(w_{t}) \\right) $\n",
    "\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta g_t $$\n",
    "\n",
    "Where:  \n",
    "- $ t $ : step number  \n",
    "- $ w_t $ : current parameters at step $ t $  \n",
    "- $ w_{t+1} $ : updated parameters  \n",
    "- $ g_t $ : gradients at step $ t $, i.e.,\n",
    "- $ \\eta $ : is learning r\n",
    "\n",
    "#### Drawback\n",
    "\n",
    "One of the drawbacks of Gradient Descent is that it requires computing the gradient for the entire dataset for each update. This can be computationally expensive, especially for large datasets.ta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a8dce-fcb9-48e6-b9c4-ca01b625d8e7",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent\n",
    "\n",
    "#### Key Features:  \n",
    "\n",
    "**Incremental Updates:** Parameters are updated after processing each sample, making it computationally more efficient compared to Gradient Descent, especially for large datasets.  \n",
    "\n",
    "**Random Sampling:** SGD typically involves shuffling the dataset and then sequentially sampling one instance at a time to update the parameters. This randomness can help in escaping local minima and reaching the global minimum more efficiently.\n",
    "\n",
    "#### Cons:\n",
    "**Noisy Updates:** Since each update is based on a single sample, the updates can be noisy and may not necessarily decrease the loss function monotonically.  \n",
    "**Convergence:** Due to the stochastic nature of SGD, convergence to the optimal solution may be slower compared to Gradient Descent. However, it's often used in practice due to its efficiency and ability to handle large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a734b-0bcb-4775-a1cc-1522454f0346",
   "metadata": {},
   "source": [
    "## 3. Minibatch Gradient Descent\n",
    "\n",
    "#### Key Features:  \n",
    "**Efficient Updates:** Parameters are updated after processing a mini-batch of samples, which strikes a balance between the efficiency of SGD and the stability of Gradient Descent. This approach often results in faster convergence compared to pure SGD.\n",
    "\n",
    "#### Advantage:  \n",
    "**Efficiency:** MGD, along with SGD, performs more frequent updates compared to Gradient Descent, which can lead to faster convergence, especially for large datasets.  \n",
    "\n",
    "\n",
    "#### Disadvantages:\n",
    "**Slow Convergence:** Although MGD is more efficient than GD, it can still suffer from slow convergence. This is because throughout training, some weights may have steeper slopes and require larger updates, while others may have flatter slopes, requiring smaller updates. This imbalance can slow down convergence.  \n",
    "**Oscillating Updates:** The addition of randomness in the updates (especially in SGD and MiniBatch variants) can sometimes cause the gradients to oscillate, making convergence difficult.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657294bc-14d4-45d9-9248-0581194ebb67",
   "metadata": {},
   "source": [
    "## 4. Momentum Optimizer\n",
    "\n",
    "Momentum optimization is a technique commonly used to accelerate the convergence of gradient-based optimization algorithms, such as Gradient Descent variants. It introduces a momentum term that accumulates gradients over time and adjusts the parameter updates accordingly.\n",
    "\n",
    "Mathematical Representation:\n",
    "The update rule for the parameters $w$ in Momentum Optimizer is given by:\n",
    "\n",
    "$$w_{t+1} = w_t + m_t$$\n",
    "$$m_t = \\beta m_{t-1} - \\eta g_{t}$$\n",
    "\n",
    "Where:\n",
    "- $m_t$ : Exponentially decaying moving average of past gradients.\n",
    "- $\\beta$ : is another paratemeter called (**decay coefficient** ranging between [0,1]) which tells us how fast we can forget about the past gradients\n",
    "- $\\eta$ : is learning rate.\n",
    "\n",
    "\n",
    "    Initialize --> $m_0 = 0$  \n",
    "    Step 1 --> $m_1 = -\\eta g_1$  \n",
    "    Step 2 --> $m_2 = \\beta m_1 - \\eta g_2 = -\\eta(\\beta g_1 + g_2)$  \n",
    "     .  \n",
    "     .  \n",
    "     .  \n",
    "    Step $\\tau$ --> $m_{\\tau}$ = $-\\eta(\\beta^{\\tau-1} g_1 + \\beta^{\\tau-2} g_2$ $+ ..... +$ $g_{\\tau})$\n",
    "\n",
    "\n",
    "- $g_1$ has smaller coefficient than $g_2$ and so on.. thats why we called it exponentially decaying moving average of past gradients.\n",
    "\n",
    "- Momentum optimization helps in accelerating convergence, especially in scenarios where the gradients fluctuate significantly or the loss surface has long, narrow valleys.\n",
    "\n",
    "- It's important to tune the hyperparameters, particularly the learning rate $\\eta$ and the decay coefficient $\\beta$, to achieve optimal performance for a given optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb1dd7-93da-42ab-b852-4f51e0874b67",
   "metadata": {},
   "source": [
    "## 5. Nestrov Optimizer\n",
    "\n",
    "Nesterov Accelerated Gradient (NAG) is an enhancement of the Momentum optimizer that improves convergence by employing a \"look-ahead\" approach for gradient computation. It adjusts the gradient calculation by partially updating the parameters before evaluating the gradient.\n",
    "\n",
    "Mathematical Representation:\n",
    "The update rule for the parameters $w$ in Nesterov Optimizer is given by:\n",
    "\n",
    "$$w_{t+1} = w_t + m_t$$\n",
    "$$m_t = \\beta m_{t-1} - \\eta ∇_w f(w_t + \\beta m_{t-1})$$\n",
    "\n",
    "- $∇_w f(w_t + \\beta m_{t-1})$ - Nestrov Accelerated Gradients (NAG)\n",
    "\n",
    "#### Explanation:\n",
    "- Instead of directly computing the gradients at time step $t$, Nesterov optimizer uses a look-ahead approach by performing a partial update ($w_t + \\beta m_{t-1}$) of the parameters before computing the gradients.  \n",
    "- The gradient is then calculated based on the partially updated parameters.  \n",
    "- This helps in correcting the gradient direction more accurately, especially in scenarios where the momentum term might lead the optimization astray.  \n",
    "- The parameter update is then adjusted using the momentum term $m_t$, similar to the Momentum optimizer.    \n",
    "#### Performance:\n",
    "Nesterov Optimizer, with its Nesterov Accelerated Gradients (NAG), often outperforms traditional SGD and Momentum methods in terms of convergence speed and efficiency, especially for deep learning tasks with complex loss surfaces.  \n",
    "\n",
    "#### Note:\n",
    "Tuning the hyperparameters, particularly the learning rate $\\eta$ and the decay coefficient $\\beta$, is crucial for achieving optimal performance with Nesterov Optimizer. Additionally, it's important to consider the computational overhead of the look-ahead approach, especially for large-scale optimization tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca82500-7367-4c7c-b447-2286d8c78058",
   "metadata": {},
   "source": [
    "# Learning Rate Otimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab1a6d-ed49-410b-b8ed-7b590cb7cfc9",
   "metadata": {},
   "source": [
    "## 6. AdaGrad Optimizer\n",
    "AdaGrad (Adaptive Gradient Algorithm) is an optimization algorithm that adapts the learning rate for each parameter based on the history of gradients. It aims to provide a larger learning rate for infrequent parameters and a smaller learning rate for frequent parameters.\n",
    "\n",
    "Mathematical Representation:\n",
    "The update rule for the parameters $w$ in AdaGrad Optimizer is given by:\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} g_{t}$$\n",
    "\n",
    "- Global learning rate $\\eta$ is divided by square root of $v_t$ which is running average of the squared gradients.\n",
    "$$v_t = v_{t-1} + g^{2}_{t}$$\n",
    "\n",
    "#### Explanation:\n",
    "- AdaGrad adapts the learning rate individually for each parameter based on the history of gradients.\n",
    "- The learning rate is scaled by the square root of the accumulated squared gradients, which effectively reduces the learning rate for parameters with frequently occurring large gradients and increases it for parameters with infrequent large gradients.\n",
    "- The accumulation of squared gradients allows the optimizer to decay the effective learning rate over time for each parameter, thus adapting the learning rate to the specific requirements of each parameter.\n",
    "#### Problems:\n",
    "- Sensitive to Initial Gradients: At the beginning of training (t=0), the accumulation $v_0$ is initialized with 0. This initialization makes the optimizer sensitive to the initial gradients, potentially affecting convergence.\n",
    "- Cumulative Accumulation: The accumulation of squared gradients, $v_{\\tau} = g^{2}{1}+g^{2}{2}+....g^{2}_{\\tau}$ increases with each step. This cumulative effect can lead to extremely small effective learning rates over time, slowing down the optimization process, especially in long training sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011e568-1619-4dc5-abfb-c2914cf58c5a",
   "metadata": {},
   "source": [
    "## 7. RMSProp Otimizer - Root Mean Square Propagation\n",
    "\n",
    "RMSProp (Root Mean Square Propagation) is an optimization algorithm that addresses some of the limitations of AdaGrad by using an exponentially decaying moving average of squared gradients. It aims to provide more stable and adaptive learning rates during training.\n",
    "\n",
    "Mathematical Representation:\n",
    "The update rule for the parameters $w$ in RMSProp Optimizer is given by:\n",
    "\n",
    "$$w_{t+1} = w_{t}+ \\triangle w_{t}$$\n",
    "$$\\triangle w_{t} = -\\frac{\\eta}{RMS(g_t)}.g_t$$\n",
    "$$RMS(g_t) = \\sqrt{v_t + \\epsilon}$$\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta)g^2_t$$\n",
    "\n",
    "#### Explanation:\n",
    "- RMSProp addresses the sensitivity to initial gradients and the issue of monotonically decreasing learning rates observed in AdaGrad.\n",
    "- Instead of accumulating all past squared gradients, RMSProp computes an exponentially decaying moving average of squared gradients using the parameter $\\beta$.\n",
    "- This moving average helps to stabilize and adaptively adjust the learning rates for each parameter during training.\n",
    "- The RMS term in the update rule normalizes the gradient by the square root of the moving average of squared gradients, providing a more balanced and stable learning rate.\n",
    "#### Benefits:\n",
    "- Less Sensitive to Initial Gradients: By using an exponentially decaying moving average of squared gradients, RMSProp is less sensitive to the initial gradients encountered during training.\n",
    "- Avoids Monotonically Decreasing Learning Rates: The adaptive learning rates provided by RMSProp help avoid the issue of monotonically decreasing learning rates, which can occur in AdaGrad due to the accumulation of squared gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218ba97-52b8-4e56-892b-a55771a46ed6",
   "metadata": {},
   "source": [
    "## 8. AdaDelta Optimizer  \n",
    "\n",
    "AdaDelta is an extension of the RMSProp optimizer that aims to overcome its reliance on a global learning rate $\\eta$. It achieves this by using a running average of the parameter updates to adaptively adjust the learning rates.\n",
    "\n",
    "Mathematical Representation:\n",
    "The update rule for the parameters $w$ in AdaDelta Optimizer is given by:\n",
    "\n",
    "$$w_{t+1} = w_{t}+ \\triangle w_{t}$$\n",
    "$$\\triangle w_{t} = -\\frac{RMS(\\triangle w_{t-1})}{RMS(g_t)} g_t$$\n",
    "\n",
    "$$RMS(\\triangle w_{t}) = \\sqrt{u_t + \\epsilon}$$\n",
    "$$u_t = \\beta u_{t-1} + (1-\\beta)(\\triangle w_t)^2$$\n",
    "\n",
    "$$RMS(g_t) = \\sqrt{v_t + \\epsilon}$$\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta)(g_t)^2$$\n",
    "\n",
    "#### Explanation:\n",
    "- AdaDelta addresses the issues of sensitivity to initial gradients and continually decreasing learning rates observed in some optimization algorithms.\n",
    "- By using a running average of the parameter updates, AdaDelta adapts the learning rates on a per-parameter basis without requiring a global learning rate $\\eta$.\n",
    "- The update rule for $\\triangle w_{t}$ is scaled by the ratio of the root mean square of the past parameter updates to the root mean square of the gradients.\n",
    "- This normalization helps to stabilize the learning rates and avoid overly aggressive or overly conservative updates.\n",
    "- AdaDelta effectively removes the need for manual tuning of the learning rate hyperparameter, making it more convenient to use and less sensitive to the choice of hyperparameters.\n",
    "#### Benefits:\n",
    "- Less Sensitive to Initial Gradients: AdaDelta, like RMSProp, is less sensitive to the initial gradients encountered during training, thanks to its adaptive learning rates.\n",
    "- Avoids Continually Decreasing Learning Rates: By adapting the learning rates on a per-parameter basis, AdaDelta avoids the issue of continually decreasing learning rates observed in some optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a0293-ac69-46cb-a653-3686282c01b7",
   "metadata": {},
   "source": [
    "# Adaptive Moment Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab93cef-dafb-4c9b-bd59-083282c4b746",
   "metadata": {},
   "source": [
    "## 9. Adam Optimizer  \n",
    "\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm that computes adaptive learning rates for each parameter by estimating the first and second moments of the gradients. It combines the concepts of momentum optimization and RMSProp, providing adaptive learning rates along with momentum.  \n",
    "\n",
    "Mathematical Representation:  \n",
    "Adam maintains two moving averages:  \n",
    "\n",
    "+ The first moment $m_t$, which is the exponentially decaying moving average of gradients.  \n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
    "\n",
    "+ The second moment $v_t$, which is the exponentially decaying moving average of squared gradients.  \n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g^2_t$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters controlling the decay rates of the moving averages, typically close to 1 but less than 1.  \n",
    "- $g_t$ is the gradient at time step $t$.  \n",
    "- $\\epsilon$ is a small value added to prevent division by zero.  \n",
    "\n",
    "$\\beta_1, \\beta_2  \\epsilon [0,1)$   \n",
    "e.g.,  $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$  \n",
    "\n",
    "Adam also incorporates bias correction to remove initialization bias:  \n",
    "\n",
    "$$\\hat{m_t} = \\frac{m_t}{1-\\beta^t_1}$$ \n",
    "$$\\hat{v_t} = \\frac{v_t}{1-\\beta^t_2}$$\n",
    "\n",
    "$$w_t = w_{t-1} - \\eta \\frac{\\hat{m_t}}{{\\sqrt{\\hat{v_t}} + \\epsilon}}$$  \n",
    "\n",
    "#### Explanation:\n",
    "- Adam combines the concepts of momentum optimization (using $m_t$) and RMSProp (using $v_t$) to provide adaptive learning rates for each parameter.\n",
    "- The first moment $m_t$ represents the average gradient, providing momentum to the parameter updates.\n",
    "- The second moment $v_t$ represents the uncentered variance of the gradients, adapting the learning rates based on the magnitude and direction of the gradients.\n",
    "- Bias correction is applied to account for the initialization bias of $m_t$ and $v_t$.\n",
    "- The parameters are updated using the bias-corrected first moment divided by the square root of the bias-corrected second moment, scaled by the global learning rate $\\eta$.\n",
    "#### Benefits:\n",
    "- Invariance to Diagonal Rescaling: Adam is invariant to diagonal rescaling of gradients, making it suitable for a wide range of optimization problems.\n",
    "- Suitable for Online and Non-Stationary Problems: Adam adapts to changes in the optimization landscape, making it suitable for online learning and non-stationary optimization problems.\n",
    "- Handling Noisy and Sparse Gradients: Adam's adaptive learning rates help in handling noisy and sparse gradients commonly encountered in deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268811f7-00f7-476f-bfe5-f36b26d93ce5",
   "metadata": {},
   "source": [
    "## 10. AdaMax Optimizer\n",
    "\n",
    "AdaMax is a variant of the Adam optimizer that generalizes the concept of the second moment to the $L^{\\infty}$ norm of the gradients. This adaptation aims to provide a more stable and effective optimization algorithm, particularly for deep learning tasks.\n",
    " \n",
    "Mathematical Representation:  \n",
    "AdaMax maintains two moving averages similar to Adam:  \n",
    "- The first moment $m_t$, which is the exponentially decaying moving average of gradients.\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
    "\n",
    "Generalizing Adam $L^p$ --> norm of the gradients (p can be 2 or higher)\n",
    "\n",
    "$$v_t = \\beta^p_2 v_{t-1} + (1-\\beta^p_2)|g_t|^p$$\n",
    "\n",
    "For Adam: $p = 2$  \n",
    "For AdaMax: $p \\to \\infty$  --> inifinity norm of gradients --> $u_t$\n",
    "- The generalized $L^{\\infty}$ norm of the gradients, denoted as $u_t$, which represents the maximum absolute value of the gradients encountered so far.\n",
    "\n",
    "$$u_t = max(\\beta_2 u_{t-1}, |g_t|)$$\n",
    "\n",
    "$$\\hat{m_t} = \\frac{m_t}{1-\\beta^t_1}$$ \n",
    "\n",
    "$$w_t = w_{t-1} - \\eta \\frac{\\hat{m_t}}{u_t}$$\n",
    "\n",
    "#### Explanation:\n",
    "- AdaMax extends the concept of the second moment in Adam by generalizing it to the $L^{\\infty}$ norm of the gradients.\n",
    "- The $L^{\\infty}$ norm represents the maximum absolute value of the gradients encountered during training, providing a stable and adaptive learning rate for each parameter.\n",
    "- The parameter update is scaled by the ratio of the bias-corrected first moment to the $L^{\\infty}$ norm of the gradients, similar to the update rule in Adam.\n",
    "- AdaMax offers an alternative approach to adaptive learning rate optimization, particularly suitable for scenarios where the maximum gradient magnitude is of interest.\n",
    "#### Benefits:\n",
    "- Stable and Adaptive Learning Rates: AdaMax provides stable and adaptive learning rates based on the maximum absolute value of the gradients encountered during training.\n",
    "- Alternative to Adam: AdaMax offers an alternative to Adam, particularly suited for scenarios where the $L^{\\infty}$ norm of the gradients is of importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3044fca-9236-458c-b787-795a5866cb37",
   "metadata": {},
   "source": [
    "# 11. AMSGrad Optimizer\n",
    "\n",
    "AMSGrad (Adaptive Moment Estimation with Stable Gradients) is an optimization algorithm that addresses the convergence issues observed in some adaptive learning rate methods like Adam and AdaDelta. It achieves this by modifying the update rule for the second moment to ensure stability and prevent excessive growth.\n",
    "\n",
    "Mathematical Representation:\n",
    "AMSGrad modifies the update rule for the second moment compared to Adam and AdaDelta. Instead of applying bias correction, it takes the maximum of the past second moment and the current second moment: $$\\hat{v}_t= max(\\hat{v}_{t-1}, v_t)$$\n",
    "\n",
    "The update rule for the parameters $w$ in AMSGrad is similar to Adam:\n",
    "\n",
    "$$w_t = w_{t-1} - \\eta \\frac{\\hat{m_t}}{{\\sqrt{\\hat{v_t}} + \\epsilon}}$$ \n",
    "\n",
    "#### Explanation:\n",
    "- AMSGrad modifies the update rule for the second moment to prevent its excessive growth, which can lead to convergence issues observed in Adam and AdaDelta.\n",
    "- By taking the maximum of the past second moment and the current second moment, AMSGrad ensures stability and prevents the learning rate from decreasing too quickly.\n",
    "- The parameter update rule is similar to Adam, with the bias-corrected first moment divided by the square root of the modified second moment, scaled by the global learning rate.\n",
    "#### Benefits:\n",
    "- Improved Stability: AMSGrad ensures stability by modifying the update rule for the second moment, preventing its excessive growth.\n",
    "- Convergence: By addressing the convergence issues observed in some adaptive learning rate methods, AMSGrad aims to provide more reliable convergence during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1166fb-5610-4eb1-b393-65784df6c97f",
   "metadata": {},
   "source": [
    "# 12. AdaBound Optimizer\n",
    "\n",
    "AdaBound is an optimization algorithm that dynamically clips the learning rates during training to keep them within a desired range. It is inspired by the behavior of adaptive learning rate methods like Adam and combines it with the concept of learning rate clipping to achieve more stable and controlled optimization.\n",
    "\n",
    "Mathematical Representation:\n",
    "AdaBound maintains the first and second moments similar to Adam:\n",
    "\n",
    "- The first moment $m_t$:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
    "\n",
    "- The second moment $v_t$:\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g^2_t$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters controlling the decay rates of the moving averages, typically close to 1 but less than 1.\n",
    "\n",
    "AdaBound then dynamically clips the learning rate $\\eta_t'$ to keep it within a desired range:  \n",
    "$${\\eta_t}' = Clip \\biggl( \\frac{\\eta}{\\sqrt{v_t}}, \\eta_{lower}(t), \\eta_{upper}(t) \\biggl) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\eta$ : global learning rate  \n",
    "- $\\eta_{lower}(t)$ : lower bound  \n",
    "- $\\eta_{upper}(t)$ : upper bound  \n",
    "\n",
    "The clipped learning rate $\\eta_t'$ is further scaled by the square root of the time step $t$ to provide a decaying effect:\n",
    "\n",
    "$$\\hat{\\eta_t} = \\frac{{\\eta_t}'}{\\sqrt{t}}$$\n",
    "\n",
    "The parameters $w$ are then updated using the bias-corrected first moment and the scaled, clipped learning rate:\n",
    "\n",
    "$$w_t = w_{t-1} - \\hat{\\eta_t}\\odot m_t$$\n",
    "\n",
    "#### Explanation:\n",
    "- AdaBound combines the adaptive learning rate behavior of algorithms like Adam with the concept of learning rate clipping to achieve more stable and controlled optimization.\n",
    "- By dynamically clipping the learning rates within a desired range, AdaBound prevents large fluctuations and ensures more stable convergence.\n",
    "The clipped learning rate is further scaled by the square root of the time step to provide a decaying effect, allowing for smoother optimization.\n",
    "#### Benefits:\n",
    "- Controlled Learning Rates: AdaBound dynamically clips the learning rates during training, ensuring that they stay within a desired range, which can lead to more stable and controlled optimization.\n",
    "- Stable Convergence: By preventing large fluctuations in the learning rates, AdaBound aims to achieve more stable convergence during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d3d5f-445b-4c7c-979f-d743306760b6",
   "metadata": {},
   "source": [
    "# 13. AdamW Optimizer\n",
    "\n",
    "AdamW is an optimization algorithm that addresses the issue of weight decay regularization in Adam by incorporating it directly into the parameter update rule. It fixes the discrepancy between $L_2$ regularization and weight decay regularization observed in Adam, ensuring more consistent and effective regularization.\n",
    "\n",
    "First moment:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
    "\n",
    "Second (raw) moment:\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g^2_t$$\n",
    "\n",
    "Bias Correction:  \n",
    "$$\\hat{m_t} = \\frac{m_t}{1-\\beta^t_1}$$ \n",
    "$$\\hat{v_t} = \\frac{v_t}{1-\\beta^t_2}$$  \n",
    "\n",
    "Mathematical Representation:  \n",
    "The update rule for the parameters $w$ in AdamW incorporates both the first and second moments as well as weight decay regularization:  \n",
    "\n",
    "$$w_t = w_{t-1} - \\Bigg( \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda w_{t-1} \\Bigg)$$\n",
    "\n",
    "Weight decay regularization:\n",
    "\n",
    "$$w_{t+1} = (1-\\lambda)w_{t} - \\eta \\nabla f(w_t)$$\n",
    "\n",
    "$L_2$ regularization:\n",
    "\n",
    "$$L^{reg}(w_t) = f(w_t) + \\frac{\\lambda}{2}||w_t||^2_2$$\n",
    "\n",
    "$$\\nabla L^{reg}(w_t) = \\nabla f({w_t}) + \\lambda w_t$$\n",
    "\n",
    "$SGD:w_{t+1} = w_t - \\eta(\\nabla f(w_t) + \\lambda w_t)$\n",
    "\n",
    "Under standard SGD, $L_2$ regularization is equivalent to reparameterised weight decay regularization. But it is not in case of Adam optimiser.\n",
    "\n",
    "\n",
    "#### Explanation:\n",
    "- AdamW fixes the discrepancy between $L_2$ regularization and weight decay regularization observed in Adam by directly incorporating weight decay into the parameter update rule.\n",
    "- Weight decay regularization is added to the parameter update alongside the gradient-based update, ensuring consistent regularization throughout the optimization process.\n",
    "- By integrating weight decay into the parameter update, AdamW provides more consistent and effective regularization, leading to improved generalization performance.\n",
    "#### Benefits:\n",
    "- Consistent Regularization: AdamW ensures consistent regularization throughout the optimization process by incorporating weight decay directly into the parameter update rule.\n",
    "- Improved Generalization: By providing more effective regularization, AdamW can lead to improved generalization performance, especially in deep learning tasks with complex models and datasets.\n",
    "#### Important Note:\n",
    "- Prefer AdamW Over Adam with $L_2$ Regularization: When using Adam optimizer, it is recommended to use AdamW instead of applying $L_2$ regularization separately. This ensures more consistent regularization and can lead to better optimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b2516-ca59-4969-aa63-f2523387fda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
