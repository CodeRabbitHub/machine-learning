{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec50d96-afa8-442d-9335-8e69482ace51",
   "metadata": {},
   "source": [
    "# Introduction to Clustering Methods and Distances\n",
    "\n",
    "Clustering represents a form of unsupervised learning wherein the primary aim is to discern patterns inherent within unlabeled data. This technique is predominantly employed to partition vast datasets into distinct subgroups, facilitating informed decision-making. Clustering algorithms operate by segregating data into disparate clusters, each characterized by similar features, yet markedly distinct from data points in other clusters.\n",
    "\n",
    "## Clustering Types\n",
    "\n",
    "Clustering algorithms can employ either a hard or soft methodology for classifying data points. In the former approach, data points are unequivocally assigned to a single cluster, while in the latter, probabilities are computed to ascertain the likelihood of a data point belonging to each cluster. Based on the principle of similarity between data points, clustering algorithms can be categorized into several distinct groups:\n",
    "\n",
    "1. **Connectivity-based Models:**\n",
    "   These models rely on spatial proximity within the data space to gauge similarity. Clusters are formed by initially assigning all data points to a single cluster and then iteratively partitioning the data into smaller clusters as inter-point distances increase. Alternatively, each data point may be initially assigned an individual cluster, followed by the aggregation of nearby data points. Hierarchical clustering exemplifies this methodology.\n",
    "\n",
    "2. **Density-based Models:**\n",
    "   Clusters are delineated based on data point density within the data space. High-density regions represent clusters, typically delineated from one another by regions of lower density. The DBSCAN algorithm is illustrative of this approach.\n",
    "\n",
    "3. **Distribution-based Models:**\n",
    "   Models in this category predicate cluster identification on the assumption that all data points within a cluster adhere to a shared distribution, such as a Gaussian distribution. Gaussian Mixture Models (GMM) typify this methodology, positing that data points arise from a blend of distinct Gaussian distributions.\n",
    "\n",
    "4. **Centroid-based Models:**\n",
    "   Operating on the principle of defining a centroid for each cluster, these models employ an iterative process to continuously update centroids. Data points are then allocated to the cluster where their proximity to the centroid is minimized. The k-means algorithm serves as a prominent example of this approach.\n",
    "\n",
    "\n",
    "## Distance metrics\n",
    "\n",
    "Understanding distance metrics is crucial in clustering algorithms as they quantify the dissimilarity or similarity between data points, forming the foundation for cluster formation. Various distance metrics, such as Euclidean distance, Manhattan distance, and Cosine similarity, offer distinct perspectives on data point separation. The most commonly used is Eucledian distance in case numerical data types and Hamming distance for categorical data types. \n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "<center><img src=\"./imgs/euc.png\"/></center>\n",
    "\n",
    "Euclidean distance is a widely recognized metric that measures the straight-line distance between two points in a multidimensional space. It is often familiar to many due to its intuitive geometric interpretation. This distance metric can take on any positive real number or be zero. Mathematically, Euclidean distance between two points, $x$ and $y$, in an $n$-dimensional space is calculated as the square root of the sum of squares of differences between corresponding coordinates:\n",
    "\n",
    "$$ d_n(x,y) =  \\sqrt {(x_1-y_1)^2 + (x_2-y_2)^2 + (x_3-y_3)^2 + \\ldots + (x_n-y_n)^2} $$\n",
    "\n",
    "For higher dimensions, the Euclidean distance computation involves adding the squared differences between each pair of coordinates and then taking the square root. Its versatility and ease of interpretation make it a fundamental tool in various fields, particularly in clustering algorithms where it serves as a crucial component for measuring dissimilarity between data points.\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "<center><img src=\"./imgs/man.png\"/></center>\n",
    "\n",
    "Manhattan distance is a metric used to measure the absolute numerical difference between two points in space, employing Cartesian coordinates. Unlike Euclidean distance, which calculates the straight-line distance using Pythagoras theorem, Manhattan distance considers distance as the sum of the line vectors (x,y), akin to a taxi navigating among blocks of buildings.\n",
    "\n",
    "$$d_n(x,y) = \\sum_{i=1}^{n}{|(x_i-y_i)|}$$\n",
    "\n",
    "Mathematically, the Manhattan distance between two points, $x$ and $y$, in an $n$-dimensional space is computed as the sum of the absolute differences between corresponding coordinates. This metric is particularly useful in scenarios where movement is constrained to a grid-like pattern, such as city navigation or grid-based data representation.\n",
    "\n",
    "### Minkowski Distance\n",
    "\n",
    "The Minkowski distance is a generalized distance metric calculated using the following formula:\n",
    "\n",
    "$$d_n(x,y) = {\\biggl(\\sum_{i=1}^{n}|(x_i-y_i)|^p\\biggr)}^{1/p}$$\n",
    "\n",
    "This distance metric allows for manipulation by substituting different values of 'p', enabling various ways to compute the distance between two data points. Consequently, Minkowski Distance is also referred to as Lp norm distance. Common values of 'p' yield well-known distance measures:\n",
    "\n",
    "- $p = 1$: Manhattan Distance\n",
    "- $p = 2$: Euclidean Distance\n",
    "- $p = \\infty$: Chebyshev Distance\n",
    "\n",
    "The flexibility of the Minkowski distance makes it a versatile tool in diverse applications, offering different perspectives on data similarity or dissimilarity depending on the chosen 'p' value.\n",
    "\n",
    "### Hamming Distance\n",
    "\n",
    "<center><img src=\"./imgs/hamm.png\"/></center>\n",
    "\n",
    "Hamming distance is a metric employed to calculate the distances between categorical variables, often referred to as nominal variables. It quantifies the number of differences between two binary strings, reflecting the dissimilarity between categorical values. Unlike numerical variables, categorical variables lack a natural ordering, making Hamming distance particularly suitable for assessing the similarity between categorical data points.\n",
    "\n",
    "When dealing exclusively with categorical features in a dataset, Hamming distance serves as a valuable tool for measuring the similarity between two data points. However, it's essential to note that Hamming distance can only be computed when comparing vectors of equal length; comparisons between vectors of unequal lengths are not feasible.\n",
    "\n",
    "### Gower Distance\n",
    "\n",
    "Gower (1971) distance is a hybrid measure designed to accommodate both continuous and categorical data. \n",
    "\n",
    "For continuous or ordinal data features, Gower distance employs either the Manhattan distance or a ranked ordinal Manhattan distance, respectively. On the other hand, for categorical data features, it utilizes the DICE coefficient, which quantifies the similarity between two sets.\n",
    "\n",
    "The DICE coefficient is calculated as:\n",
    "\n",
    "$$DICE = \\frac{2|X\\cap Y|}{|X|+|Y|} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "where $X$ and $Y$ represent two sets, and $TP$, $FP$, and $FN$ denote True Positives, False Positives, and False Negatives, respectively.\n",
    "\n",
    "The Gower distance between a pair of points $p$ and $q$, denoted as $G_n(p,q)$, is computed as:\n",
    "\n",
    "$$G_n(p,q) = \\frac{\\sum_{i=1}^{n}W_{pqk}S_{pqk}}{\\sum_{i=1}^{n}W_{pqk}}$$\n",
    "\n",
    "Here, $S_{pqk}$ represents either the Manhattan distance or the DICE coefficient for feature $k$, while $W_{pqk}$ is a binary indicator (1 or 0) denoting the validity of feature $k$. The Gower distance is calculated as the sum of feature scores divided by the sum of feature weights, providing a comprehensive measure of dissimilarity between data points with mixed data types.\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a widely utilized metric that addresses the challenges of high dimensionality often encountered with Euclidean distance. It quantifies the similarity between two vectors by measuring the cosine of the angle between them. Notably, cosine similarity remains invariant to the magnitude of the vectors, focusing solely on their orientation.\n",
    "\n",
    "Mathematically, the cosine similarity between two vectors, $x$ and $y$, is computed as:\n",
    "\n",
    "$$d_{(x,y)} = \\cos(\\theta) = \\frac{x \\cdot y}{||x|| \\ ||y||}$$\n",
    "\n",
    "Here, $x \\cdot y$ represents the dot product of the vectors, while $||x||$ and $||y||$ denote their respective magnitudes. \n",
    "\n",
    "Cosine similarity finds extensive application in scenarios involving high-dimensional data, where the magnitude of vectors is insignificant. In text analyses, for instance, it is commonly employed when data is represented by word counts, allowing for effective comparison and similarity assessment.\n",
    "\n",
    "### Haversine Distance\n",
    "\n",
    "<center><img src=\"./imgs/hav.png\"/></center>\n",
    "\n",
    "Haversine distance calculates the distance between two points on a sphere, typically represented by their longitudes and latitudes. Similar to Euclidean distance, it determines the shortest line connecting two points. However, unlike Euclidean distance, Haversine distance accounts for the curvature of the Earth's surface, assuming that the two points are positioned on a sphere.\n",
    "\n",
    "The formula for calculating Haversine distance between two points is:\n",
    "\n",
    "$$ d = 2\\ r\\ \\sin^{-1}\\biggl(\\sqrt{\\sin^2\\biggl(\\frac{\\phi_2-\\phi_1}{2}\\biggr) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\biggl(\\frac{\\lambda_2-\\lambda_1}{2}\\biggr)}\\biggr)$$\n",
    "\n",
    "Here, $r$ denotes the radius of the sphere, $\\phi_1$ and $\\phi_2$ represent the latitudes of the two points, and $\\lambda_1$ and $\\lambda_2$ represent their longitudes.\n",
    "\n",
    "One notable disadvantage of Haversine distance is its assumption that the points lie on a perfect sphere. In reality, the Earth's shape is more complex, which can lead to inaccuracies in distance calculations, especially over long distances or in regions with significant topographical variations.\n",
    "\n",
    "### Jaccard Distance\n",
    "\n",
    "<center><img src=\"./imgs/jac.png\"/></center>\n",
    "\n",
    "The Jaccard index, also known as the Intersection over Union, serves as a metric for assessing the similarity and diversity of sample sets. It quantifies the similarity between two sets by measuring the ratio of the size of their intersection to the size of their union. In essence, it represents the proportion of common entities between sets relative to their total number of entities.\n",
    "\n",
    "The Jaccard index, denoted as $J(x,y)$, is calculated as:\n",
    "\n",
    "$$J(x,y) = \\frac{|x \\cap y|}{|x \\cup y|}$$\n",
    "\n",
    "To derive the Jaccard distance, we subtract the Jaccard index from 1:\n",
    "\n",
    "$$D_{(x,y)} = 1 - J(x,y) = 1 - \\frac{|x \\cap y|}{|x \\cup y|}$$\n",
    "\n",
    "One notable drawback of the Jaccard index is its sensitivity to dataset size. Large datasets can disproportionately impact the index, potentially skewing results by inflating the union while maintaining a relatively constant intersection. Despite this limitation, the Jaccard index finds widespread application in scenarios involving binary or binarized data, such as image segmentation accuracy assessment in deep learning models or text similarity analysis. It enables the comparison of sets of patterns, facilitating tasks ranging from image analysis to natural language processing.\n",
    "\n",
    "\n",
    "# Importance of Feature Scaling:\n",
    "\n",
    "Feature scaling is crucial as it ensures that all features have the same scale. Many machine learning algorithms, including KNN, rely on distance-based metrics to assess the similarity between data points. When features are on different scales, the algorithm may assign more weight to features with larger scales, potentially resulting in biased or inaccurate outcomes.\n",
    "\n",
    "Let's consider an example of distance calculation using two features whose magnitudes/ranges vary greatly:\n",
    "\n",
    "$$\\text{Euclidean distance} = \\sqrt{(820000 - 325000)^2 - (3.75 - 0.50)^2}$$\n",
    "\n",
    "From the above equation, it's evident that features with high magnitudes contribute significantly more than those with lower magnitudes. Thus, normalizing the data to a range of 0-1 is recommended for better results.\n",
    "\n",
    "One common method to achieve this normalization is by using sklearn's MinMaxScaler:\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$\n",
    "\n",
    "**Note:** Standardization is not employed here because it doesn't assume data to follow any specific distribution.\n",
    "\n",
    "# Curse of Dimensionality\r\n",
    "\r\n",
    "The Curse of Dimensionality occurs when your data has too many features. Having an excessive number of features makes it challenging to cluster observations effectively. This happens because with too many dimensions, every observation in the dataset seems equidistant from all the others.\r\n",
    "\r\n",
    "Clustering relies on distance measures like Euclidean distance to gauge the similarity between observations. When distances are nearly equal for all observations, it becomes problematic. In such cases, all observations seem equally similar (and dissimilar) to each other, making it impossible to form meaningful cluster# Applications of Clustering\r\n",
    "\r\n",
    "Clustering finds applications across diverse domains due to its versatility. Some of the most common applications of clustering include:\r\n",
    "\r\n",
    "- **Recommendation engines:** Clustering helps in grouping similar users or items to provide personalized recommendations.\r\n",
    "\r\n",
    "- **Market segmentation:** Clustering aids in dividing customers into distinct groups based on their characteristics, allowing businesses to tailor marketing strategies accordingly.\r\n",
    "\r\n",
    "- **Social network analysis:** Clustering helps identify communities or groups within social networks, enabling analysis of network structures and behaviors.\r\n",
    "\r\n",
    "- **Search result grouping:** Clustering assists in organizing search results into coherent groups, enhancing user experience and information retrieval.\r\n",
    "\r\n",
    "- **Medical imaging:** Clustering techniques are utilized for segmenting medical images to identify structures or anomalies, aiding in diagnosis and treatment planning.\r\n",
    "\r\n",
    "- **Image segmentation:** Clustering is employed to partition images into meaningful regions or objects, facilitating tasks like object recognition and image understanding.\r\n",
    "\r\n",
    "- **Anomaly detection:** Clustering helps in detecting outliers or anomalies in datasets, highlighting potentially unusual or suspicious instances for further investigation.\r\n",
    "s.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a845c34-1191-4aed-b608-359bfcc261ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
