{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997100a9-78ca-4aec-9a18-31a08440b7df",
   "metadata": {},
   "source": [
    "## CatBoost Regressor\n",
    "\n",
    "CatBoost (Categorical Boosting) is a high-performance gradient boosting library designed for handling categorical features automatically. It is known for its superior performance, ease of use, and ability to handle categorical data without the need for extensive preprocessing. Here, we will focus on the CatBoost Regressor, which is used for regression tasks.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, optimizing a specific loss function.\n",
    "\n",
    "#### 2. Handling Categorical Features\n",
    "\n",
    "CatBoost is specifically designed to handle categorical features natively. It employs an efficient technique called \"Ordered Boosting\" to prevent overfitting, which ensures that the training process is not biased by the order of the data.\n",
    "\n",
    "#### 3. Symmetric Trees\n",
    "\n",
    "CatBoost builds symmetric trees, meaning all splits at a given level are performed simultaneously. This leads to faster training and prediction times.\n",
    "\n",
    "### Steps Involved in CatBoost Regressor\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The CatBoost process begins by initializing the model with a constant value. For regression, this is typically the mean of the target values $ y $.\n",
    "\n",
    "For a regression task:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $ L $ is the loss function, such as Mean Squared Error (MSE), and $ N $ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For regression, typically squared loss is used.\n",
    "- **Initial Prediction ($ F_0 $):** We find $ \\gamma $ that minimizes the sum of the loss function. For MSE, this $ \\gamma $ turns out to be the mean of $ y $.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "CatBoost constructs an ensemble of trees in a sequential manner. At each iteration $ m $:\n",
    "\n",
    "**Step 2-1: Calculate Residuals**\n",
    "\n",
    "- Compute the residuals $ r_{im} $, which are the gradients of the loss function with respect to the predictions:\n",
    "$$ r_{im} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For squared loss, the residuals simplify to:\n",
    "$$ r_{im} = y_i - F_{m-1}(x_i) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Residuals ($ r_{im} $):** These are the negative gradients of the loss function and represent the difference between the actual and predicted values.\n",
    "- **Interpretation:** These residuals are used as the new target values for the next tree. They guide the model on how to adjust its predictions to reduce the overall error.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a regression tree $ h_m(x) $ to these residuals by minimizing the loss:\n",
    "$$ h_m(x) = \\arg\\min_h \\sum_{i=1}^N L(r_{im}, h(x_i)) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weak Learner:** A decision tree is typically used as the weak learner, trained to predict the residuals from the previous step.\n",
    "\n",
    "**Step 2-3: Compute Terminal Node Values**\n",
    "\n",
    "- For each terminal node $ j $ in the tree $ h_m $, compute the optimal value $ \\gamma_{jm} $ that minimizes the loss:\n",
    "$$ \\gamma_{jm} = \\arg\\min_\\gamma \\sum_{x_i \\in R_{jm}} L(r_{im}, \\gamma) $$\n",
    "\n",
    "For squared loss, $ \\gamma_{jm} $ is the mean of the residuals in the terminal node $ R_{jm} $:\n",
    "$$ \\gamma_{jm} = \\frac{1}{n_j} \\sum_{x_i \\in R_{jm}} r_{im} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Terminal Node Value ($ \\gamma_{jm} $):** This is the value added to the predictions of all samples in the terminal node. For regression, itâ€™s the mean of residuals in that node.\n",
    "- **Derivation:** Taking the derivative of the loss function within each terminal node and setting it to zero, we get the mean of residuals as the optimal value.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted weak learner, scaled by a learning rate $ \\eta $:\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($ \\eta $):** This controls the contribution of each new tree to the final model. It helps in preventing overfitting.\n",
    "- **Model Update:** The new prediction $ F_m(x) $ is the previous prediction $ F_{m-1}(x) $ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $ M $ iterations, the final boosted model $ F(x) $ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in CatBoost Regressor include:\n",
    "\n",
    "- **iterations:** Number of boosting iterations.\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **depth:** Depth of the trees.\n",
    "- **l2_leaf_reg:** L2 regularization term on weights.\n",
    "- **random_strength:** Strength of the random component.\n",
    "- **bagging_temperature:** Controls the amount of randomness in bagging.\n",
    "- **border_count:** Number of splits for numerical features.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Handling Categorical Data:** CatBoost natively handles categorical features without the need for extensive preprocessing.\n",
    "2. **Performance:** Often achieves high accuracy on complex datasets.\n",
    "3. **Efficiency:** Optimized for speed and memory usage.\n",
    "4. **Scalability:** Can handle large datasets with millions of instances and features.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "3. **Sensitive to Noisy Data:** Can be prone to overfitting if not properly regularized.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how CatBoost Regressor can be implemented using the CatBoost library in Python:\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "catboost_regressor = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "catboost_regressor.fit(X_train, y_train, cat_features=[categorical_feature_indices], verbose=0)\n",
    "\n",
    "# Predict\n",
    "y_pred = catboost_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "CatBoost Regressor is a powerful and efficient boosting technique for regression tasks, specifically designed to handle categorical features. By leveraging advanced techniques such as ordered boosting and symmetric trees, it achieves high performance and scalability. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffffd8-2ca0-49ea-a7e9-09c1231213d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
