{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4189858e-c04a-48c5-963d-4eb8fdd53718",
   "metadata": {},
   "source": [
    "## CatBoost Classifier\n",
    "\n",
    "CatBoost (Categorical Boosting) is a high-performance gradient boosting library designed to handle categorical features efficiently. CatBoost Classifier is specifically tailored for classification tasks, leveraging the strengths of CatBoost's unique methodologies to achieve high accuracy and robustness.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models sequentially. Each new model attempts to correct the errors made by the previous models, guided by gradient descent optimization of a specific loss function.\n",
    "\n",
    "#### 2. Handling Categorical Features\n",
    "\n",
    "CatBoost is optimized for handling categorical features natively. It employs an efficient technique called \"Ordered Boosting\" to prevent overfitting and biases introduced by the order of the data.\n",
    "\n",
    "#### 3. Symmetric Trees\n",
    "\n",
    "CatBoost builds symmetric trees, where all splits at a given level are performed simultaneously. This leads to faster training and prediction times.\n",
    "\n",
    "### Steps Involved in CatBoost Classifier\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The CatBoost process begins by initializing the model with a constant value. For classification, this is typically the log-odds of the target classes.\n",
    "\n",
    "For a binary classification task:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $ L $ is the loss function, such as Log-Loss for binary classification, and $ N $ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For classification, typically log-loss is used.\n",
    "- **Initial Prediction ($ F_0 $):** We find $ \\gamma $ that minimizes the sum of the loss function. For log-loss, this $ \\gamma $ is typically the log-odds of the class probabilities.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "CatBoost constructs an ensemble of trees in a sequential manner. At each iteration $ m $:\n",
    "\n",
    "**Step 2-1: Calculate Pseudo-Residuals**\n",
    "\n",
    "- Compute the pseudo-residuals $ r_{im} $, which are the gradients of the loss function with respect to the predictions:\n",
    "$$ r_{im} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For log-loss, the residuals simplify to:\n",
    "$$ r_{im} = y_i - p_{m-1}(x_i) $$\n",
    "where $ p_{m-1}(x_i) $ is the predicted probability for class 1.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Residuals ($ r_{im} $):** These are the negative gradients of the loss function and represent the difference between the actual and predicted class probabilities.\n",
    "- **Interpretation:** These residuals are used as the new target values for the next tree. They guide the model on how to adjust its predictions to reduce the overall error.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a decision tree $ h_m(x) $ to these pseudo-residuals by minimizing the loss:\n",
    "$$ h_m(x) = \\arg\\min_h \\sum_{i=1}^N L(r_{im}, h(x_i)) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weak Learner:** A decision tree is typically used as the weak learner, trained to predict the residuals from the previous step.\n",
    "\n",
    "**Step 2-3: Compute Terminal Node Values**\n",
    "\n",
    "- For each terminal node $ j $ in the tree $ h_m $, compute the optimal value $ \\gamma_{jm} $ that minimizes the loss:\n",
    "$$ \\gamma_{jm} = \\arg\\min_\\gamma \\sum_{x_i \\in R_{jm}} L(r_{im}, \\gamma) $$\n",
    "\n",
    "For log-loss, $ \\gamma_{jm} $ is typically the mean of the residuals in the terminal node $ R_{jm} $:\n",
    "$$ \\gamma_{jm} = \\frac{1}{n_j} \\sum_{x_i \\in R_{jm}} r_{im} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Terminal Node Value ($ \\gamma_{jm} $):** This is the value added to the predictions of all samples in the terminal node. For classification, itâ€™s the mean of residuals in that node.\n",
    "- **Derivation:** Taking the derivative of the loss function within each terminal node and setting it to zero, we get the mean of residuals as the optimal value.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted weak learner, scaled by a learning rate $ \\eta $:\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($ \\eta $):** This controls the contribution of each new tree to the final model. It helps in preventing overfitting.\n",
    "- **Model Update:** The new prediction $ F_m(x) $ is the previous prediction $ F_{m-1}(x) $ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $ M $ iterations, the final boosted model $ F(x) $ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in CatBoost Classifier include:\n",
    "\n",
    "- **iterations:** Number of boosting iterations.\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **depth:** Depth of the trees.\n",
    "- **l2_leaf_reg:** L2 regularization term on weights.\n",
    "- **random_strength:** Strength of the random component.\n",
    "- **bagging_temperature:** Controls the amount of randomness in bagging.\n",
    "- **border_count:** Number of splits for numerical features.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Handling Categorical Data:** CatBoost natively handles categorical features without the need for extensive preprocessing.\n",
    "2. **Performance:** Often achieves high accuracy on complex datasets.\n",
    "3. **Efficiency:** Optimized for speed and memory usage.\n",
    "4. **Scalability:** Can handle large datasets with millions of instances and features.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "3. **Sensitive to Noisy Data:** Can be prone to overfitting if not properly regularized.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how CatBoost Classifier can be implemented using the CatBoost library in Python:\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "catboost_classifier = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "catboost_classifier.fit(X_train, y_train, cat_features=[categorical_feature_indices], verbose=0)\n",
    "\n",
    "# Predict\n",
    "y_pred = catboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "CatBoost Classifier is a powerful and efficient boosting technique for classification tasks, specifically designed to handle categorical features. By leveraging advanced techniques such as ordered boosting and symmetric trees, it achieves high performance and scalability. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6432459-72fe-4700-b3e2-eacaa66e24b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
