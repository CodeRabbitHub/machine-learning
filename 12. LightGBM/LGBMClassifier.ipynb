{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123a25c7-c6ea-4956-9e19-9fde5a28c054",
   "metadata": {},
   "source": [
    "## LightGBM Classifier\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of the gradient boosting framework that is highly optimized for performance and memory usage. It is suitable for both regression and classification tasks. Here, we will focus on its application in classification.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, optimizing a specific loss function.\n",
    "\n",
    "#### 2. Leaf-Wise Growth\n",
    "\n",
    "LightGBM grows trees leaf-wise, meaning it splits the leaf with the maximum loss reduction, leading to deeper trees and potentially better accuracy compared to traditional level-wise growth used in other boosting algorithms.\n",
    "\n",
    "#### 3. Histogram-Based Decision Trees\n",
    "\n",
    "LightGBM uses histogram-based algorithms to speed up the training process. Features are bucketed into discrete bins, significantly reducing the computational cost and memory usage.\n",
    "\n",
    "### Steps Involved in LightGBM Classifier\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The LightGBM process begins by initializing the model with a constant value. For classification, this is typically the log-odds of the positive class for binary classification or the prior probabilities for multi-class classification.\n",
    "\n",
    "For a binary classification task:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $L$ is the log-loss function, and $N$ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For binary classification, the log-loss (or binary cross-entropy) is used.\n",
    "- **Initial Prediction ($F_0$):** We find $\\gamma$ that minimizes the sum of the loss function. For log-loss, this $\\gamma$ is related to the log-odds of the positive class.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "LightGBM constructs an ensemble of trees in a sequential manner. At each iteration $m$:\n",
    "\n",
    "**Step 2-1: Calculate Gradient and Hessian**\n",
    "\n",
    "- Compute the gradient (first derivative) and Hessian (second derivative) of the loss function with respect to the predictions:\n",
    "\n",
    "$$ g_{im} = \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "$$ h_{im} = \\left[ \\frac{\\partial^2 L(y_i, F(x_i))}{\\partial F(x_i)^2} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For log-loss, the gradient $g_{im}$ and Hessian $h_{im}$ are given by:\n",
    "\n",
    "$$ g_{im} = \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)} = p_i - y_i $$\n",
    "$$ h_{im} = p_i (1 - p_i) $$\n",
    "\n",
    "where $ p_i = \\frac{1}{1 + e^{-F_{m-1}(x_i)}} $ is the predicted probability of the positive class.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Gradient ($g_{im}$):** The gradient measures the difference between the predicted probability and the actual class label.\n",
    "- **Hessian ($h_{im}$):** The Hessian represents the second-order derivative, which helps in adjusting the step size for optimization.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a regression tree $h_m(x)$ to the gradients $g_{im}$ using weighted least squares, where weights are given by the Hessians $h_{im}$.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weighted Least Squares:** Each split is chosen to minimize the weighted sum of squared errors, taking into account both gradients and Hessians.\n",
    "\n",
    "**Step 2-3: Compute Leaf Weights**\n",
    "\n",
    "- For each leaf $j$ in the tree $h_m$, compute the optimal leaf weight $\\gamma_{jm}$ that minimizes the loss:\n",
    "\n",
    "$$ \\gamma_{jm} = - \\frac{\\sum_{i \\in R_{jm}} g_{im}}{\\sum_{i \\in R_{jm}} h_{im}} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Leaf Weight ($\\gamma_{jm}$):** This value is used to update the modelâ€™s prediction for all samples in the leaf. It is derived from the ratio of the sum of gradients to the sum of Hessians within the leaf.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted tree, scaled by a learning rate $\\eta$:\n",
    "\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($\\eta$):** This controls the contribution of each new tree to the final model, helping to prevent overfitting.\n",
    "- **Model Update:** The new prediction $F_m(x)$ is the previous prediction $F_{m-1}(x)$ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $M$ iterations, the final boosted model $F(x)$ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in LightGBM Classifier include:\n",
    "\n",
    "- **num_leaves:** Maximum number of leaves in each tree.\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **n_estimators:** Number of boosting stages (i.e., the number of trees).\n",
    "- **max_depth:** Maximum depth of individual trees.\n",
    "- **min_child_weight:** Minimum sum of instance weight needed in a child.\n",
    "- **subsample:** Fraction of samples used for fitting individual trees.\n",
    "- **colsample_bytree:** Fraction of features used for fitting individual trees.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Performance:** LightGBM often achieves high accuracy on complex datasets.\n",
    "2. **Efficiency:** Optimized for speed and memory usage with histogram-based algorithms.\n",
    "3. **Scalability:** Can handle large datasets with millions of instances and features.\n",
    "4. **Flexibility:** Can handle various types of data and different loss functions.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "3. **Sensitive to Noisy Data:** Leaf-wise growth can lead to overfitting if not properly regularized.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how LightGBM Classifier can be implemented using the LightGBM library in Python:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = lgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "LightGBM Classifier is a powerful and efficient boosting technique for classification tasks. By leveraging advanced techniques such as leaf-wise growth and histogram-based algorithms, it achieves high performance and scalability. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb08248-d7b1-4333-b4b2-17fd85063b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
