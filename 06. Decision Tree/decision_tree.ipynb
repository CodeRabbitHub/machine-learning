{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164cce16-8608-4daf-921d-7ddc57a972ef",
   "metadata": {},
   "source": [
    "# Exploring Decision Trees  \n",
    "\n",
    "A decision tree is a vital tool in machine learning, used mainly for sorting data into categories or predicting values. It shines when dealing with complex relationships between different pieces of information. Unlike simpler models, decision trees are great at handling data that doesn't follow straight lines or simple patterns. They're also helpful because they can explain how they make decisions, which makes them easier for everyone involved to understand.\n",
    "\n",
    "# Types of Decision Trees\n",
    "\n",
    "Decision trees come in two main types, each designed to handle different kinds of target variables:\n",
    "\n",
    "- **Categorical Variable Decision Tree:** This type is good at dealing with target variables that have categories, not just 'Yes' or 'No'.  But also It's useful when the outcome has many options to choose from. For example, it could help predict someone's preferred way of getting around, like 'Car', 'Bus', 'Train', or 'Walk'.\n",
    "\n",
    "- **Continuous Variable Decision Tree:** This type is made for target variables that are on a continuous scale, like predicting income based on things like age and job. It's great when the outcome is something you can measure, not just choose from a list. target variables that are on a continuous scale, like predicting income based on things like age and job. It's great when the outcome is something you can measure, not just choose from a list.\n",
    "\n",
    "### Key Concepts in Decision Trees\n",
    "\n",
    "- **Root Node**: Represents the entire dataset, serving as the starting point for division into smaller groups.\n",
    "- **Splitting**: The process of dividing a node into smaller sub-nodes based on specific criteria.\n",
    "- **Decision Node**: A node that further splits into additional nodes based on decisions made during the splitting process.\n",
    "- **Leaf/Terminal Node**: Final nodes in the tree structure that do not split further.\n",
    "- **Pruning**: The removal of unnecessary sub-nodes to simplify the tree and improve its efficiency.\n",
    "- **Branch/Sub-Tree**: Segments of the tree that extend from the main structure, representing different paths or outcomes.\n",
    "- **Parent and Child Node**: Nodes in the tree where the parent node splits into smaller nodes (children) based on certain conditions.\n",
    "\n",
    "### Managing Numerical and Categorical Data in Decision Trees\n",
    "\n",
    "Decision trees are versatile in handling both numerical and categorical data simultaneously. Here's how they manage each type:\n",
    "\n",
    "- **Categorical Features**: The tree splits based on belonging to a particular class within the categorical feature.\n",
    "- **Continuous Features**: Splits are determined by values above or below a threshold within the continuous feature.\n",
    "\n",
    "The decision tree selects the best feature to split on at each step, aiming to reduce uncertainty. Whether a feature is categorical or continuous doesn't impact this decision-making process.\n",
    "\n",
    "*In real-world scenarios, converting categorical features into numerical format, often through techniques like One-Hot Encoding, can be beneficial.*\n",
    "\n",
    "# How to Choose the Set of Split Points\n",
    "\n",
    "The choice of split points for a variable depends on whether it's numeric or categorical.\n",
    "\n",
    "## Numeric Predictor Variables\n",
    "- **Unique Values**: If a predictor is numeric and has unique values, there are *n – 1* split points for *n* data points. However, considering all these points may be impractical due to their number. Instead, common practice involves selecting split points based on specific percentiles of the value distribution (e.g., every tenth percentile like 10%, 20%, 30%, etc.).\n",
    "\n",
    "  **Example**: For a column of numeric values [20, 25, 30, 35, 50, 55, 70, 90] with 8 data points, average values between each pair of data points are calculated, resulting in 7 split values [22.5, 27.5, 32.5, 42.5, 52.5, 62.5, 80.0]. Information gain (IG) is then computed for each split point to select the one maximizing IG.\n",
    "\n",
    "#### Categorical Predictor Variables\n",
    "- **Binary Splitting**: With categorical predictors, binary splitting is common, creating either one child node per class (*multiway splits*) or only two child nodes (*binary split*). Binary splits are preferred because multiway splits can quickly break data into small subsets, leading to overfitting.\n",
    "\n",
    "  - **Two-Class Predictor**: If a categorical predictor has two classes, there is only one possible split.\n",
    "  \n",
    "  - **Multiple-Class Predictor**: For predictors with more than two classes:\n",
    "    - **Small Number of Classes**: Consider all possible splits into two child nodes.\n",
    "    - **Large Number of Classes**: Order classes by their average output value and make a binary split into two groups of ordered classes (resulting in *k – 1* possible splits for *k* classes).\n",
    "\n",
    "**Example**: For three classes like apple, banana, and orange, potential binary splits include:\n",
    "  \n",
    "  |        | child1 |     child2     |\n",
    "  |:------:|:------:|:--------------:|\n",
    "  | split1 |  apple | banana, orange |\n",
    "  | split2 | banana |  apple, orange |\n",
    "  | split3 | orange |  apple, banana |\n",
    "\n",
    "For *k* classes, there are *2^(k-1)-1* possible splits, which can be computationally intensive for large *k*. This can bias decision trees towards splitting variables with many classes, resulting in potentially significant improvements but also increasing the risk of overfitting.\n",
    "  \n",
    "For four classes like apple, banana, orange, and grapes, potential binary splits include: \n",
    "  \n",
    "| Split   | Child 1        | Child 2            |\n",
    "|---------|----------------|--------------------|\n",
    "| Split 1 | Grapes, Apple  | Banana, Orange     |\n",
    "| Split 2 | Grapes, Banana | Apple, Orange      |\n",
    "| Split 3 | Grapes, Orange | Apple, Banana      |\n",
    "| Split 4 | Grapes         | Apple, Banana, Orange |\n",
    "| Split 5 | Apple          | Grapes, Banana, Orange |\n",
    "| Split 6 | Banana         | Grapes, Apple, Orange  |\n",
    "| Split 7 | Orange         | Grapes, Apple, Banana  |\n",
    "\n",
    "### Criteria Used for Enhancing Node Splits\n",
    "\n",
    "The standards for enhancing node splits vary based on whether the target variable is continuous or categorical.\n",
    "\n",
    "#### Continuous Target Variable\n",
    "- **Reduction in Sum of Squared Errors (SSE)**:\n",
    "  When the outcome is numerical, improvement is gauged by the difference in the sum of squared errors (SSE) between the node and its child nodes post-split. The squared error for a node is computed as:\n",
    "  \n",
    "  $$ \\sum_{i=1}^{n}{(y_i - c)}^2 $$\n",
    "  \n",
    "  Where:\n",
    "  - $n$ represents the number of cases at the node.\n",
    "  - $c$ denotes the average outcome of all cases at that node.\n",
    "  - $y_i$ stands for the outcome value of the $i$-th case.\n",
    "\n",
    "#### Categorical Target Variable\n",
    "- **Gini Impurity**:\n",
    "\n",
    "    Gini Impurity measures the likelihood of a randomly selected element in a dataset being wrongly classified. It's used in creating classification trees, which divide data into groups based on certain criteria. Gini Impurity ranges from 0 to 0.5, where 0 means all elements belong to the same class and 0.5 means elements are evenly distributed across different classes.\n",
    "\n",
    "  For categorical outcomes, the split can be based on:\n",
    "  - **Gini Impurity**: \n",
    "    $$ Gini\\ impurity = \\sum_{i=1}^{k}{p_i(1-p_i)} $$\n",
    "    Where:\n",
    "    - $k$ is the number of classes.\n",
    "    - $p_i$ is the proportion of cases belonging to class $i$.\n",
    "    \n",
    "  - **Information Gain**:\n",
    "    $$ Information\\ Gain = Entropy(Parent) - Entropy(Children) $$\n",
    "    Where:\n",
    "    - **Entropy**:\n",
    "      $$ Entropy = -\\sum_{i=1}^{k}{p_i \\log_2(p_i)} $$\n",
    "      $p_i$ is the probability of class $i$.\n",
    "    \n",
    "- **Chi-Square**:\n",
    "  Another method for categorical targets is Chi-Square, which assesses the statistical significance of differences between the parent node and child nodes. The Chi-Square value for a class is calculated as:\n",
    "  \n",
    "  $$ Chi-Square = \\sqrt{\\frac{(Actual-Expected)^2}{Expected}} $$\n",
    "  \n",
    "  Where:\n",
    "  - **Expected**: Expected value for a class in a child node based on parent node distribution.\n",
    "  - **Actual**: Actual value for a class in a child node.\n",
    "\n",
    "The selected criteria aim to minimize impurity or error after the split, resulting in more homogeneous child nodes. Each criterion evaluates the effectiveness of the split in reducing uncertainty or error, guiding the decision tree towards optimal node divisions.\n",
    "\n",
    "### Illustration of Node Splitting Considering SSE or Variance for a Decision Tree\n",
    "\n",
    "Consider a dataset of 50 startups, aiming to predict profit based on various features, including categorical and continuous variables.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "The dataset contains information about startups, including the amount spent on Research and Development (R&D), Administration, Marketing Spend, State of operation, and the resulting Profit.\n",
    "\n",
    "#### Sample Data\n",
    "\n",
    "| R&D Spend | Administration | Marketing Spend | State      | Profit   |\n",
    "|-----------|----------------|-----------------|------------|----------|\n",
    "| 165349    | 136897         | 471784          | New York   | 192261   |\n",
    "| 162597    | 151377         | 443898          | California | 191792   |\n",
    "| 153442    | 101145         | 407934          | Florida    | 191050   |\n",
    "| ...       | ...            | ...             | ...        | ...      |\n",
    "\n",
    "<center><img src=\"./imgs/split.png\"/></center>\n",
    "\n",
    "### Illustration of Node Splitting Considering Information Gain for a Decision Tree\n",
    "\n",
    "Consider an experiment with two predictors *variable1* and *variable2* and a target variable with two outcomes: *stop* and *continue*.\n",
    "\n",
    "| variable1 | variable2 | outcome   |\n",
    "|-----------|-----------|-----------|\n",
    "| 3         | 5         | stop      |\n",
    "| 7         | 6         | continue  |\n",
    "| 3         | 3         | stop      |\n",
    "| 4         | 8         | continue  |\n",
    "| 3         | 9         | continue  |\n",
    "| 6         | 5         | stop      |\n",
    "| 5         | 8         | continue  |\n",
    "| 6         | 4         | continue  |\n",
    "\n",
    "Let's calculate the initial entropy of the root node before any splitting:\n",
    "\n",
    "$$H = -\\left(\\frac{3}{8}\\right)\\log_2\\left(\\frac{3}{8}\\right) - \\left(\\frac{5}{8}\\right)\\log_2\\left(\\frac{5}{8}\\right) = 0.954$$\n",
    "\n",
    "Now we have two options: either to choose *variable1* for split or *variable2*. To decide which variable to split on, we will calculate entropies in both cases.\n",
    "\n",
    "**Splitting with *variable1* at split point 4 (average or 50th percentile):**\n",
    "- $p_{(>4)} = \\frac{4}{8}$\n",
    "- $H_{(>4)} = -\\left(\\frac{1}{4}\\right)\\log_2\\left(\\frac{1}{4}\\right) - \\left(\\frac{3}{4}\\right)\\log_2\\left(\\frac{3}{4}\\right) = 0.81$\n",
    "  | variable1 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 7         | continue  |\n",
    "  | 6         | stop      |\n",
    "  | 5         | continue  |\n",
    "  | 6         | continue  |\n",
    "- $p_{(\\leq 4)} = \\frac{4}{8}$\n",
    "- $H_{(\\leq 4)} = -\\left(\\frac{2}{4}\\right)\\log_2\\left(\\frac{2}{4}\\right) - \\left(\\frac{2}{4}\\right)\\log_2\\left(\\frac{2}{4}\\right) = 1.0$\n",
    "  | variable1 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 3         | stop      |\n",
    "  | 3         | stop      |\n",
    "  | 4         | continue  |\n",
    "  | 3         | continue  |\n",
    "Entropy after splitting by *variable1*:\n",
    "$$H(\\text{variable1}) = -p_{(>4)}H_{(>4)} - p_{(\\leq 4)}H_{(\\leq 4)} = 0.9$$\n",
    "\n",
    "**Splitting with *variable2* at split point 6 (average or 50th percentile):**\n",
    "- $p_{(>6)} = \\frac{3}{8}$\n",
    "- $H_{(>6)} = -\\left(\\frac{3}{3}\\right)\\log_2\\left(\\frac{3}{3}\\right) - \\left(\\frac{0}{3}\\right)\\log_2\\left(\\frac{0}{3}\\right) = 0$\n",
    "  | variable2 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 8         | continue  |\n",
    "  | 9         | continue  |\n",
    "  | 8         | continue  |\n",
    "- $p_{(\\leq 6)} = \\frac{5}{8}$\n",
    "- $H_{(\\leq 6)} = -\\left(\\frac{2}{5}\\right)\\log_2\\left(\\frac{2}{5}\\right) - \\left(\\frac{3}{5}\\right)\\log_2\\left(\\frac{3}{5}\\right) = 0.971$\n",
    "  | variable2 | outcome   |\n",
    "  |-----------|-----------|\n",
    "  | 5         | stop      |\n",
    "  | 6         | continue  |\n",
    "  | 3         | stop      |\n",
    "  | 5         | stop      |\n",
    "  | 4         | continue  |\n",
    "Entropy after splitting by *variable2*:\n",
    "$$H(\\text{variable2}) = - p_{(>6)}H_{(>6)} - p_{(\\leq 6)}H_{(\\leq 6)} = 0.28$$\n",
    "\n",
    "Calculate information gain after each split:\n",
    "$$IG(1) = H - H(\\text{variable1}) = 0.954 - 0.9 = 0.054$$\n",
    "$$IG(2) = H - H(\\text{variable2}) = 0.954 - 0.28 = 0.674$$\n",
    "\n",
    "We observe that if we split the node by considering *variable2*, we achieve the highest information gain. Therefore, we select *variable2* for the split and continue this process for impure nodes until we obtain pure leaf nodes for the decision tree. The following diagram shows the decision tree in this case.\n",
    "\n",
    "<center><img src=\"./imgs/decisiontree.png\"/></center>\n",
    "\n",
    "### Illustration of Node Splitting Considering Gini Impurity for a Decision Tree\n",
    "\n",
    "Consider the same experiment data from above.\n",
    "\n",
    "| variable1 | variable2 | outcome   |\n",
    "|-----------|-----------|-----------|\n",
    "| 3         | 5         | stop      |\n",
    "| 7         | 6         | continue  |\n",
    "| 3         | 3         | stop      |\n",
    "| 4         | 8         | continue  |\n",
    "| 3         | 9         | continue  |\n",
    "| 6         | 5         | stop      |\n",
    "| 5         | 8         | continue  |\n",
    "| 6         | 4         | continue  |\n",
    "\n",
    "We have two variables for splitting. We need to calculate the weighted Gini impurity for both *variable1* and *variable2*. Whichever variable gives the lowest impurity will be selected for the split.\n",
    "\n",
    "**Splitting with *variable1* at split point 4 (average value):**\n",
    "- Gini Impurity (>4) = 1 - $\\left(\\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2\\right) = 0.375$\n",
    "\n",
    "  | variable1 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 7         | continue |\n",
    "  | 6         | stop     |\n",
    "  | 5         | continue |\n",
    "  | 6         | continue |\n",
    "\n",
    "- Gini Impurity (<=4) = 1 - $\\left(\\left(\\frac{2}{4}\\right)^2 + \\left(\\frac{2}{4}\\right)^2\\right) = 0.375$\n",
    "\n",
    "  | variable1 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 3         | stop     |\n",
    "  | 3         | stop     |\n",
    "  | 4         | continue |\n",
    "  | 3         | continue |\n",
    "\n",
    "- Weighted Gini Impurity (variable1) = $\\frac{4}{8} \\times 0.375 + \\frac{4}{8} \\times 0.5 = 0.4375$\n",
    "\n",
    "**Splitting with *variable2* at split point 6 (average value):**\n",
    "- Gini Impurity (>6) = 1 - $\\left(\\left(\\frac{3}{3}\\right)^2 + \\left(\\frac{0}{3}\\right)^2\\right) = 0$\n",
    "\n",
    "  | variable2 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 8         | continue |\n",
    "  | 9         | continue |\n",
    "  | 8         | continue |\n",
    "\n",
    "- Gini Impurity (<=6) = 1 - $\\left(\\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2\\right) = 0.48$\n",
    "\n",
    "  | variable2 | outcome  |\n",
    "  |-----------|----------|\n",
    "  | 5         | stop     |\n",
    "  | 6         | continue |\n",
    "  | 3         | stop     |\n",
    "  | 5         | stop     |\n",
    "  | 4         | continue |\n",
    "\n",
    "- Weighted Gini Impurity (variable2) = $\\frac{3}{8} \\times 0 + \\frac{5}{8} \\times 0.48 = 0.3$\n",
    "\n",
    "From the above calculations, we see that the weighted Gini impurity is less for *variable2*. Therefore, we choose *variable2* for splitting the data. This process is continued recursively to build the entire decision tree.\n",
    "\n",
    "**Note**: For the same data, using Gini impurity and entropy measure, different variables can be selected as the root node. This is because each algorithm has its bias. Gini impurity tends to select a wider spread of data for variable selection, whereas the entropy method is biased toward compact data with lower spread.\n",
    "\n",
    "### Illustration of Node Splitting Considering Chi-Square for a Decision Tree\n",
    "\n",
    "Let's consider an example with categorical predictors and a categorical target for ease of understanding:\n",
    "\n",
    "| Performance | Class | Outcome       |\n",
    "|-------------|-------|---------------|\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Below Avg   | X     | Play Cricket  |\n",
    "| Average     | IX    | Play Cricket  |\n",
    "| Average     | X     | Doesn't Play  |\n",
    "| Average     | IX    | Doesn't Play  |\n",
    "| Below Avg   | X     | Doesn't Play  |\n",
    "| Average     | IX    | Doesn't Play  |\n",
    "| Average     | X     | Doesn't Play  |\n",
    "| Below Avg   | IX    | Doesn't Play  |\n",
    "\n",
    "Distribution of target before splitting (Parent node):\n",
    "\n",
    "- Plays Cricket = $6/12 = 0.5$\n",
    "- Doesn't play Cricket = $6/12 = 0.5$\n",
    "\n",
    "Let's try to split the node by considering the Performance variable and calculate the Chi-Square.\n",
    "\n",
    "**Splitting by Performance:**\n",
    "- Performance: Average\n",
    "  - Expected Cricket Players = $7 * (0.5) = 3.5$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0.267$\n",
    "  \n",
    "  - Expected Non-Players = $7 * (0.5) = 3.5$\n",
    "  - Actual Non-Players = $4$\n",
    "  - Chi-Square Non-Players = $0.267$\n",
    "\n",
    "- Performance: Below Avg\n",
    "  - Expected Cricket Players = $5 * (0.5) = 2.5$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0.316$\n",
    "  \n",
    "  - Expected Non-Players = $5 * (0.5) = 2.5$\n",
    "  - Actual Non-Players = $2$\n",
    "  - Chi-Square Non-Players = $0.316$\n",
    "\n",
    "Total Chi-Square (Performance) = $0.316 + 0.316 + 0.267 + 0.267 = 1.166$\n",
    "\n",
    "**Splitting by Class:**\n",
    "- Class: IX\n",
    "  - Expected Cricket Players = $6 * (0.5) = 3$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0$\n",
    "  \n",
    "  - Expected Non-Players = $6 * (0.5) = 3$\n",
    "  - Actual Non-Players = $3$\n",
    "  - Chi-Square Non-Players = $0$\n",
    "\n",
    "- Class: X\n",
    "  - Expected Cricket Players = $6 * (0.5) = 3$\n",
    "  - Actual Cricket Players = $3$\n",
    "  - Chi-Square Players = $0$\n",
    "  \n",
    "  - Expected Non-Players = $6 * (0.5) = 3$\n",
    "  - Actual Non-Players = $3$\n",
    "  - Chi-Square Non-Players = $0$\n",
    "\n",
    "Total Chi-Square (Class) = $0 + 0 + 0 + 0 = 0$\n",
    "\n",
    "We see that the total Chi-Square is greater when splitting by Performance than by Class, so we choose Performance to split the root node. This process is repeated recursively until we obtain pure leaf nodes.\n",
    "\n",
    "### Advantages of Decision Tree\n",
    "\n",
    "- Can be used for both classification and regression problems.\n",
    "- Handles both continuous and categorical variables.\n",
    "- No feature scaling required.\n",
    "- Non-linear parameters don't affect performance.\n",
    "- Can automatically handle missing values and outliers.\n",
    "- Shorter training period compared to Random Forest.\n",
    "\n",
    "### Disadvantages of Decision Tree\n",
    "\n",
    "- Prone to overfitting, leading to wrong predictions.\n",
    "- High variance in output due to overfitting.\n",
    "- Adding new data may require re-generation of the entire tree.\n",
    "- Not suitable for large datasets, prone to overfitting.\n",
    "\n",
    "### Popular Algorithms\n",
    "\n",
    "- ID3 (Iterative Dichotomiser): Uses Information Gain.\n",
    "- C4.5: Uses Gain Ratio.\n",
    "- CART (Classification and Regression Trees): Uses Gini Index.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "- Ignore missing values.\n",
    "- Treat them as another category or nominal feature.\n",
    "- Use surrogate features or diminish weights for distribution.\n",
    "\n",
    "### Robustness to Outliers\n",
    "\n",
    "- Regression trees are affected, while classification trees are less affected.\n",
    "\n",
    "### Pruning a Tree\n",
    "\n",
    "**Post-Pruning:**\n",
    "- Minimum error: Prune to the point with minimum cross-validated error.\n",
    "- Smallest tree: Prune slightly beyond minimum error for a smaller tree.\n",
    "\n",
    "**Pre-Pruning (Early Stopping):**\n",
    "- Stops tree-building early if error doesn't decrease significantly.\n",
    "- Used together or separately for accuracy and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9047d-7cda-49a6-b6ce-2404bcc2b685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
