{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82636347-dd45-4e83-bb4e-1f38c5caa3cf",
   "metadata": {},
   "source": [
    "## XGBoost Classifier\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm designed to optimize both performance and computational efficiency. It is widely used for classification tasks and offers several enhancements over traditional gradient boosting techniques, such as regularization, parallel processing, and handling missing values.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors made by the previous models. This process is guided by gradient descent, optimizing a specific loss function.\n",
    "\n",
    "#### 2. Regularization\n",
    "\n",
    "XGBoost introduces regularization to control overfitting by adding penalties to the model's complexity, making it more robust to noisy data and outliers.\n",
    "\n",
    "#### 3. Tree Pruning\n",
    "\n",
    "XGBoost employs a more sophisticated approach to tree pruning by using max depth and pruning trees backward. It starts from a given maximum depth and prunes backward to remove splits that do not improve the model.\n",
    "\n",
    "### Steps Involved in XGBoost Classifier\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The XGBoost process begins by initializing the model with a constant value, typically the log-odds of the target classes $y$. \n",
    "\n",
    "For binary classification tasks:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $L$ is the loss function, such as log-loss (binary cross-entropy), and $N$ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For binary classification, log-loss is commonly used.\n",
    "- **Initial Prediction ($F_0$):** We find $\\gamma$ that minimizes the sum of the loss function. For log-loss, this $\\gamma$ is the log-odds of the positive class.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "XGBoost constructs an ensemble of trees in a sequential manner. At each iteration $m$:\n",
    "\n",
    "**Step 2-1: Calculate Gradient and Hessian**\n",
    "\n",
    "- Compute the gradient (first derivative) and Hessian (second derivative) of the loss function with respect to the predictions:\n",
    "\n",
    "$$ g_{im} = \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "$$ h_{im} = \\left[ \\frac{\\partial^2 L(y_i, F(x_i))}{\\partial F(x_i)^2} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For log-loss, the gradient $g_{im}$ and Hessian $h_{im}$ are given by:\n",
    "\n",
    "$$ g_{im} = \\hat{y}_i - y_i $$\n",
    "$$ h_{im} = \\hat{y}_i (1 - \\hat{y}_i) $$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted probability.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Gradient ($g_{im}$):** The gradient measures the difference between the predicted probability and the actual class label.\n",
    "- **Hessian ($h_{im}$):** The Hessian measures the curvature of the loss function, helping in second-order optimization.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a regression tree $h_m(x)$ to the gradients $g_{im}$ using weighted least squares, where weights are given by the Hessians $h_{im}$.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weighted Least Squares:** Each split is chosen to minimize the weighted sum of squared errors, taking into account both gradients and Hessians.\n",
    "\n",
    "**Step 2-3: Compute Leaf Weights**\n",
    "\n",
    "- For each leaf $j$ in the tree $h_m$, compute the optimal leaf weight $\\gamma_{jm}$ that minimizes the loss:\n",
    "\n",
    "$$ \\gamma_{jm} = - \\frac{\\sum_{i \\in R_{jm}} g_{im}}{\\sum_{i \\in R_{jm}} h_{im}} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Leaf Weight ($\\gamma_{jm}$):** This value is used to update the modelâ€™s prediction for all samples in the leaf. It is derived from the ratio of the sum of gradients to the sum of Hessians within the leaf.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted tree, scaled by a learning rate $\\eta$:\n",
    "\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($\\eta$):** This controls the contribution of each new tree to the final model, helping to prevent overfitting.\n",
    "- **Model Update:** The new prediction $F_m(x)$ is the previous prediction $F_{m-1}(x)$ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $M$ iterations, the final boosted model $F(x)$ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in XGBoost Classifier include:\n",
    "\n",
    "- **n_estimators:** Number of boosting stages (i.e., the number of trees).\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **max_depth:** Maximum depth of individual trees.\n",
    "- **min_child_weight:** Minimum sum of instance weight needed in a child.\n",
    "- **subsample:** Fraction of samples used for fitting individual trees. Reducing this can improve generalization.\n",
    "- **colsample_bytree:** Fraction of features used for fitting individual trees.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Performance:** XGBoost often achieves high accuracy on complex datasets.\n",
    "2. **Efficiency:** Optimized for speed and memory usage with parallel processing.\n",
    "3. **Regularization:** Built-in regularization helps prevent overfitting.\n",
    "4. **Flexibility:** Can handle various types of data and different loss functions.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how XGBoost Classifier can be implemented using the XGBoost library in Python:\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "XGBoost Classifier is a powerful and efficient boosting technique for classification tasks. By iteratively fitting weak learners to the residuals of the previous learners and incorporating regularization, it builds a robust model capable of high accuracy. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2dcc3-34c7-4875-a7ac-d64983355a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
