{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad204f8-6048-436f-894b-f7525dddf59e",
   "metadata": {},
   "source": [
    "## XGBoost Regressor\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm designed to optimize performance and computational efficiency. It is widely used in regression tasks and has several enhancements over traditional gradient boosting techniques, such as regularization, parallel processing, and handling missing values.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble technique that builds models in a sequential manner, where each new model attempts to correct the errors made by the previous models. This is guided by gradient descent, optimizing a specific loss function.\n",
    "\n",
    "#### 2. Regularization\n",
    "\n",
    "XGBoost introduces regularization to control overfitting by adding penalties to the model's complexity, making it more robust to noisy data and outliers.\n",
    "\n",
    "#### 3. Tree Pruning\n",
    "\n",
    "XGBoost employs a more sophisticated approach to tree pruning by using max depth and pruning trees backward. It starts from a given maximum depth and prunes backward to remove splits that do not improve the model.\n",
    "\n",
    "### Steps Involved in XGBoost Regressor\n",
    "\n",
    "1. **Initialization**\n",
    "2. **Iterative Learning**\n",
    "3. **Model Update**\n",
    "4. **Final Prediction**\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "The XGBoost process begins by initializing the model with a constant value, typically the mean of the target values $y$. \n",
    "\n",
    "For regression tasks:\n",
    "$$ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma) $$\n",
    "\n",
    "where $L$ is the loss function, such as Mean Squared Error (MSE), and $N$ is the number of samples.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Loss Function (L):** For regression, squared loss is commonly used.\n",
    "- **Initial Prediction ($F_0$):** We find $\\gamma$ that minimizes the sum of the loss function. For MSE, this $\\gamma$ is the mean of $y$.\n",
    "\n",
    "#### 2. Iterative Learning\n",
    "\n",
    "XGBoost constructs an ensemble of trees in a sequential manner. At each iteration $m$:\n",
    "\n",
    "**Step 2-1: Calculate Gradient and Hessian**\n",
    "\n",
    "- Compute the gradient (first derivative) and Hessian (second derivative) of the loss function with respect to the predictions:\n",
    "\n",
    "$$ g_{im} = \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "$$ h_{im} = \\left[ \\frac{\\partial^2 L(y_i, F(x_i))}{\\partial F(x_i)^2} \\right]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "For squared loss, the gradient $g_{im}$ simplifies to:\n",
    "$$ g_{im} = F_{m-1}(x_i) - y_i $$\n",
    "And the Hessian $h_{im}$ simplifies to:\n",
    "$$ h_{im} = 1 $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Gradient ($g_{im}$):** The gradient measures the direction and magnitude of the steepest descent of the loss function.\n",
    "- **Hessian ($h_{im}$):** The Hessian measures the curvature of the loss function, helping in second-order optimization.\n",
    "\n",
    "**Step 2-2: Fit a Weak Learner**\n",
    "\n",
    "- Fit a regression tree $h_m(x)$ to the gradients $g_{im}$ using weighted least squares, where weights are given by the Hessians $h_{im}$.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Weighted Least Squares:** Each split is chosen to minimize the weighted sum of squared errors, taking into account both gradients and Hessians.\n",
    "\n",
    "**Step 2-3: Compute Leaf Weights**\n",
    "\n",
    "- For each leaf $j$ in the tree $h_m$, compute the optimal leaf weight $\\gamma_{jm}$ that minimizes the loss:\n",
    "\n",
    "$$ \\gamma_{jm} = - \\frac{\\sum_{i \\in R_{jm}} g_{im}}{\\sum_{i \\in R_{jm}} h_{im}} $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Leaf Weight ($\\gamma_{jm}$):** This value is used to update the modelâ€™s prediction for all samples in the leaf. It is derived from the ratio of the sum of gradients to the sum of Hessians within the leaf.\n",
    "\n",
    "**Step 2-4: Update the Model**\n",
    "\n",
    "- Update the model by adding the fitted tree, scaled by a learning rate $\\eta$:\n",
    "\n",
    "$$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- **Learning Rate ($\\eta$):** This controls the contribution of each new tree to the final model, helping to prevent overfitting.\n",
    "- **Model Update:** The new prediction $F_m(x)$ is the previous prediction $F_{m-1}(x)$ plus a scaled version of the new tree's predictions.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "After $M$ iterations, the final boosted model $F(x)$ is a weighted sum of the weak learners:\n",
    "\n",
    "$$ F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x) $$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters in XGBoost Regressor include:\n",
    "\n",
    "- **n_estimators:** Number of boosting stages (i.e., the number of trees).\n",
    "- **learning_rate:** Step size for each iteration. Smaller values make the model more robust to overfitting but require more iterations.\n",
    "- **max_depth:** Maximum depth of individual trees.\n",
    "- **min_child_weight:** Minimum sum of instance weight needed in a child.\n",
    "- **subsample:** Fraction of samples used for fitting individual trees. Reducing this can improve generalization.\n",
    "- **colsample_bytree:** Fraction of features used for fitting individual trees.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Performance:** XGBoost often achieves high accuracy on complex datasets.\n",
    "2. **Efficiency:** Optimized for speed and memory usage with parallel processing.\n",
    "3. **Regularization:** Built-in regularization helps prevent overfitting.\n",
    "4. **Flexibility:** Can handle various types of data and different loss functions.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity:** More complex than simpler models and harder to interpret.\n",
    "2. **Parameter Tuning:** Requires careful tuning of hyperparameters to achieve optimal performance.\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "Here's a brief overview of how XGBoost Regressor can be implemented using the XGBoost library in Python:\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "xgb_regressor = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "XGBoost Regressor is a powerful and efficient boosting technique for regression tasks. By iteratively fitting weak learners to the residuals of the previous learners and incorporating regularization, it builds a robust model capable of high accuracy. Proper tuning of hyperparameters and understanding the underlying process can lead to highly accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065bd0d-22cb-406d-93e5-f1af74a7a246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
