{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b588cb-6b01-41ef-8979-b8388b62cc60",
   "metadata": {},
   "source": [
    "# Loss functions for both regression and classification problems.\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "- **Type:** Regression\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of observations.\n",
    "- **How it works:** MSE measures the average of the squares of the errors—that is, the average squared difference between the actual and predicted values. It is sensitive to outliers because the errors are squared.\n",
    "- **Use case:** Used in linear regression and other regression problems where the goal is to minimize the prediction error.\n",
    "\n",
    "### 2. Mean Absolute Error (MAE)\n",
    "- **Type:** Regression\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of observations.\n",
    "- **How it works:** MAE measures the average of the absolute differences between the actual and predicted values. It is more robust to outliers compared to MSE.\n",
    "- **Use case:** Used in regression problems where robustness to outliers is desired.\n",
    "\n",
    "### 3. Huber Loss\n",
    "- **Type:** Regression\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{Huber Loss} = \\begin{cases} \n",
    "  \\frac{1}{2} (y_i - \\hat{y}_i)^2 & \\text{for } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "  \\delta |y_i - \\hat{y}_i| - \\frac{1}{2} \\delta^2 & \\text{for } |y_i - \\hat{y}_i| > \\delta \n",
    "  \\end{cases}\n",
    "  $$\n",
    "  where $ \\delta $ is a threshold parameter.\n",
    "- **How it works:** Huber loss combines MSE and MAE, being quadratic for small errors and linear for large errors. This makes it less sensitive to outliers than MSE while maintaining sensitivity for small errors.\n",
    "- **Use case:** Used in regression problems where the presence of outliers is a concern, but small errors should still be penalized more than MAE.\n",
    "\n",
    "### 4. Logarithmic Loss (Log Loss)\n",
    "- **Type:** Classification (Binary)\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]\n",
    "  $$\n",
    "  where $ y_i $ is the actual binary label (0 or 1), $ \\hat{p}_i $ is the predicted probability of the positive class, and $ n $ is the number of observations.\n",
    "- **How it works:** Log loss measures the performance of a classification model whose output is a probability value between 0 and 1. It penalizes incorrect predictions more heavily when the prediction is confident but wrong.\n",
    "- **Use case:** Used in binary classification problems, commonly in logistic regression and neural networks.\n",
    "\n",
    "### 5. Cross-Entropy Loss\n",
    "- **Type:** Classification (Multi-class)\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{p}_{i,c})\n",
    "  $$\n",
    "  where $ y_{i,c} $ is a binary indicator (0 or 1) if class label $ c $ is the correct classification for observation $ i $, $ \\hat{p}_{i,c} $ is the predicted probability of class $ c $ for observation $ i $, $ n $ is the number of observations, and $ C $ is the number of classes.\n",
    "- **How it works:** Cross-entropy loss generalizes log loss to multiple classes. It measures the performance of a classification model whose output is a probability distribution over classes.\n",
    "- **Use case:** Used in multi-class classification problems, commonly in softmax regression and neural networks.\n",
    "\n",
    "### 6. Hinge Loss\n",
    "- **Type:** Classification (Binary)\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i \\hat{y}_i)\n",
    "  $$\n",
    "  where $ y_i $ is the actual label (−1 or 1), $ \\hat{y}_i $ is the predicted label, and $ n $ is the number of observations.\n",
    "- **How it works:** Hinge loss is used for training classifiers, particularly support vector machines (SVMs). It penalizes predictions that are not only wrong but also within a margin of error.\n",
    "- **Use case:** Used in binary classification problems, especially with support vector machines (SVMs).\n",
    "\n",
    "### 7. Kullback-Leibler Divergence (KL Divergence)\n",
    "- **Type:** Classification (Probability Distributions)\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{KL Divergence} = \\sum_{i=1}^{n} P(x_i) \\log \\left( \\frac{P(x_i)}{Q(x_i)} \\right)\n",
    "  $$\n",
    "  where $ P(x) $ is the true probability distribution and $ Q(x) $ is the predicted probability distribution.\n",
    "- **How it works:** KL divergence measures how one probability distribution diverges from a second, expected probability distribution.\n",
    "- **Use case:** Used in classification problems involving probability distributions, such as variational autoencoders.\n",
    "\n",
    "### 8. Poisson Loss\n",
    "- **Type:** Regression (Count Data)\n",
    "- **Mathematical Equation:** \n",
    "  $$\n",
    "  \\text{Poisson Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\log(\\hat{y}_i) + \\log(y_i!) \\right)\n",
    "  $$\n",
    "  where $ y_i $ is the actual count, $ \\hat{y}_i $ is the predicted count, and $ n $ is the number of observations.\n",
    "- **How it works:** Poisson loss is used for count data and is derived from the Poisson distribution. It measures the error between predicted and actual counts.\n",
    "- **Use case:** Used in regression problems involving count data, such as the number of events occurring within a fixed period.\n",
    "\n",
    "These are some of the most commonly used loss functions in machine learning. The choice of loss function depends on the specific problem (regression or classification), the nature of the data, and the model being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bcf0b-d387-4566-8b72-36979baf53c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
